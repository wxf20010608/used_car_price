{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(149999, 83)\n",
      "(50000, 83)\n",
      "fold n°1\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.081016 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17643\n",
      "[LightGBM] [Info] Number of data points in the train set: 134999, number of used features: 83\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Info] Start training from score 8.086718\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\ttraining's l1: 0.14616\ttraining's myFeval: 759.277\tvalid_1's l1: 0.148133\tvalid_1's myFeval: 806.277\n",
      "[600]\ttraining's l1: 0.127635\ttraining's myFeval: 597.761\tvalid_1's l1: 0.129608\tvalid_1's myFeval: 628.858\n",
      "[900]\ttraining's l1: 0.120952\ttraining's myFeval: 545.781\tvalid_1's l1: 0.123165\tvalid_1's myFeval: 574.072\n",
      "[1200]\ttraining's l1: 0.117161\ttraining's myFeval: 519.097\tvalid_1's l1: 0.119728\tvalid_1's myFeval: 548.005\n",
      "[1500]\ttraining's l1: 0.114667\ttraining's myFeval: 499.419\tvalid_1's l1: 0.117561\tvalid_1's myFeval: 529.236\n",
      "[1800]\ttraining's l1: 0.112729\ttraining's myFeval: 484.989\tvalid_1's l1: 0.116058\tvalid_1's myFeval: 517.092\n",
      "[2100]\ttraining's l1: 0.111222\ttraining's myFeval: 473.881\tvalid_1's l1: 0.11494\tvalid_1's myFeval: 508.486\n",
      "[2400]\ttraining's l1: 0.109934\ttraining's myFeval: 464.638\tvalid_1's l1: 0.114048\tvalid_1's myFeval: 501.562\n",
      "[2700]\ttraining's l1: 0.108786\ttraining's myFeval: 457.088\tvalid_1's l1: 0.113271\tvalid_1's myFeval: 496.206\n",
      "[3000]\ttraining's l1: 0.107759\ttraining's myFeval: 450.564\tvalid_1's l1: 0.112566\tvalid_1's myFeval: 491.813\n",
      "[3300]\ttraining's l1: 0.10688\ttraining's myFeval: 444.895\tvalid_1's l1: 0.111999\tvalid_1's myFeval: 488.165\n",
      "[3600]\ttraining's l1: 0.105942\ttraining's myFeval: 439.318\tvalid_1's l1: 0.111412\tvalid_1's myFeval: 484.729\n",
      "[3900]\ttraining's l1: 0.105067\ttraining's myFeval: 434.608\tvalid_1's l1: 0.110869\tvalid_1's myFeval: 481.935\n",
      "[4200]\ttraining's l1: 0.104333\ttraining's myFeval: 430.071\tvalid_1's l1: 0.110455\tvalid_1's myFeval: 479.46\n",
      "[4500]\ttraining's l1: 0.103718\ttraining's myFeval: 426.459\tvalid_1's l1: 0.110129\tvalid_1's myFeval: 477.539\n",
      "[4800]\ttraining's l1: 0.103129\ttraining's myFeval: 423.189\tvalid_1's l1: 0.109813\tvalid_1's myFeval: 475.576\n",
      "[5100]\ttraining's l1: 0.102489\ttraining's myFeval: 419.727\tvalid_1's l1: 0.109499\tvalid_1's myFeval: 473.823\n",
      "[5400]\ttraining's l1: 0.101878\ttraining's myFeval: 416.26\tvalid_1's l1: 0.109163\tvalid_1's myFeval: 471.686\n",
      "[5700]\ttraining's l1: 0.1013\ttraining's myFeval: 412.854\tvalid_1's l1: 0.108876\tvalid_1's myFeval: 470.005\n",
      "[6000]\ttraining's l1: 0.100839\ttraining's myFeval: 410.121\tvalid_1's l1: 0.108682\tvalid_1's myFeval: 468.672\n",
      "[6300]\ttraining's l1: 0.100369\ttraining's myFeval: 407.469\tvalid_1's l1: 0.108467\tvalid_1's myFeval: 467.351\n",
      "[6600]\ttraining's l1: 0.0998674\ttraining's myFeval: 404.659\tvalid_1's l1: 0.108239\tvalid_1's myFeval: 465.945\n",
      "[6900]\ttraining's l1: 0.0994419\ttraining's myFeval: 401.939\tvalid_1's l1: 0.108042\tvalid_1's myFeval: 464.782\n",
      "[7200]\ttraining's l1: 0.0990048\ttraining's myFeval: 399.522\tvalid_1's l1: 0.10787\tvalid_1's myFeval: 463.866\n",
      "[7500]\ttraining's l1: 0.0985836\ttraining's myFeval: 397.097\tvalid_1's l1: 0.107691\tvalid_1's myFeval: 462.88\n",
      "[7800]\ttraining's l1: 0.0981744\ttraining's myFeval: 394.822\tvalid_1's l1: 0.107567\tvalid_1's myFeval: 462.022\n",
      "[8100]\ttraining's l1: 0.097802\ttraining's myFeval: 392.514\tvalid_1's l1: 0.107422\tvalid_1's myFeval: 461.001\n",
      "[8400]\ttraining's l1: 0.0973781\ttraining's myFeval: 390.242\tvalid_1's l1: 0.107272\tvalid_1's myFeval: 460.064\n",
      "[8700]\ttraining's l1: 0.0969652\ttraining's myFeval: 388.032\tvalid_1's l1: 0.107122\tvalid_1's myFeval: 459.198\n",
      "[9000]\ttraining's l1: 0.0966234\ttraining's myFeval: 386.179\tvalid_1's l1: 0.106993\tvalid_1's myFeval: 458.484\n",
      "[9300]\ttraining's l1: 0.0962473\ttraining's myFeval: 384.149\tvalid_1's l1: 0.106857\tvalid_1's myFeval: 457.661\n",
      "[9600]\ttraining's l1: 0.0958844\ttraining's myFeval: 382.357\tvalid_1's l1: 0.106724\tvalid_1's myFeval: 456.922\n",
      "[9900]\ttraining's l1: 0.0955054\ttraining's myFeval: 380.583\tvalid_1's l1: 0.1066\tvalid_1's myFeval: 456.265\n",
      "[10200]\ttraining's l1: 0.0951917\ttraining's myFeval: 378.737\tvalid_1's l1: 0.106495\tvalid_1's myFeval: 455.598\n",
      "[10500]\ttraining's l1: 0.0949343\ttraining's myFeval: 377.101\tvalid_1's l1: 0.106421\tvalid_1's myFeval: 455.015\n",
      "[10800]\ttraining's l1: 0.0946479\ttraining's myFeval: 375.519\tvalid_1's l1: 0.106344\tvalid_1's myFeval: 454.618\n",
      "[11100]\ttraining's l1: 0.0943258\ttraining's myFeval: 373.881\tvalid_1's l1: 0.10625\tvalid_1's myFeval: 454.048\n",
      "[11400]\ttraining's l1: 0.0940463\ttraining's myFeval: 372.529\tvalid_1's l1: 0.106196\tvalid_1's myFeval: 453.624\n",
      "[11700]\ttraining's l1: 0.0937836\ttraining's myFeval: 371.203\tvalid_1's l1: 0.106119\tvalid_1's myFeval: 453.246\n",
      "[12000]\ttraining's l1: 0.0935054\ttraining's myFeval: 369.907\tvalid_1's l1: 0.106066\tvalid_1's myFeval: 453.012\n",
      "[12300]\ttraining's l1: 0.0932796\ttraining's myFeval: 368.663\tvalid_1's l1: 0.106012\tvalid_1's myFeval: 452.644\n",
      "[12600]\ttraining's l1: 0.0930167\ttraining's myFeval: 367.235\tvalid_1's l1: 0.105957\tvalid_1's myFeval: 452.245\n",
      "[12900]\ttraining's l1: 0.0927826\ttraining's myFeval: 366.053\tvalid_1's l1: 0.10589\tvalid_1's myFeval: 451.874\n",
      "[13200]\ttraining's l1: 0.092515\ttraining's myFeval: 364.453\tvalid_1's l1: 0.105833\tvalid_1's myFeval: 451.558\n",
      "[13500]\ttraining's l1: 0.0922356\ttraining's myFeval: 363.204\tvalid_1's l1: 0.105749\tvalid_1's myFeval: 451.219\n",
      "[13800]\ttraining's l1: 0.0920112\ttraining's myFeval: 362.039\tvalid_1's l1: 0.105707\tvalid_1's myFeval: 450.921\n",
      "[14100]\ttraining's l1: 0.0917783\ttraining's myFeval: 360.81\tvalid_1's l1: 0.105646\tvalid_1's myFeval: 450.517\n",
      "[14400]\ttraining's l1: 0.0915345\ttraining's myFeval: 359.573\tvalid_1's l1: 0.105588\tvalid_1's myFeval: 450.182\n",
      "[14700]\ttraining's l1: 0.0912867\ttraining's myFeval: 358.448\tvalid_1's l1: 0.10554\tvalid_1's myFeval: 449.97\n",
      "[15000]\ttraining's l1: 0.0911191\ttraining's myFeval: 357.549\tvalid_1's l1: 0.1055\tvalid_1's myFeval: 449.729\n",
      "[15300]\ttraining's l1: 0.0909113\ttraining's myFeval: 356.413\tvalid_1's l1: 0.105461\tvalid_1's myFeval: 449.482\n",
      "[15600]\ttraining's l1: 0.0906722\ttraining's myFeval: 355.325\tvalid_1's l1: 0.105418\tvalid_1's myFeval: 449.213\n",
      "[15900]\ttraining's l1: 0.0904405\ttraining's myFeval: 354.103\tvalid_1's l1: 0.105371\tvalid_1's myFeval: 448.948\n",
      "[16200]\ttraining's l1: 0.0902372\ttraining's myFeval: 353.269\tvalid_1's l1: 0.105328\tvalid_1's myFeval: 448.81\n",
      "[16500]\ttraining's l1: 0.0900277\ttraining's myFeval: 352.315\tvalid_1's l1: 0.105275\tvalid_1's myFeval: 448.551\n",
      "[16800]\ttraining's l1: 0.0898394\ttraining's myFeval: 351.292\tvalid_1's l1: 0.105232\tvalid_1's myFeval: 448.371\n",
      "[17100]\ttraining's l1: 0.0896536\ttraining's myFeval: 350.29\tvalid_1's l1: 0.105194\tvalid_1's myFeval: 448.195\n",
      "[17400]\ttraining's l1: 0.0894405\ttraining's myFeval: 349.366\tvalid_1's l1: 0.105162\tvalid_1's myFeval: 448.025\n",
      "[17700]\ttraining's l1: 0.0892206\ttraining's myFeval: 348.413\tvalid_1's l1: 0.105122\tvalid_1's myFeval: 447.778\n",
      "[18000]\ttraining's l1: 0.0890037\ttraining's myFeval: 347.428\tvalid_1's l1: 0.105081\tvalid_1's myFeval: 447.667\n",
      "[18300]\ttraining's l1: 0.0887917\ttraining's myFeval: 346.461\tvalid_1's l1: 0.105039\tvalid_1's myFeval: 447.432\n",
      "[18600]\ttraining's l1: 0.0886124\ttraining's myFeval: 345.593\tvalid_1's l1: 0.105009\tvalid_1's myFeval: 447.296\n",
      "[18900]\ttraining's l1: 0.0884555\ttraining's myFeval: 344.891\tvalid_1's l1: 0.104985\tvalid_1's myFeval: 447.109\n",
      "[19200]\ttraining's l1: 0.0882874\ttraining's myFeval: 344.142\tvalid_1's l1: 0.104953\tvalid_1's myFeval: 446.909\n",
      "[19500]\ttraining's l1: 0.0881298\ttraining's myFeval: 343.425\tvalid_1's l1: 0.104934\tvalid_1's myFeval: 446.771\n",
      "[19800]\ttraining's l1: 0.0879895\ttraining's myFeval: 342.659\tvalid_1's l1: 0.104918\tvalid_1's myFeval: 446.65\n",
      "[20100]\ttraining's l1: 0.0878387\ttraining's myFeval: 341.831\tvalid_1's l1: 0.104876\tvalid_1's myFeval: 446.441\n",
      "[20400]\ttraining's l1: 0.0876747\ttraining's myFeval: 341.036\tvalid_1's l1: 0.10485\tvalid_1's myFeval: 446.303\n",
      "[20700]\ttraining's l1: 0.0875117\ttraining's myFeval: 340.21\tvalid_1's l1: 0.104827\tvalid_1's myFeval: 446.123\n",
      "[21000]\ttraining's l1: 0.0873744\ttraining's myFeval: 339.414\tvalid_1's l1: 0.104789\tvalid_1's myFeval: 445.929\n",
      "[21300]\ttraining's l1: 0.0872162\ttraining's myFeval: 338.697\tvalid_1's l1: 0.10479\tvalid_1's myFeval: 445.978\n",
      "Early stopping, best iteration is:\n",
      "[21054]\ttraining's l1: 0.0873471\ttraining's myFeval: 339.261\tvalid_1's l1: 0.104784\tvalid_1's myFeval: 445.882\n",
      "fold n°2\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.083478 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17642\n",
      "[LightGBM] [Info] Number of data points in the train set: 134999, number of used features: 83\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Info] Start training from score 8.086718\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\ttraining's l1: 0.14521\ttraining's myFeval: 760.171\tvalid_1's l1: 0.149482\tvalid_1's myFeval: 752.821\n",
      "[600]\ttraining's l1: 0.127359\ttraining's myFeval: 599.714\tvalid_1's l1: 0.132005\tvalid_1's myFeval: 605.986\n",
      "[900]\ttraining's l1: 0.120316\ttraining's myFeval: 545.033\tvalid_1's l1: 0.125308\tvalid_1's myFeval: 560.323\n",
      "[1200]\ttraining's l1: 0.116554\ttraining's myFeval: 515.516\tvalid_1's l1: 0.122025\tvalid_1's myFeval: 536.6\n",
      "[1500]\ttraining's l1: 0.113975\ttraining's myFeval: 497.142\tvalid_1's l1: 0.119899\tvalid_1's myFeval: 523.261\n",
      "[1800]\ttraining's l1: 0.112014\ttraining's myFeval: 482.112\tvalid_1's l1: 0.11838\tvalid_1's myFeval: 512.366\n",
      "[2100]\ttraining's l1: 0.11046\ttraining's myFeval: 470.9\tvalid_1's l1: 0.117275\tvalid_1's myFeval: 504.488\n",
      "[2400]\ttraining's l1: 0.109156\ttraining's myFeval: 462.206\tvalid_1's l1: 0.1164\tvalid_1's myFeval: 498.764\n",
      "[2700]\ttraining's l1: 0.107964\ttraining's myFeval: 454.361\tvalid_1's l1: 0.115606\tvalid_1's myFeval: 493.547\n",
      "[3000]\ttraining's l1: 0.106919\ttraining's myFeval: 447.902\tvalid_1's l1: 0.114939\tvalid_1's myFeval: 489.598\n",
      "[3300]\ttraining's l1: 0.105986\ttraining's myFeval: 441.934\tvalid_1's l1: 0.114354\tvalid_1's myFeval: 485.946\n",
      "[3600]\ttraining's l1: 0.105239\ttraining's myFeval: 436.82\tvalid_1's l1: 0.113925\tvalid_1's myFeval: 483.168\n",
      "[3900]\ttraining's l1: 0.104449\ttraining's myFeval: 431.665\tvalid_1's l1: 0.113492\tvalid_1's myFeval: 480.396\n",
      "[4200]\ttraining's l1: 0.103736\ttraining's myFeval: 427.264\tvalid_1's l1: 0.113106\tvalid_1's myFeval: 478.256\n",
      "[4500]\ttraining's l1: 0.103088\ttraining's myFeval: 423.103\tvalid_1's l1: 0.11278\tvalid_1's myFeval: 476.246\n",
      "[4800]\ttraining's l1: 0.102453\ttraining's myFeval: 419.646\tvalid_1's l1: 0.11247\tvalid_1's myFeval: 474.498\n",
      "[5100]\ttraining's l1: 0.101848\ttraining's myFeval: 416.26\tvalid_1's l1: 0.112199\tvalid_1's myFeval: 473.108\n",
      "[5400]\ttraining's l1: 0.101277\ttraining's myFeval: 413.118\tvalid_1's l1: 0.111952\tvalid_1's myFeval: 471.828\n",
      "[5700]\ttraining's l1: 0.100731\ttraining's myFeval: 410.277\tvalid_1's l1: 0.111741\tvalid_1's myFeval: 470.616\n",
      "[6000]\ttraining's l1: 0.10023\ttraining's myFeval: 407.547\tvalid_1's l1: 0.111546\tvalid_1's myFeval: 469.59\n",
      "[6300]\ttraining's l1: 0.0996645\ttraining's myFeval: 404.765\tvalid_1's l1: 0.111319\tvalid_1's myFeval: 468.378\n",
      "[6600]\ttraining's l1: 0.0991869\ttraining's myFeval: 402.201\tvalid_1's l1: 0.111169\tvalid_1's myFeval: 467.435\n",
      "[6900]\ttraining's l1: 0.0987179\ttraining's myFeval: 399.813\tvalid_1's l1: 0.111024\tvalid_1's myFeval: 466.625\n",
      "[7200]\ttraining's l1: 0.098315\ttraining's myFeval: 397.713\tvalid_1's l1: 0.11088\tvalid_1's myFeval: 465.888\n",
      "[7500]\ttraining's l1: 0.0979468\ttraining's myFeval: 395.294\tvalid_1's l1: 0.110784\tvalid_1's myFeval: 465.192\n",
      "[7800]\ttraining's l1: 0.0975647\ttraining's myFeval: 392.877\tvalid_1's l1: 0.110654\tvalid_1's myFeval: 464.47\n",
      "[8100]\ttraining's l1: 0.0971989\ttraining's myFeval: 390.594\tvalid_1's l1: 0.110556\tvalid_1's myFeval: 463.828\n",
      "[8400]\ttraining's l1: 0.0968256\ttraining's myFeval: 388.654\tvalid_1's l1: 0.110461\tvalid_1's myFeval: 463.29\n",
      "[8700]\ttraining's l1: 0.0964833\ttraining's myFeval: 386.772\tvalid_1's l1: 0.110363\tvalid_1's myFeval: 462.736\n",
      "[9000]\ttraining's l1: 0.0961456\ttraining's myFeval: 384.933\tvalid_1's l1: 0.110285\tvalid_1's myFeval: 462.118\n",
      "[9300]\ttraining's l1: 0.0957616\ttraining's myFeval: 383.011\tvalid_1's l1: 0.110207\tvalid_1's myFeval: 461.543\n",
      "[9600]\ttraining's l1: 0.095457\ttraining's myFeval: 381.425\tvalid_1's l1: 0.110128\tvalid_1's myFeval: 461.191\n",
      "[9900]\ttraining's l1: 0.0951219\ttraining's myFeval: 379.427\tvalid_1's l1: 0.110039\tvalid_1's myFeval: 460.694\n",
      "[10200]\ttraining's l1: 0.0948128\ttraining's myFeval: 377.664\tvalid_1's l1: 0.109969\tvalid_1's myFeval: 460.299\n",
      "[10500]\ttraining's l1: 0.0944908\ttraining's myFeval: 376.112\tvalid_1's l1: 0.109885\tvalid_1's myFeval: 459.914\n",
      "[10800]\ttraining's l1: 0.0942304\ttraining's myFeval: 374.722\tvalid_1's l1: 0.109817\tvalid_1's myFeval: 459.545\n",
      "[11100]\ttraining's l1: 0.0939619\ttraining's myFeval: 373.181\tvalid_1's l1: 0.109756\tvalid_1's myFeval: 459.062\n",
      "[11400]\ttraining's l1: 0.0936114\ttraining's myFeval: 371.563\tvalid_1's l1: 0.109671\tvalid_1's myFeval: 458.635\n",
      "[11700]\ttraining's l1: 0.0933131\ttraining's myFeval: 370.218\tvalid_1's l1: 0.109625\tvalid_1's myFeval: 458.295\n",
      "[12000]\ttraining's l1: 0.0930869\ttraining's myFeval: 368.91\tvalid_1's l1: 0.109563\tvalid_1's myFeval: 457.918\n",
      "[12300]\ttraining's l1: 0.0927591\ttraining's myFeval: 367.489\tvalid_1's l1: 0.109484\tvalid_1's myFeval: 457.612\n",
      "[12600]\ttraining's l1: 0.0924563\ttraining's myFeval: 366.024\tvalid_1's l1: 0.109426\tvalid_1's myFeval: 457.253\n",
      "[12900]\ttraining's l1: 0.0921554\ttraining's myFeval: 364.521\tvalid_1's l1: 0.109361\tvalid_1's myFeval: 456.941\n",
      "[13200]\ttraining's l1: 0.0919421\ttraining's myFeval: 363.369\tvalid_1's l1: 0.109307\tvalid_1's myFeval: 456.64\n",
      "[13500]\ttraining's l1: 0.0917131\ttraining's myFeval: 362.125\tvalid_1's l1: 0.109258\tvalid_1's myFeval: 456.433\n",
      "[13800]\ttraining's l1: 0.0914304\ttraining's myFeval: 360.89\tvalid_1's l1: 0.109231\tvalid_1's myFeval: 456.227\n",
      "[14100]\ttraining's l1: 0.0911957\ttraining's myFeval: 359.794\tvalid_1's l1: 0.109183\tvalid_1's myFeval: 455.919\n",
      "[14400]\ttraining's l1: 0.0909805\ttraining's myFeval: 358.583\tvalid_1's l1: 0.109147\tvalid_1's myFeval: 455.689\n",
      "[14700]\ttraining's l1: 0.0907281\ttraining's myFeval: 357.494\tvalid_1's l1: 0.109111\tvalid_1's myFeval: 455.525\n",
      "[15000]\ttraining's l1: 0.0905201\ttraining's myFeval: 356.413\tvalid_1's l1: 0.109082\tvalid_1's myFeval: 455.328\n",
      "[15300]\ttraining's l1: 0.0903301\ttraining's myFeval: 355.3\tvalid_1's l1: 0.10906\tvalid_1's myFeval: 455.162\n",
      "[15600]\ttraining's l1: 0.0901272\ttraining's myFeval: 354.351\tvalid_1's l1: 0.109019\tvalid_1's myFeval: 454.951\n",
      "[15900]\ttraining's l1: 0.0898894\ttraining's myFeval: 353.276\tvalid_1's l1: 0.108988\tvalid_1's myFeval: 454.767\n",
      "[16200]\ttraining's l1: 0.089703\ttraining's myFeval: 352.466\tvalid_1's l1: 0.108959\tvalid_1's myFeval: 454.604\n",
      "[16500]\ttraining's l1: 0.0895102\ttraining's myFeval: 351.55\tvalid_1's l1: 0.108925\tvalid_1's myFeval: 454.392\n",
      "[16800]\ttraining's l1: 0.0893276\ttraining's myFeval: 350.671\tvalid_1's l1: 0.108899\tvalid_1's myFeval: 454.23\n",
      "[17100]\ttraining's l1: 0.0891272\ttraining's myFeval: 349.645\tvalid_1's l1: 0.108861\tvalid_1's myFeval: 454.054\n",
      "[17400]\ttraining's l1: 0.0889597\ttraining's myFeval: 348.808\tvalid_1's l1: 0.108834\tvalid_1's myFeval: 453.897\n",
      "[17700]\ttraining's l1: 0.0887965\ttraining's myFeval: 348.004\tvalid_1's l1: 0.1088\tvalid_1's myFeval: 453.758\n",
      "[18000]\ttraining's l1: 0.0886328\ttraining's myFeval: 347.132\tvalid_1's l1: 0.108778\tvalid_1's myFeval: 453.592\n",
      "[18300]\ttraining's l1: 0.0884347\ttraining's myFeval: 346.234\tvalid_1's l1: 0.108754\tvalid_1's myFeval: 453.347\n",
      "[18600]\ttraining's l1: 0.0882696\ttraining's myFeval: 345.514\tvalid_1's l1: 0.108737\tvalid_1's myFeval: 453.222\n",
      "[18900]\ttraining's l1: 0.0881124\ttraining's myFeval: 344.75\tvalid_1's l1: 0.108724\tvalid_1's myFeval: 453.12\n",
      "[19200]\ttraining's l1: 0.0879474\ttraining's myFeval: 344.031\tvalid_1's l1: 0.10868\tvalid_1's myFeval: 452.967\n",
      "[19500]\ttraining's l1: 0.0878065\ttraining's myFeval: 343.288\tvalid_1's l1: 0.108661\tvalid_1's myFeval: 452.814\n",
      "[19800]\ttraining's l1: 0.0876258\ttraining's myFeval: 342.431\tvalid_1's l1: 0.108639\tvalid_1's myFeval: 452.675\n",
      "[20100]\ttraining's l1: 0.0874477\ttraining's myFeval: 341.602\tvalid_1's l1: 0.108612\tvalid_1's myFeval: 452.573\n",
      "[20400]\ttraining's l1: 0.0872578\ttraining's myFeval: 340.786\tvalid_1's l1: 0.108585\tvalid_1's myFeval: 452.48\n",
      "[20700]\ttraining's l1: 0.0870873\ttraining's myFeval: 340.024\tvalid_1's l1: 0.108567\tvalid_1's myFeval: 452.426\n",
      "[21000]\ttraining's l1: 0.0869213\ttraining's myFeval: 339.27\tvalid_1's l1: 0.108553\tvalid_1's myFeval: 452.375\n",
      "Early stopping, best iteration is:\n",
      "[20898]\ttraining's l1: 0.0869696\ttraining's myFeval: 339.47\tvalid_1's l1: 0.108554\tvalid_1's myFeval: 452.364\n",
      "fold n°3\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.068028 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17637\n",
      "[LightGBM] [Info] Number of data points in the train set: 134999, number of used features: 83\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Info] Start training from score 8.098947\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\ttraining's l1: 0.145552\ttraining's myFeval: 756.67\tvalid_1's l1: 0.147856\tvalid_1's myFeval: 732.441\n",
      "[600]\ttraining's l1: 0.127235\ttraining's myFeval: 597.624\tvalid_1's l1: 0.130218\tvalid_1's myFeval: 594.289\n",
      "[900]\ttraining's l1: 0.120528\ttraining's myFeval: 545.25\tvalid_1's l1: 0.124145\tvalid_1's myFeval: 551.644\n",
      "[1200]\ttraining's l1: 0.116881\ttraining's myFeval: 517.303\tvalid_1's l1: 0.121018\tvalid_1's myFeval: 529.881\n",
      "[1500]\ttraining's l1: 0.114286\ttraining's myFeval: 497.82\tvalid_1's l1: 0.118808\tvalid_1's myFeval: 515.291\n",
      "[1800]\ttraining's l1: 0.112341\ttraining's myFeval: 484.516\tvalid_1's l1: 0.117274\tvalid_1's myFeval: 505.666\n",
      "[2100]\ttraining's l1: 0.110831\ttraining's myFeval: 474.092\tvalid_1's l1: 0.116057\tvalid_1's myFeval: 498.019\n",
      "[2400]\ttraining's l1: 0.10954\ttraining's myFeval: 464.766\tvalid_1's l1: 0.115108\tvalid_1's myFeval: 491.609\n",
      "[2700]\ttraining's l1: 0.108439\ttraining's myFeval: 456.937\tvalid_1's l1: 0.114338\tvalid_1's myFeval: 486.321\n",
      "[3000]\ttraining's l1: 0.107355\ttraining's myFeval: 450.097\tvalid_1's l1: 0.113634\tvalid_1's myFeval: 482.116\n",
      "[3300]\ttraining's l1: 0.106533\ttraining's myFeval: 444.445\tvalid_1's l1: 0.113092\tvalid_1's myFeval: 478.505\n",
      "[3600]\ttraining's l1: 0.105774\ttraining's myFeval: 439.802\tvalid_1's l1: 0.112635\tvalid_1's myFeval: 475.738\n",
      "[3900]\ttraining's l1: 0.105067\ttraining's myFeval: 435.337\tvalid_1's l1: 0.112264\tvalid_1's myFeval: 473.438\n",
      "[4200]\ttraining's l1: 0.104274\ttraining's myFeval: 430.735\tvalid_1's l1: 0.11186\tvalid_1's myFeval: 471.049\n",
      "[4500]\ttraining's l1: 0.103609\ttraining's myFeval: 426.518\tvalid_1's l1: 0.111509\tvalid_1's myFeval: 468.755\n",
      "[4800]\ttraining's l1: 0.102991\ttraining's myFeval: 422.942\tvalid_1's l1: 0.111215\tvalid_1's myFeval: 467.123\n",
      "[5100]\ttraining's l1: 0.102343\ttraining's myFeval: 419.278\tvalid_1's l1: 0.110939\tvalid_1's myFeval: 465.652\n",
      "[5400]\ttraining's l1: 0.101836\ttraining's myFeval: 416.55\tvalid_1's l1: 0.110724\tvalid_1's myFeval: 464.323\n",
      "[5700]\ttraining's l1: 0.101245\ttraining's myFeval: 413.121\tvalid_1's l1: 0.110476\tvalid_1's myFeval: 462.946\n",
      "[6000]\ttraining's l1: 0.100765\ttraining's myFeval: 410.323\tvalid_1's l1: 0.110276\tvalid_1's myFeval: 461.744\n",
      "[6300]\ttraining's l1: 0.100297\ttraining's myFeval: 407.686\tvalid_1's l1: 0.110094\tvalid_1's myFeval: 460.697\n",
      "[6600]\ttraining's l1: 0.0998514\ttraining's myFeval: 404.982\tvalid_1's l1: 0.109915\tvalid_1's myFeval: 459.554\n",
      "[6900]\ttraining's l1: 0.0993916\ttraining's myFeval: 402.212\tvalid_1's l1: 0.109732\tvalid_1's myFeval: 458.538\n",
      "[7200]\ttraining's l1: 0.0989731\ttraining's myFeval: 399.601\tvalid_1's l1: 0.109588\tvalid_1's myFeval: 457.72\n",
      "[7500]\ttraining's l1: 0.0985304\ttraining's myFeval: 397.407\tvalid_1's l1: 0.109443\tvalid_1's myFeval: 457.037\n",
      "[7800]\ttraining's l1: 0.0981303\ttraining's myFeval: 395.148\tvalid_1's l1: 0.109319\tvalid_1's myFeval: 456.323\n",
      "[8100]\ttraining's l1: 0.0977145\ttraining's myFeval: 393.067\tvalid_1's l1: 0.109193\tvalid_1's myFeval: 455.539\n",
      "[8400]\ttraining's l1: 0.0973367\ttraining's myFeval: 391.047\tvalid_1's l1: 0.109064\tvalid_1's myFeval: 454.753\n",
      "[8700]\ttraining's l1: 0.0969412\ttraining's myFeval: 388.82\tvalid_1's l1: 0.108922\tvalid_1's myFeval: 454.011\n",
      "[9000]\ttraining's l1: 0.0965988\ttraining's myFeval: 386.929\tvalid_1's l1: 0.108838\tvalid_1's myFeval: 453.393\n",
      "[9300]\ttraining's l1: 0.0962347\ttraining's myFeval: 384.961\tvalid_1's l1: 0.10874\tvalid_1's myFeval: 452.545\n",
      "[9600]\ttraining's l1: 0.0958915\ttraining's myFeval: 383.072\tvalid_1's l1: 0.108647\tvalid_1's myFeval: 451.941\n",
      "[9900]\ttraining's l1: 0.0955225\ttraining's myFeval: 381.346\tvalid_1's l1: 0.108534\tvalid_1's myFeval: 451.298\n",
      "[10200]\ttraining's l1: 0.0952727\ttraining's myFeval: 379.876\tvalid_1's l1: 0.108467\tvalid_1's myFeval: 450.913\n",
      "[10500]\ttraining's l1: 0.0949606\ttraining's myFeval: 378.367\tvalid_1's l1: 0.10837\tvalid_1's myFeval: 450.44\n",
      "[10800]\ttraining's l1: 0.0946901\ttraining's myFeval: 376.816\tvalid_1's l1: 0.108306\tvalid_1's myFeval: 449.968\n",
      "[11100]\ttraining's l1: 0.0944512\ttraining's myFeval: 375.359\tvalid_1's l1: 0.10824\tvalid_1's myFeval: 449.525\n",
      "[11400]\ttraining's l1: 0.0941801\ttraining's myFeval: 373.737\tvalid_1's l1: 0.10816\tvalid_1's myFeval: 449.003\n",
      "[11700]\ttraining's l1: 0.0939178\ttraining's myFeval: 372.356\tvalid_1's l1: 0.108074\tvalid_1's myFeval: 448.536\n",
      "[12000]\ttraining's l1: 0.0936223\ttraining's myFeval: 370.975\tvalid_1's l1: 0.107989\tvalid_1's myFeval: 448.096\n",
      "[12300]\ttraining's l1: 0.0933701\ttraining's myFeval: 369.76\tvalid_1's l1: 0.107946\tvalid_1's myFeval: 447.919\n",
      "[12600]\ttraining's l1: 0.0931009\ttraining's myFeval: 368.421\tvalid_1's l1: 0.107884\tvalid_1's myFeval: 447.537\n",
      "[12900]\ttraining's l1: 0.0928533\ttraining's myFeval: 367.147\tvalid_1's l1: 0.107829\tvalid_1's myFeval: 447.2\n",
      "[13200]\ttraining's l1: 0.0925853\ttraining's myFeval: 365.963\tvalid_1's l1: 0.107774\tvalid_1's myFeval: 446.951\n",
      "[13500]\ttraining's l1: 0.0923608\ttraining's myFeval: 365.04\tvalid_1's l1: 0.107743\tvalid_1's myFeval: 446.768\n",
      "[13800]\ttraining's l1: 0.0921038\ttraining's myFeval: 363.763\tvalid_1's l1: 0.107697\tvalid_1's myFeval: 446.493\n",
      "[14100]\ttraining's l1: 0.0918208\ttraining's myFeval: 362.535\tvalid_1's l1: 0.107667\tvalid_1's myFeval: 446.244\n",
      "[14400]\ttraining's l1: 0.0915657\ttraining's myFeval: 361.37\tvalid_1's l1: 0.107603\tvalid_1's myFeval: 445.947\n",
      "[14700]\ttraining's l1: 0.0913545\ttraining's myFeval: 360.171\tvalid_1's l1: 0.107563\tvalid_1's myFeval: 445.771\n",
      "[15000]\ttraining's l1: 0.0911238\ttraining's myFeval: 359.136\tvalid_1's l1: 0.107504\tvalid_1's myFeval: 445.468\n",
      "[15300]\ttraining's l1: 0.0909084\ttraining's myFeval: 358.076\tvalid_1's l1: 0.107465\tvalid_1's myFeval: 445.269\n",
      "[15600]\ttraining's l1: 0.0907184\ttraining's myFeval: 357.118\tvalid_1's l1: 0.107412\tvalid_1's myFeval: 444.973\n",
      "[15900]\ttraining's l1: 0.0905014\ttraining's myFeval: 356.122\tvalid_1's l1: 0.107395\tvalid_1's myFeval: 444.795\n",
      "[16200]\ttraining's l1: 0.0903168\ttraining's myFeval: 355.196\tvalid_1's l1: 0.107347\tvalid_1's myFeval: 444.517\n",
      "[16500]\ttraining's l1: 0.0901336\ttraining's myFeval: 354.244\tvalid_1's l1: 0.107306\tvalid_1's myFeval: 444.318\n",
      "[16800]\ttraining's l1: 0.0899426\ttraining's myFeval: 353.342\tvalid_1's l1: 0.107265\tvalid_1's myFeval: 444.196\n",
      "[17100]\ttraining's l1: 0.0897058\ttraining's myFeval: 352.266\tvalid_1's l1: 0.107223\tvalid_1's myFeval: 443.903\n",
      "[17400]\ttraining's l1: 0.0895257\ttraining's myFeval: 351.372\tvalid_1's l1: 0.107188\tvalid_1's myFeval: 443.77\n",
      "[17700]\ttraining's l1: 0.0893535\ttraining's myFeval: 350.524\tvalid_1's l1: 0.107176\tvalid_1's myFeval: 443.682\n",
      "[18000]\ttraining's l1: 0.0891897\ttraining's myFeval: 349.7\tvalid_1's l1: 0.107163\tvalid_1's myFeval: 443.603\n",
      "[18300]\ttraining's l1: 0.0889827\ttraining's myFeval: 348.709\tvalid_1's l1: 0.107109\tvalid_1's myFeval: 443.368\n",
      "[18600]\ttraining's l1: 0.0887964\ttraining's myFeval: 347.821\tvalid_1's l1: 0.107093\tvalid_1's myFeval: 443.249\n",
      "[18900]\ttraining's l1: 0.0886383\ttraining's myFeval: 346.965\tvalid_1's l1: 0.107071\tvalid_1's myFeval: 443.135\n",
      "[19200]\ttraining's l1: 0.0884741\ttraining's myFeval: 346.123\tvalid_1's l1: 0.107041\tvalid_1's myFeval: 442.986\n",
      "[19500]\ttraining's l1: 0.0883124\ttraining's myFeval: 345.172\tvalid_1's l1: 0.10702\tvalid_1's myFeval: 442.902\n",
      "[19800]\ttraining's l1: 0.0881165\ttraining's myFeval: 344.29\tvalid_1's l1: 0.10699\tvalid_1's myFeval: 442.715\n",
      "[20100]\ttraining's l1: 0.0879245\ttraining's myFeval: 343.371\tvalid_1's l1: 0.106959\tvalid_1's myFeval: 442.528\n",
      "[20400]\ttraining's l1: 0.087759\ttraining's myFeval: 342.652\tvalid_1's l1: 0.10694\tvalid_1's myFeval: 442.439\n",
      "[20700]\ttraining's l1: 0.0875766\ttraining's myFeval: 341.907\tvalid_1's l1: 0.106916\tvalid_1's myFeval: 442.266\n",
      "[21000]\ttraining's l1: 0.0874109\ttraining's myFeval: 341.181\tvalid_1's l1: 0.106891\tvalid_1's myFeval: 442.178\n",
      "[21300]\ttraining's l1: 0.0872469\ttraining's myFeval: 340.422\tvalid_1's l1: 0.106866\tvalid_1's myFeval: 442.104\n",
      "[21600]\ttraining's l1: 0.0870511\ttraining's myFeval: 339.606\tvalid_1's l1: 0.106839\tvalid_1's myFeval: 441.959\n",
      "[21900]\ttraining's l1: 0.0868752\ttraining's myFeval: 338.809\tvalid_1's l1: 0.106825\tvalid_1's myFeval: 441.824\n",
      "[22200]\ttraining's l1: 0.0866844\ttraining's myFeval: 337.972\tvalid_1's l1: 0.106795\tvalid_1's myFeval: 441.646\n",
      "[22500]\ttraining's l1: 0.0865127\ttraining's myFeval: 337.185\tvalid_1's l1: 0.106758\tvalid_1's myFeval: 441.519\n",
      "[22800]\ttraining's l1: 0.086356\ttraining's myFeval: 336.592\tvalid_1's l1: 0.106742\tvalid_1's myFeval: 441.402\n",
      "[23100]\ttraining's l1: 0.0862178\ttraining's myFeval: 335.939\tvalid_1's l1: 0.106727\tvalid_1's myFeval: 441.296\n",
      "[23400]\ttraining's l1: 0.0860727\ttraining's myFeval: 335.261\tvalid_1's l1: 0.106707\tvalid_1's myFeval: 441.204\n",
      "[23700]\ttraining's l1: 0.0859311\ttraining's myFeval: 334.609\tvalid_1's l1: 0.106686\tvalid_1's myFeval: 441.159\n",
      "[24000]\ttraining's l1: 0.0857902\ttraining's myFeval: 333.942\tvalid_1's l1: 0.106675\tvalid_1's myFeval: 441.123\n",
      "[24300]\ttraining's l1: 0.0856394\ttraining's myFeval: 333.233\tvalid_1's l1: 0.106653\tvalid_1's myFeval: 441.002\n",
      "[24600]\ttraining's l1: 0.0854955\ttraining's myFeval: 332.547\tvalid_1's l1: 0.106637\tvalid_1's myFeval: 440.913\n",
      "[24900]\ttraining's l1: 0.0853521\ttraining's myFeval: 331.866\tvalid_1's l1: 0.10662\tvalid_1's myFeval: 440.813\n",
      "[25200]\ttraining's l1: 0.0852106\ttraining's myFeval: 331.2\tvalid_1's l1: 0.1066\tvalid_1's myFeval: 440.73\n",
      "[25500]\ttraining's l1: 0.0850482\ttraining's myFeval: 330.436\tvalid_1's l1: 0.106596\tvalid_1's myFeval: 440.71\n",
      "[25800]\ttraining's l1: 0.0849324\ttraining's myFeval: 329.904\tvalid_1's l1: 0.106583\tvalid_1's myFeval: 440.609\n",
      "[26100]\ttraining's l1: 0.0847988\ttraining's myFeval: 329.203\tvalid_1's l1: 0.106587\tvalid_1's myFeval: 440.512\n",
      "Early stopping, best iteration is:\n",
      "[25902]\ttraining's l1: 0.0848997\ttraining's myFeval: 329.743\tvalid_1's l1: 0.106578\tvalid_1's myFeval: 440.578\n",
      "fold n°4\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.095689 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17638\n",
      "[LightGBM] [Info] Number of data points in the train set: 134999, number of used features: 83\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Info] Start training from score 8.086718\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\ttraining's l1: 0.146168\ttraining's myFeval: 757.546\tvalid_1's l1: 0.143941\tvalid_1's myFeval: 740.213\n",
      "[600]\ttraining's l1: 0.127759\ttraining's myFeval: 599.84\tvalid_1's l1: 0.127139\tvalid_1's myFeval: 600.643\n",
      "[900]\ttraining's l1: 0.120779\ttraining's myFeval: 547.24\tvalid_1's l1: 0.121037\tvalid_1's myFeval: 553.542\n",
      "[1200]\ttraining's l1: 0.117148\ttraining's myFeval: 517.903\tvalid_1's l1: 0.11799\tvalid_1's myFeval: 528.645\n",
      "[1500]\ttraining's l1: 0.114623\ttraining's myFeval: 498.757\tvalid_1's l1: 0.116016\tvalid_1's myFeval: 512.919\n",
      "[1800]\ttraining's l1: 0.11274\ttraining's myFeval: 484.785\tvalid_1's l1: 0.114629\tvalid_1's myFeval: 502.069\n",
      "[2100]\ttraining's l1: 0.111094\ttraining's myFeval: 473.245\tvalid_1's l1: 0.11347\tvalid_1's myFeval: 494.104\n",
      "[2400]\ttraining's l1: 0.109743\ttraining's myFeval: 463.678\tvalid_1's l1: 0.112541\tvalid_1's myFeval: 487.472\n",
      "[2700]\ttraining's l1: 0.108499\ttraining's myFeval: 456.243\tvalid_1's l1: 0.111748\tvalid_1's myFeval: 482.698\n",
      "[3000]\ttraining's l1: 0.107524\ttraining's myFeval: 449.645\tvalid_1's l1: 0.111136\tvalid_1's myFeval: 478.489\n",
      "[3300]\ttraining's l1: 0.106602\ttraining's myFeval: 444.2\tvalid_1's l1: 0.110599\tvalid_1's myFeval: 475.286\n",
      "[3600]\ttraining's l1: 0.105684\ttraining's myFeval: 438.788\tvalid_1's l1: 0.110071\tvalid_1's myFeval: 472.154\n",
      "[3900]\ttraining's l1: 0.104929\ttraining's myFeval: 433.844\tvalid_1's l1: 0.109652\tvalid_1's myFeval: 469.734\n",
      "[4200]\ttraining's l1: 0.104253\ttraining's myFeval: 429.537\tvalid_1's l1: 0.10928\tvalid_1's myFeval: 467.541\n",
      "[4500]\ttraining's l1: 0.103534\ttraining's myFeval: 425.211\tvalid_1's l1: 0.108921\tvalid_1's myFeval: 465.462\n",
      "[4800]\ttraining's l1: 0.102892\ttraining's myFeval: 421.647\tvalid_1's l1: 0.108642\tvalid_1's myFeval: 463.996\n",
      "[5100]\ttraining's l1: 0.102231\ttraining's myFeval: 418.252\tvalid_1's l1: 0.108345\tvalid_1's myFeval: 462.321\n",
      "[5400]\ttraining's l1: 0.101715\ttraining's myFeval: 415.349\tvalid_1's l1: 0.108142\tvalid_1's myFeval: 461.012\n",
      "[5700]\ttraining's l1: 0.101134\ttraining's myFeval: 412.063\tvalid_1's l1: 0.107898\tvalid_1's myFeval: 459.377\n",
      "[6000]\ttraining's l1: 0.100556\ttraining's myFeval: 408.931\tvalid_1's l1: 0.107677\tvalid_1's myFeval: 458.018\n",
      "[6300]\ttraining's l1: 0.100056\ttraining's myFeval: 406.088\tvalid_1's l1: 0.107468\tvalid_1's myFeval: 456.835\n",
      "[6600]\ttraining's l1: 0.0996202\ttraining's myFeval: 403.469\tvalid_1's l1: 0.107294\tvalid_1's myFeval: 455.658\n",
      "[6900]\ttraining's l1: 0.099211\ttraining's myFeval: 400.882\tvalid_1's l1: 0.107145\tvalid_1's myFeval: 454.779\n",
      "[7200]\ttraining's l1: 0.0987892\ttraining's myFeval: 398.365\tvalid_1's l1: 0.106997\tvalid_1's myFeval: 453.901\n",
      "[7500]\ttraining's l1: 0.0983672\ttraining's myFeval: 396.013\tvalid_1's l1: 0.106864\tvalid_1's myFeval: 453.11\n",
      "[7800]\ttraining's l1: 0.0979357\ttraining's myFeval: 393.647\tvalid_1's l1: 0.106734\tvalid_1's myFeval: 452.373\n",
      "[8100]\ttraining's l1: 0.0976002\ttraining's myFeval: 391.472\tvalid_1's l1: 0.106618\tvalid_1's myFeval: 451.524\n",
      "[8400]\ttraining's l1: 0.0972432\ttraining's myFeval: 389.492\tvalid_1's l1: 0.106505\tvalid_1's myFeval: 450.814\n",
      "[8700]\ttraining's l1: 0.0968668\ttraining's myFeval: 387.271\tvalid_1's l1: 0.106389\tvalid_1's myFeval: 450.229\n",
      "[9000]\ttraining's l1: 0.0965186\ttraining's myFeval: 385.175\tvalid_1's l1: 0.106295\tvalid_1's myFeval: 449.671\n",
      "[9300]\ttraining's l1: 0.0961851\ttraining's myFeval: 383.325\tvalid_1's l1: 0.10621\tvalid_1's myFeval: 449.215\n",
      "[9600]\ttraining's l1: 0.0958978\ttraining's myFeval: 381.502\tvalid_1's l1: 0.106127\tvalid_1's myFeval: 448.645\n",
      "[9900]\ttraining's l1: 0.095556\ttraining's myFeval: 379.761\tvalid_1's l1: 0.106034\tvalid_1's myFeval: 448.127\n",
      "[10200]\ttraining's l1: 0.0952408\ttraining's myFeval: 378.03\tvalid_1's l1: 0.105962\tvalid_1's myFeval: 447.663\n",
      "[10500]\ttraining's l1: 0.0949222\ttraining's myFeval: 376.305\tvalid_1's l1: 0.105867\tvalid_1's myFeval: 447.102\n",
      "[10800]\ttraining's l1: 0.0946186\ttraining's myFeval: 374.74\tvalid_1's l1: 0.105806\tvalid_1's myFeval: 446.797\n",
      "[11100]\ttraining's l1: 0.0943113\ttraining's myFeval: 373.178\tvalid_1's l1: 0.105745\tvalid_1's myFeval: 446.355\n",
      "[11400]\ttraining's l1: 0.0940435\ttraining's myFeval: 371.779\tvalid_1's l1: 0.105671\tvalid_1's myFeval: 445.927\n",
      "[11700]\ttraining's l1: 0.0937515\ttraining's myFeval: 370.369\tvalid_1's l1: 0.105596\tvalid_1's myFeval: 445.56\n",
      "[12000]\ttraining's l1: 0.0935152\ttraining's myFeval: 369.054\tvalid_1's l1: 0.105527\tvalid_1's myFeval: 445.089\n",
      "[12300]\ttraining's l1: 0.0932776\ttraining's myFeval: 367.724\tvalid_1's l1: 0.105477\tvalid_1's myFeval: 444.706\n",
      "[12600]\ttraining's l1: 0.0930453\ttraining's myFeval: 366.54\tvalid_1's l1: 0.105416\tvalid_1's myFeval: 444.463\n",
      "[12900]\ttraining's l1: 0.092813\ttraining's myFeval: 365.344\tvalid_1's l1: 0.105356\tvalid_1's myFeval: 444.106\n",
      "[13200]\ttraining's l1: 0.0925398\ttraining's myFeval: 364.01\tvalid_1's l1: 0.105308\tvalid_1's myFeval: 443.805\n",
      "[13500]\ttraining's l1: 0.0923164\ttraining's myFeval: 362.82\tvalid_1's l1: 0.105273\tvalid_1's myFeval: 443.443\n",
      "[13800]\ttraining's l1: 0.092091\ttraining's myFeval: 361.654\tvalid_1's l1: 0.105241\tvalid_1's myFeval: 443.235\n",
      "[14100]\ttraining's l1: 0.0918746\ttraining's myFeval: 360.472\tvalid_1's l1: 0.105188\tvalid_1's myFeval: 442.838\n",
      "[14400]\ttraining's l1: 0.0916441\ttraining's myFeval: 359.464\tvalid_1's l1: 0.105137\tvalid_1's myFeval: 442.623\n",
      "[14700]\ttraining's l1: 0.0914047\ttraining's myFeval: 358.436\tvalid_1's l1: 0.105075\tvalid_1's myFeval: 442.372\n",
      "[15000]\ttraining's l1: 0.09119\ttraining's myFeval: 357.34\tvalid_1's l1: 0.105021\tvalid_1's myFeval: 442.096\n",
      "[15300]\ttraining's l1: 0.0909523\ttraining's myFeval: 356.271\tvalid_1's l1: 0.104979\tvalid_1's myFeval: 441.91\n",
      "[15600]\ttraining's l1: 0.0907509\ttraining's myFeval: 355.173\tvalid_1's l1: 0.104952\tvalid_1's myFeval: 441.733\n",
      "[15900]\ttraining's l1: 0.0905752\ttraining's myFeval: 354.173\tvalid_1's l1: 0.104926\tvalid_1's myFeval: 441.527\n",
      "[16200]\ttraining's l1: 0.0903413\ttraining's myFeval: 353.165\tvalid_1's l1: 0.104905\tvalid_1's myFeval: 441.35\n",
      "[16500]\ttraining's l1: 0.0901473\ttraining's myFeval: 352.251\tvalid_1's l1: 0.104884\tvalid_1's myFeval: 441.223\n",
      "[16800]\ttraining's l1: 0.0899507\ttraining's myFeval: 351.353\tvalid_1's l1: 0.104851\tvalid_1's myFeval: 440.994\n",
      "[17100]\ttraining's l1: 0.0897481\ttraining's myFeval: 350.312\tvalid_1's l1: 0.104815\tvalid_1's myFeval: 440.782\n",
      "[17400]\ttraining's l1: 0.0895295\ttraining's myFeval: 349.314\tvalid_1's l1: 0.104797\tvalid_1's myFeval: 440.642\n",
      "[17700]\ttraining's l1: 0.0893131\ttraining's myFeval: 348.219\tvalid_1's l1: 0.104784\tvalid_1's myFeval: 440.562\n",
      "[18000]\ttraining's l1: 0.0891277\ttraining's myFeval: 347.371\tvalid_1's l1: 0.104763\tvalid_1's myFeval: 440.392\n",
      "[18300]\ttraining's l1: 0.0889254\ttraining's myFeval: 346.484\tvalid_1's l1: 0.104735\tvalid_1's myFeval: 440.203\n",
      "[18600]\ttraining's l1: 0.0887325\ttraining's myFeval: 345.569\tvalid_1's l1: 0.10473\tvalid_1's myFeval: 440.108\n",
      "[18900]\ttraining's l1: 0.0885215\ttraining's myFeval: 344.558\tvalid_1's l1: 0.104699\tvalid_1's myFeval: 439.951\n",
      "[19200]\ttraining's l1: 0.0883263\ttraining's myFeval: 343.582\tvalid_1's l1: 0.104666\tvalid_1's myFeval: 439.8\n",
      "[19500]\ttraining's l1: 0.0881628\ttraining's myFeval: 342.803\tvalid_1's l1: 0.104647\tvalid_1's myFeval: 439.684\n",
      "[19800]\ttraining's l1: 0.0879971\ttraining's myFeval: 342.039\tvalid_1's l1: 0.104626\tvalid_1's myFeval: 439.527\n",
      "[20100]\ttraining's l1: 0.087828\ttraining's myFeval: 341.274\tvalid_1's l1: 0.10462\tvalid_1's myFeval: 439.536\n",
      "[20400]\ttraining's l1: 0.0876775\ttraining's myFeval: 340.612\tvalid_1's l1: 0.104583\tvalid_1's myFeval: 439.356\n",
      "[20700]\ttraining's l1: 0.0875298\ttraining's myFeval: 339.857\tvalid_1's l1: 0.104558\tvalid_1's myFeval: 439.224\n",
      "[21000]\ttraining's l1: 0.0873514\ttraining's myFeval: 339.011\tvalid_1's l1: 0.104534\tvalid_1's myFeval: 439.062\n",
      "[21300]\ttraining's l1: 0.087188\ttraining's myFeval: 338.266\tvalid_1's l1: 0.104522\tvalid_1's myFeval: 438.942\n",
      "[21600]\ttraining's l1: 0.0870063\ttraining's myFeval: 337.384\tvalid_1's l1: 0.104515\tvalid_1's myFeval: 438.827\n",
      "[21900]\ttraining's l1: 0.0868643\ttraining's myFeval: 336.763\tvalid_1's l1: 0.10449\tvalid_1's myFeval: 438.698\n",
      "[22200]\ttraining's l1: 0.0867014\ttraining's myFeval: 336.104\tvalid_1's l1: 0.104481\tvalid_1's myFeval: 438.643\n",
      "[22500]\ttraining's l1: 0.0865409\ttraining's myFeval: 335.379\tvalid_1's l1: 0.104467\tvalid_1's myFeval: 438.477\n",
      "[22800]\ttraining's l1: 0.0863786\ttraining's myFeval: 334.69\tvalid_1's l1: 0.104439\tvalid_1's myFeval: 438.362\n",
      "[23100]\ttraining's l1: 0.086229\ttraining's myFeval: 334.012\tvalid_1's l1: 0.104424\tvalid_1's myFeval: 438.278\n",
      "[23400]\ttraining's l1: 0.0860961\ttraining's myFeval: 333.298\tvalid_1's l1: 0.104408\tvalid_1's myFeval: 438.206\n",
      "[23700]\ttraining's l1: 0.0859806\ttraining's myFeval: 332.663\tvalid_1's l1: 0.10439\tvalid_1's myFeval: 438.135\n",
      "[24000]\ttraining's l1: 0.0858548\ttraining's myFeval: 332.013\tvalid_1's l1: 0.104385\tvalid_1's myFeval: 438.137\n",
      "Early stopping, best iteration is:\n",
      "[23817]\ttraining's l1: 0.0859259\ttraining's myFeval: 332.426\tvalid_1's l1: 0.104384\tvalid_1's myFeval: 438.101\n",
      "fold n°5\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.039186 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 17639\n",
      "[LightGBM] [Info] Number of data points in the train set: 134999, number of used features: 83\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Info] Start training from score 8.086718\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\ttraining's l1: 0.14519\ttraining's myFeval: 753.366\tvalid_1's l1: 0.151838\tvalid_1's myFeval: 793.79\n",
      "[600]\ttraining's l1: 0.127166\ttraining's myFeval: 594.981\tvalid_1's l1: 0.134315\tvalid_1's myFeval: 640.997\n",
      "[900]\ttraining's l1: 0.120225\ttraining's myFeval: 543.882\tvalid_1's l1: 0.127791\tvalid_1's myFeval: 593.327\n",
      "[1200]\ttraining's l1: 0.116589\ttraining's myFeval: 515.69\tvalid_1's l1: 0.124428\tvalid_1's myFeval: 566.002\n",
      "[1500]\ttraining's l1: 0.114108\ttraining's myFeval: 496.954\tvalid_1's l1: 0.122205\tvalid_1's myFeval: 548.627\n",
      "[1800]\ttraining's l1: 0.112098\ttraining's myFeval: 482.764\tvalid_1's l1: 0.120452\tvalid_1's myFeval: 535.836\n",
      "[2100]\ttraining's l1: 0.110539\ttraining's myFeval: 471.473\tvalid_1's l1: 0.119207\tvalid_1's myFeval: 526.366\n",
      "[2400]\ttraining's l1: 0.109164\ttraining's myFeval: 462.399\tvalid_1's l1: 0.118164\tvalid_1's myFeval: 519.478\n",
      "[2700]\ttraining's l1: 0.108057\ttraining's myFeval: 455.197\tvalid_1's l1: 0.117366\tvalid_1's myFeval: 514.365\n",
      "[3000]\ttraining's l1: 0.106971\ttraining's myFeval: 448.164\tvalid_1's l1: 0.116605\tvalid_1's myFeval: 509.099\n",
      "[3300]\ttraining's l1: 0.105936\ttraining's myFeval: 441.927\tvalid_1's l1: 0.115935\tvalid_1's myFeval: 504.881\n",
      "[3600]\ttraining's l1: 0.105117\ttraining's myFeval: 436.547\tvalid_1's l1: 0.115439\tvalid_1's myFeval: 501.597\n",
      "[3900]\ttraining's l1: 0.104375\ttraining's myFeval: 432.315\tvalid_1's l1: 0.115043\tvalid_1's myFeval: 499.155\n",
      "[4200]\ttraining's l1: 0.103762\ttraining's myFeval: 428.438\tvalid_1's l1: 0.114678\tvalid_1's myFeval: 496.968\n",
      "[4500]\ttraining's l1: 0.103039\ttraining's myFeval: 424.349\tvalid_1's l1: 0.114296\tvalid_1's myFeval: 494.777\n",
      "[4800]\ttraining's l1: 0.102359\ttraining's myFeval: 420.686\tvalid_1's l1: 0.113974\tvalid_1's myFeval: 492.947\n",
      "[5100]\ttraining's l1: 0.101784\ttraining's myFeval: 417.187\tvalid_1's l1: 0.1137\tvalid_1's myFeval: 491.232\n",
      "[5400]\ttraining's l1: 0.101149\ttraining's myFeval: 413.815\tvalid_1's l1: 0.113372\tvalid_1's myFeval: 489.526\n",
      "[5700]\ttraining's l1: 0.100586\ttraining's myFeval: 410.605\tvalid_1's l1: 0.113134\tvalid_1's myFeval: 488.042\n",
      "[6000]\ttraining's l1: 0.10008\ttraining's myFeval: 407.719\tvalid_1's l1: 0.112911\tvalid_1's myFeval: 486.673\n",
      "[6300]\ttraining's l1: 0.0996233\ttraining's myFeval: 404.851\tvalid_1's l1: 0.112705\tvalid_1's myFeval: 485.417\n",
      "[6600]\ttraining's l1: 0.0991532\ttraining's myFeval: 401.982\tvalid_1's l1: 0.112543\tvalid_1's myFeval: 484.307\n",
      "[6900]\ttraining's l1: 0.0986965\ttraining's myFeval: 399.5\tvalid_1's l1: 0.112412\tvalid_1's myFeval: 483.43\n",
      "[7200]\ttraining's l1: 0.098224\ttraining's myFeval: 396.972\tvalid_1's l1: 0.112279\tvalid_1's myFeval: 482.529\n",
      "[7500]\ttraining's l1: 0.0977963\ttraining's myFeval: 394.769\tvalid_1's l1: 0.112154\tvalid_1's myFeval: 481.668\n",
      "[7800]\ttraining's l1: 0.0974111\ttraining's myFeval: 392.582\tvalid_1's l1: 0.112023\tvalid_1's myFeval: 480.835\n",
      "[8100]\ttraining's l1: 0.0970438\ttraining's myFeval: 390.655\tvalid_1's l1: 0.111916\tvalid_1's myFeval: 480.245\n",
      "[8400]\ttraining's l1: 0.0966832\ttraining's myFeval: 388.639\tvalid_1's l1: 0.111797\tvalid_1's myFeval: 479.556\n",
      "[8700]\ttraining's l1: 0.0963117\ttraining's myFeval: 386.613\tvalid_1's l1: 0.111687\tvalid_1's myFeval: 478.802\n",
      "[9000]\ttraining's l1: 0.0959663\ttraining's myFeval: 384.657\tvalid_1's l1: 0.111585\tvalid_1's myFeval: 478.047\n",
      "[9300]\ttraining's l1: 0.0956027\ttraining's myFeval: 382.78\tvalid_1's l1: 0.111475\tvalid_1's myFeval: 477.525\n",
      "[9600]\ttraining's l1: 0.0952745\ttraining's myFeval: 380.971\tvalid_1's l1: 0.111387\tvalid_1's myFeval: 476.953\n",
      "[9900]\ttraining's l1: 0.0949618\ttraining's myFeval: 379.118\tvalid_1's l1: 0.11132\tvalid_1's myFeval: 476.477\n",
      "[10200]\ttraining's l1: 0.0946171\ttraining's myFeval: 377.114\tvalid_1's l1: 0.111244\tvalid_1's myFeval: 475.903\n",
      "[10500]\ttraining's l1: 0.0943054\ttraining's myFeval: 375.491\tvalid_1's l1: 0.111155\tvalid_1's myFeval: 475.485\n",
      "[10800]\ttraining's l1: 0.093974\ttraining's myFeval: 373.816\tvalid_1's l1: 0.111051\tvalid_1's myFeval: 474.956\n",
      "[11100]\ttraining's l1: 0.0937153\ttraining's myFeval: 372.401\tvalid_1's l1: 0.110996\tvalid_1's myFeval: 474.63\n",
      "[11400]\ttraining's l1: 0.0934024\ttraining's myFeval: 370.783\tvalid_1's l1: 0.110913\tvalid_1's myFeval: 474.117\n",
      "[11700]\ttraining's l1: 0.0931041\ttraining's myFeval: 369.393\tvalid_1's l1: 0.110838\tvalid_1's myFeval: 473.652\n",
      "[12000]\ttraining's l1: 0.0928558\ttraining's myFeval: 367.868\tvalid_1's l1: 0.110784\tvalid_1's myFeval: 473.352\n",
      "[12300]\ttraining's l1: 0.0926036\ttraining's myFeval: 366.324\tvalid_1's l1: 0.110737\tvalid_1's myFeval: 473.15\n",
      "[12600]\ttraining's l1: 0.0923249\ttraining's myFeval: 365.092\tvalid_1's l1: 0.11067\tvalid_1's myFeval: 472.824\n",
      "[12900]\ttraining's l1: 0.0920559\ttraining's myFeval: 363.723\tvalid_1's l1: 0.110612\tvalid_1's myFeval: 472.45\n",
      "[13200]\ttraining's l1: 0.0918202\ttraining's myFeval: 362.532\tvalid_1's l1: 0.110579\tvalid_1's myFeval: 472.274\n",
      "[13500]\ttraining's l1: 0.091603\ttraining's myFeval: 361.359\tvalid_1's l1: 0.110517\tvalid_1's myFeval: 472.007\n",
      "[13800]\ttraining's l1: 0.0913528\ttraining's myFeval: 360.134\tvalid_1's l1: 0.110474\tvalid_1's myFeval: 471.793\n",
      "[14100]\ttraining's l1: 0.0911086\ttraining's myFeval: 358.761\tvalid_1's l1: 0.110433\tvalid_1's myFeval: 471.583\n",
      "[14400]\ttraining's l1: 0.090891\ttraining's myFeval: 357.752\tvalid_1's l1: 0.1104\tvalid_1's myFeval: 471.37\n",
      "[14700]\ttraining's l1: 0.090667\ttraining's myFeval: 356.559\tvalid_1's l1: 0.110355\tvalid_1's myFeval: 471.079\n",
      "[15000]\ttraining's l1: 0.090469\ttraining's myFeval: 355.489\tvalid_1's l1: 0.110309\tvalid_1's myFeval: 470.854\n",
      "[15300]\ttraining's l1: 0.0902465\ttraining's myFeval: 354.406\tvalid_1's l1: 0.110256\tvalid_1's myFeval: 470.597\n",
      "[15600]\ttraining's l1: 0.0899991\ttraining's myFeval: 353.164\tvalid_1's l1: 0.110207\tvalid_1's myFeval: 470.348\n",
      "[15900]\ttraining's l1: 0.0897725\ttraining's myFeval: 352.022\tvalid_1's l1: 0.110166\tvalid_1's myFeval: 470.148\n",
      "[16200]\ttraining's l1: 0.0895744\ttraining's myFeval: 351.001\tvalid_1's l1: 0.110128\tvalid_1's myFeval: 469.959\n",
      "[16500]\ttraining's l1: 0.0893701\ttraining's myFeval: 350.055\tvalid_1's l1: 0.110104\tvalid_1's myFeval: 469.784\n",
      "[16800]\ttraining's l1: 0.0891529\ttraining's myFeval: 349.017\tvalid_1's l1: 0.110067\tvalid_1's myFeval: 469.557\n",
      "[17100]\ttraining's l1: 0.088916\ttraining's myFeval: 348.063\tvalid_1's l1: 0.110027\tvalid_1's myFeval: 469.363\n",
      "[17400]\ttraining's l1: 0.0887111\ttraining's myFeval: 347.004\tvalid_1's l1: 0.110002\tvalid_1's myFeval: 469.209\n",
      "[17700]\ttraining's l1: 0.0884938\ttraining's myFeval: 346.056\tvalid_1's l1: 0.109983\tvalid_1's myFeval: 469.037\n",
      "[18000]\ttraining's l1: 0.0883148\ttraining's myFeval: 345.11\tvalid_1's l1: 0.109948\tvalid_1's myFeval: 468.806\n",
      "[18300]\ttraining's l1: 0.0881445\ttraining's myFeval: 344.303\tvalid_1's l1: 0.109926\tvalid_1's myFeval: 468.694\n",
      "[18600]\ttraining's l1: 0.0879422\ttraining's myFeval: 343.415\tvalid_1's l1: 0.109897\tvalid_1's myFeval: 468.542\n",
      "[18900]\ttraining's l1: 0.0877832\ttraining's myFeval: 342.609\tvalid_1's l1: 0.10987\tvalid_1's myFeval: 468.443\n",
      "[19200]\ttraining's l1: 0.0876436\ttraining's myFeval: 341.943\tvalid_1's l1: 0.109838\tvalid_1's myFeval: 468.294\n",
      "[19500]\ttraining's l1: 0.0874621\ttraining's myFeval: 341.08\tvalid_1's l1: 0.109797\tvalid_1's myFeval: 468.077\n",
      "[19800]\ttraining's l1: 0.0873119\ttraining's myFeval: 340.379\tvalid_1's l1: 0.109762\tvalid_1's myFeval: 467.981\n",
      "[20100]\ttraining's l1: 0.0871784\ttraining's myFeval: 339.729\tvalid_1's l1: 0.109744\tvalid_1's myFeval: 467.819\n",
      "[20400]\ttraining's l1: 0.0870595\ttraining's myFeval: 339.109\tvalid_1's l1: 0.10973\tvalid_1's myFeval: 467.744\n",
      "[20700]\ttraining's l1: 0.0868977\ttraining's myFeval: 338.377\tvalid_1's l1: 0.109704\tvalid_1's myFeval: 467.684\n",
      "[21000]\ttraining's l1: 0.0867224\ttraining's myFeval: 337.589\tvalid_1's l1: 0.109682\tvalid_1's myFeval: 467.605\n",
      "[21300]\ttraining's l1: 0.0865512\ttraining's myFeval: 336.836\tvalid_1's l1: 0.109656\tvalid_1's myFeval: 467.443\n",
      "[21600]\ttraining's l1: 0.0863962\ttraining's myFeval: 336.118\tvalid_1's l1: 0.109637\tvalid_1's myFeval: 467.326\n",
      "[21900]\ttraining's l1: 0.0862445\ttraining's myFeval: 335.354\tvalid_1's l1: 0.109616\tvalid_1's myFeval: 467.167\n",
      "[22200]\ttraining's l1: 0.0861215\ttraining's myFeval: 334.757\tvalid_1's l1: 0.10959\tvalid_1's myFeval: 467.062\n",
      "[22500]\ttraining's l1: 0.0859877\ttraining's myFeval: 334.098\tvalid_1's l1: 0.109575\tvalid_1's myFeval: 466.969\n",
      "[22800]\ttraining's l1: 0.0858346\ttraining's myFeval: 333.459\tvalid_1's l1: 0.109559\tvalid_1's myFeval: 466.936\n",
      "[23100]\ttraining's l1: 0.0856905\ttraining's myFeval: 332.69\tvalid_1's l1: 0.109546\tvalid_1's myFeval: 466.815\n",
      "[23400]\ttraining's l1: 0.0855483\ttraining's myFeval: 331.955\tvalid_1's l1: 0.109527\tvalid_1's myFeval: 466.685\n",
      "[23700]\ttraining's l1: 0.0854163\ttraining's myFeval: 331.33\tvalid_1's l1: 0.109516\tvalid_1's myFeval: 466.639\n",
      "[24000]\ttraining's l1: 0.0852829\ttraining's myFeval: 330.681\tvalid_1's l1: 0.109507\tvalid_1's myFeval: 466.5\n",
      "[24300]\ttraining's l1: 0.0851706\ttraining's myFeval: 330.142\tvalid_1's l1: 0.109482\tvalid_1's myFeval: 466.387\n",
      "[24600]\ttraining's l1: 0.0850279\ttraining's myFeval: 329.526\tvalid_1's l1: 0.109466\tvalid_1's myFeval: 466.349\n",
      "[24900]\ttraining's l1: 0.0848978\ttraining's myFeval: 328.903\tvalid_1's l1: 0.109459\tvalid_1's myFeval: 466.272\n",
      "[25200]\ttraining's l1: 0.0847367\ttraining's myFeval: 328.15\tvalid_1's l1: 0.109433\tvalid_1's myFeval: 466.124\n",
      "[25500]\ttraining's l1: 0.0846056\ttraining's myFeval: 327.441\tvalid_1's l1: 0.109411\tvalid_1's myFeval: 466.059\n",
      "[25800]\ttraining's l1: 0.0844943\ttraining's myFeval: 326.89\tvalid_1's l1: 0.109396\tvalid_1's myFeval: 465.989\n",
      "[26100]\ttraining's l1: 0.0843495\ttraining's myFeval: 326.343\tvalid_1's l1: 0.109374\tvalid_1's myFeval: 465.917\n",
      "[26400]\ttraining's l1: 0.0841924\ttraining's myFeval: 325.767\tvalid_1's l1: 0.109342\tvalid_1's myFeval: 465.846\n",
      "[26700]\ttraining's l1: 0.0840617\ttraining's myFeval: 325.088\tvalid_1's l1: 0.109328\tvalid_1's myFeval: 465.729\n",
      "[27000]\ttraining's l1: 0.0839383\ttraining's myFeval: 324.405\tvalid_1's l1: 0.109314\tvalid_1's myFeval: 465.664\n",
      "[27300]\ttraining's l1: 0.08383\ttraining's myFeval: 323.847\tvalid_1's l1: 0.109305\tvalid_1's myFeval: 465.626\n",
      "[27600]\ttraining's l1: 0.0837189\ttraining's myFeval: 323.316\tvalid_1's l1: 0.109287\tvalid_1's myFeval: 465.567\n",
      "[27900]\ttraining's l1: 0.0835637\ttraining's myFeval: 322.628\tvalid_1's l1: 0.10926\tvalid_1's myFeval: 465.394\n",
      "[28200]\ttraining's l1: 0.0834474\ttraining's myFeval: 322.082\tvalid_1's l1: 0.109251\tvalid_1's myFeval: 465.337\n",
      "[28500]\ttraining's l1: 0.0833255\ttraining's myFeval: 321.533\tvalid_1's l1: 0.109235\tvalid_1's myFeval: 465.283\n",
      "[28800]\ttraining's l1: 0.0831937\ttraining's myFeval: 320.944\tvalid_1's l1: 0.109235\tvalid_1's myFeval: 465.214\n",
      "[29100]\ttraining's l1: 0.0830732\ttraining's myFeval: 320.328\tvalid_1's l1: 0.109215\tvalid_1's myFeval: 465.134\n",
      "[29400]\ttraining's l1: 0.0829531\ttraining's myFeval: 319.758\tvalid_1's l1: 0.109193\tvalid_1's myFeval: 465\n",
      "[29700]\ttraining's l1: 0.0828481\ttraining's myFeval: 319.261\tvalid_1's l1: 0.10919\tvalid_1's myFeval: 464.998\n",
      "[30000]\ttraining's l1: 0.0827329\ttraining's myFeval: 318.778\tvalid_1's l1: 0.109181\tvalid_1's myFeval: 464.934\n",
      "[30300]\ttraining's l1: 0.08262\ttraining's myFeval: 318.304\tvalid_1's l1: 0.109171\tvalid_1's myFeval: 464.839\n",
      "[30600]\ttraining's l1: 0.0824977\ttraining's myFeval: 317.712\tvalid_1's l1: 0.109159\tvalid_1's myFeval: 464.77\n",
      "[30900]\ttraining's l1: 0.0823773\ttraining's myFeval: 317.196\tvalid_1's l1: 0.109146\tvalid_1's myFeval: 464.705\n",
      "[31200]\ttraining's l1: 0.0822871\ttraining's myFeval: 316.79\tvalid_1's l1: 0.109126\tvalid_1's myFeval: 464.689\n",
      "[31500]\ttraining's l1: 0.0821975\ttraining's myFeval: 316.346\tvalid_1's l1: 0.109106\tvalid_1's myFeval: 464.599\n",
      "[31800]\ttraining's l1: 0.0820844\ttraining's myFeval: 315.867\tvalid_1's l1: 0.109098\tvalid_1's myFeval: 464.517\n",
      "[32100]\ttraining's l1: 0.0819879\ttraining's myFeval: 315.381\tvalid_1's l1: 0.109094\tvalid_1's myFeval: 464.457\n",
      "[32400]\ttraining's l1: 0.0818951\ttraining's myFeval: 314.915\tvalid_1's l1: 0.109089\tvalid_1's myFeval: 464.374\n",
      "[32700]\ttraining's l1: 0.0817995\ttraining's myFeval: 314.481\tvalid_1's l1: 0.109071\tvalid_1's myFeval: 464.29\n",
      "[33000]\ttraining's l1: 0.0817014\ttraining's myFeval: 314.073\tvalid_1's l1: 0.10907\tvalid_1's myFeval: 464.236\n",
      "[33300]\ttraining's l1: 0.0816112\ttraining's myFeval: 313.616\tvalid_1's l1: 0.109068\tvalid_1's myFeval: 464.187\n",
      "[33600]\ttraining's l1: 0.0815118\ttraining's myFeval: 313.224\tvalid_1's l1: 0.109063\tvalid_1's myFeval: 464.214\n",
      "Early stopping, best iteration is:\n",
      "[33332]\ttraining's l1: 0.0816055\ttraining's myFeval: 313.59\tvalid_1's l1: 0.109066\tvalid_1's myFeval: 464.172\n",
      "fold n°6\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.069594 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17640\n",
      "[LightGBM] [Info] Number of data points in the train set: 134999, number of used features: 83\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Info] Start training from score 8.086718\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\ttraining's l1: 0.145611\ttraining's myFeval: 758.229\tvalid_1's l1: 0.148506\tvalid_1's myFeval: 767.2\n",
      "[600]\ttraining's l1: 0.1274\ttraining's myFeval: 600.269\tvalid_1's l1: 0.131309\tvalid_1's myFeval: 612.303\n",
      "[900]\ttraining's l1: 0.120667\ttraining's myFeval: 545.996\tvalid_1's l1: 0.125212\tvalid_1's myFeval: 561.137\n",
      "[1200]\ttraining's l1: 0.117001\ttraining's myFeval: 517.702\tvalid_1's l1: 0.121983\tvalid_1's myFeval: 536.585\n",
      "[1500]\ttraining's l1: 0.114418\ttraining's myFeval: 498.286\tvalid_1's l1: 0.11985\tvalid_1's myFeval: 520.68\n",
      "[1800]\ttraining's l1: 0.112529\ttraining's myFeval: 484.261\tvalid_1's l1: 0.118364\tvalid_1's myFeval: 510.232\n",
      "[2100]\ttraining's l1: 0.110889\ttraining's myFeval: 472.638\tvalid_1's l1: 0.117152\tvalid_1's myFeval: 502.296\n",
      "[2400]\ttraining's l1: 0.109535\ttraining's myFeval: 463.039\tvalid_1's l1: 0.116199\tvalid_1's myFeval: 496.36\n",
      "[2700]\ttraining's l1: 0.108323\ttraining's myFeval: 454.984\tvalid_1's l1: 0.115418\tvalid_1's myFeval: 491.587\n",
      "[3000]\ttraining's l1: 0.107367\ttraining's myFeval: 448.607\tvalid_1's l1: 0.114814\tvalid_1's myFeval: 487.871\n",
      "[3300]\ttraining's l1: 0.106435\ttraining's myFeval: 442.849\tvalid_1's l1: 0.114275\tvalid_1's myFeval: 484.902\n",
      "[3600]\ttraining's l1: 0.105553\ttraining's myFeval: 437.523\tvalid_1's l1: 0.11376\tvalid_1's myFeval: 482.02\n",
      "[3900]\ttraining's l1: 0.104779\ttraining's myFeval: 432.482\tvalid_1's l1: 0.11333\tvalid_1's myFeval: 479.619\n",
      "[4200]\ttraining's l1: 0.104105\ttraining's myFeval: 428.157\tvalid_1's l1: 0.11296\tvalid_1's myFeval: 477.498\n",
      "[4500]\ttraining's l1: 0.103482\ttraining's myFeval: 424.438\tvalid_1's l1: 0.112652\tvalid_1's myFeval: 475.776\n",
      "[4800]\ttraining's l1: 0.102825\ttraining's myFeval: 420.77\tvalid_1's l1: 0.112322\tvalid_1's myFeval: 473.957\n",
      "[5100]\ttraining's l1: 0.102202\ttraining's myFeval: 417.107\tvalid_1's l1: 0.112025\tvalid_1's myFeval: 472.291\n",
      "[5400]\ttraining's l1: 0.101633\ttraining's myFeval: 414.058\tvalid_1's l1: 0.111777\tvalid_1's myFeval: 471.013\n",
      "[5700]\ttraining's l1: 0.101085\ttraining's myFeval: 411.171\tvalid_1's l1: 0.111571\tvalid_1's myFeval: 469.89\n",
      "[6000]\ttraining's l1: 0.100566\ttraining's myFeval: 407.9\tvalid_1's l1: 0.111344\tvalid_1's myFeval: 468.541\n",
      "[6300]\ttraining's l1: 0.10004\ttraining's myFeval: 405.296\tvalid_1's l1: 0.111116\tvalid_1's myFeval: 467.608\n",
      "[6600]\ttraining's l1: 0.0995848\ttraining's myFeval: 402.675\tvalid_1's l1: 0.110929\tvalid_1's myFeval: 466.611\n",
      "[6900]\ttraining's l1: 0.0990994\ttraining's myFeval: 399.872\tvalid_1's l1: 0.11075\tvalid_1's myFeval: 465.693\n",
      "[7200]\ttraining's l1: 0.0987075\ttraining's myFeval: 397.393\tvalid_1's l1: 0.110621\tvalid_1's myFeval: 464.988\n",
      "[7500]\ttraining's l1: 0.0982486\ttraining's myFeval: 394.914\tvalid_1's l1: 0.11047\tvalid_1's myFeval: 464.383\n",
      "[7800]\ttraining's l1: 0.0978276\ttraining's myFeval: 392.649\tvalid_1's l1: 0.110315\tvalid_1's myFeval: 463.622\n",
      "[8100]\ttraining's l1: 0.0974423\ttraining's myFeval: 390.353\tvalid_1's l1: 0.110193\tvalid_1's myFeval: 462.785\n",
      "[8400]\ttraining's l1: 0.0970643\ttraining's myFeval: 388.318\tvalid_1's l1: 0.110093\tvalid_1's myFeval: 462.186\n",
      "[8700]\ttraining's l1: 0.0967341\ttraining's myFeval: 386.6\tvalid_1's l1: 0.109993\tvalid_1's myFeval: 461.703\n",
      "[9000]\ttraining's l1: 0.0963967\ttraining's myFeval: 384.523\tvalid_1's l1: 0.109888\tvalid_1's myFeval: 461.118\n",
      "[9300]\ttraining's l1: 0.096077\ttraining's myFeval: 382.668\tvalid_1's l1: 0.109822\tvalid_1's myFeval: 460.599\n",
      "[9600]\ttraining's l1: 0.0957146\ttraining's myFeval: 380.903\tvalid_1's l1: 0.109738\tvalid_1's myFeval: 460.039\n",
      "[9900]\ttraining's l1: 0.0953812\ttraining's myFeval: 379.176\tvalid_1's l1: 0.109633\tvalid_1's myFeval: 459.515\n",
      "[10200]\ttraining's l1: 0.0950764\ttraining's myFeval: 377.471\tvalid_1's l1: 0.109582\tvalid_1's myFeval: 459.197\n",
      "[10500]\ttraining's l1: 0.0947756\ttraining's myFeval: 375.999\tvalid_1's l1: 0.109493\tvalid_1's myFeval: 458.62\n",
      "[10800]\ttraining's l1: 0.0944767\ttraining's myFeval: 374.514\tvalid_1's l1: 0.109437\tvalid_1's myFeval: 458.229\n",
      "[11100]\ttraining's l1: 0.0942012\ttraining's myFeval: 373.021\tvalid_1's l1: 0.109371\tvalid_1's myFeval: 457.809\n",
      "[11400]\ttraining's l1: 0.0939163\ttraining's myFeval: 371.631\tvalid_1's l1: 0.1093\tvalid_1's myFeval: 457.445\n",
      "[11700]\ttraining's l1: 0.0936422\ttraining's myFeval: 370.179\tvalid_1's l1: 0.109251\tvalid_1's myFeval: 457.122\n",
      "[12000]\ttraining's l1: 0.0933874\ttraining's myFeval: 368.856\tvalid_1's l1: 0.109196\tvalid_1's myFeval: 456.743\n",
      "[12300]\ttraining's l1: 0.0930908\ttraining's myFeval: 367.411\tvalid_1's l1: 0.109122\tvalid_1's myFeval: 456.317\n",
      "[12600]\ttraining's l1: 0.0928187\ttraining's myFeval: 366.186\tvalid_1's l1: 0.109072\tvalid_1's myFeval: 456.039\n",
      "[12900]\ttraining's l1: 0.0925778\ttraining's myFeval: 364.964\tvalid_1's l1: 0.109018\tvalid_1's myFeval: 455.77\n",
      "[13200]\ttraining's l1: 0.092324\ttraining's myFeval: 363.711\tvalid_1's l1: 0.108959\tvalid_1's myFeval: 455.437\n",
      "[13500]\ttraining's l1: 0.0920973\ttraining's myFeval: 362.587\tvalid_1's l1: 0.108928\tvalid_1's myFeval: 455.206\n",
      "[13800]\ttraining's l1: 0.0918711\ttraining's myFeval: 361.383\tvalid_1's l1: 0.108887\tvalid_1's myFeval: 454.888\n",
      "[14100]\ttraining's l1: 0.0916539\ttraining's myFeval: 360.118\tvalid_1's l1: 0.108847\tvalid_1's myFeval: 454.609\n",
      "[14400]\ttraining's l1: 0.0914147\ttraining's myFeval: 358.899\tvalid_1's l1: 0.108795\tvalid_1's myFeval: 454.244\n",
      "[14700]\ttraining's l1: 0.0911513\ttraining's myFeval: 357.585\tvalid_1's l1: 0.108728\tvalid_1's myFeval: 453.949\n",
      "[15000]\ttraining's l1: 0.0909064\ttraining's myFeval: 356.279\tvalid_1's l1: 0.108676\tvalid_1's myFeval: 453.592\n",
      "[15300]\ttraining's l1: 0.0906883\ttraining's myFeval: 355.129\tvalid_1's l1: 0.108633\tvalid_1's myFeval: 453.323\n",
      "[15600]\ttraining's l1: 0.0905058\ttraining's myFeval: 354.136\tvalid_1's l1: 0.1086\tvalid_1's myFeval: 453.074\n",
      "[15900]\ttraining's l1: 0.0902935\ttraining's myFeval: 353.134\tvalid_1's l1: 0.108568\tvalid_1's myFeval: 452.866\n",
      "[16200]\ttraining's l1: 0.0900743\ttraining's myFeval: 352.234\tvalid_1's l1: 0.108526\tvalid_1's myFeval: 452.712\n",
      "[16500]\ttraining's l1: 0.0898337\ttraining's myFeval: 351.059\tvalid_1's l1: 0.108489\tvalid_1's myFeval: 452.529\n",
      "[16800]\ttraining's l1: 0.0896441\ttraining's myFeval: 350.099\tvalid_1's l1: 0.10845\tvalid_1's myFeval: 452.378\n",
      "[17100]\ttraining's l1: 0.0894582\ttraining's myFeval: 349.247\tvalid_1's l1: 0.108432\tvalid_1's myFeval: 452.305\n",
      "[17400]\ttraining's l1: 0.0892239\ttraining's myFeval: 348.241\tvalid_1's l1: 0.108414\tvalid_1's myFeval: 452.178\n",
      "[17700]\ttraining's l1: 0.0890523\ttraining's myFeval: 347.348\tvalid_1's l1: 0.108395\tvalid_1's myFeval: 452.001\n",
      "[18000]\ttraining's l1: 0.0888822\ttraining's myFeval: 346.554\tvalid_1's l1: 0.108385\tvalid_1's myFeval: 451.853\n",
      "[18300]\ttraining's l1: 0.0887054\ttraining's myFeval: 345.68\tvalid_1's l1: 0.108359\tvalid_1's myFeval: 451.709\n",
      "[18600]\ttraining's l1: 0.0885191\ttraining's myFeval: 344.676\tvalid_1's l1: 0.108316\tvalid_1's myFeval: 451.529\n",
      "[18900]\ttraining's l1: 0.0883005\ttraining's myFeval: 343.625\tvalid_1's l1: 0.108272\tvalid_1's myFeval: 451.31\n",
      "[19200]\ttraining's l1: 0.0881184\ttraining's myFeval: 342.796\tvalid_1's l1: 0.10824\tvalid_1's myFeval: 451.153\n",
      "[19500]\ttraining's l1: 0.0879362\ttraining's myFeval: 341.954\tvalid_1's l1: 0.108221\tvalid_1's myFeval: 451.047\n",
      "[19800]\ttraining's l1: 0.0877631\ttraining's myFeval: 341.196\tvalid_1's l1: 0.1082\tvalid_1's myFeval: 450.939\n",
      "[20100]\ttraining's l1: 0.0875846\ttraining's myFeval: 340.405\tvalid_1's l1: 0.108182\tvalid_1's myFeval: 450.758\n",
      "[20400]\ttraining's l1: 0.087359\ttraining's myFeval: 339.567\tvalid_1's l1: 0.108155\tvalid_1's myFeval: 450.579\n",
      "[20700]\ttraining's l1: 0.0871826\ttraining's myFeval: 338.845\tvalid_1's l1: 0.108131\tvalid_1's myFeval: 450.459\n",
      "[21000]\ttraining's l1: 0.0869994\ttraining's myFeval: 338.095\tvalid_1's l1: 0.108112\tvalid_1's myFeval: 450.328\n",
      "[21300]\ttraining's l1: 0.0868439\ttraining's myFeval: 337.267\tvalid_1's l1: 0.1081\tvalid_1's myFeval: 450.225\n",
      "[21600]\ttraining's l1: 0.0867205\ttraining's myFeval: 336.709\tvalid_1's l1: 0.108083\tvalid_1's myFeval: 450.147\n",
      "Early stopping, best iteration is:\n",
      "[21596]\ttraining's l1: 0.0867225\ttraining's myFeval: 336.713\tvalid_1's l1: 0.108081\tvalid_1's myFeval: 450.145\n",
      "fold n°7\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.061687 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17637\n",
      "[LightGBM] [Info] Number of data points in the train set: 134999, number of used features: 83\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Info] Start training from score 8.086718\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\ttraining's l1: 0.14607\ttraining's myFeval: 759.88\tvalid_1's l1: 0.147\tvalid_1's myFeval: 771.951\n",
      "[600]\ttraining's l1: 0.127629\ttraining's myFeval: 599.647\tvalid_1's l1: 0.12877\tvalid_1's myFeval: 615.853\n",
      "[900]\ttraining's l1: 0.120863\ttraining's myFeval: 546.175\tvalid_1's l1: 0.122522\tvalid_1's myFeval: 567.221\n",
      "[1200]\ttraining's l1: 0.117139\ttraining's myFeval: 518.596\tvalid_1's l1: 0.119294\tvalid_1's myFeval: 542.71\n",
      "[1500]\ttraining's l1: 0.114592\ttraining's myFeval: 499.253\tvalid_1's l1: 0.117207\tvalid_1's myFeval: 526.527\n",
      "[1800]\ttraining's l1: 0.112747\ttraining's myFeval: 485.539\tvalid_1's l1: 0.115695\tvalid_1's myFeval: 515.34\n",
      "[2100]\ttraining's l1: 0.11119\ttraining's myFeval: 474.086\tvalid_1's l1: 0.114523\tvalid_1's myFeval: 506.908\n",
      "[2400]\ttraining's l1: 0.109861\ttraining's myFeval: 464.799\tvalid_1's l1: 0.113596\tvalid_1's myFeval: 500.472\n",
      "[2700]\ttraining's l1: 0.108776\ttraining's myFeval: 457.267\tvalid_1's l1: 0.112893\tvalid_1's myFeval: 495.534\n",
      "[3000]\ttraining's l1: 0.107751\ttraining's myFeval: 450.199\tvalid_1's l1: 0.112274\tvalid_1's myFeval: 490.839\n",
      "[3300]\ttraining's l1: 0.10681\ttraining's myFeval: 444.53\tvalid_1's l1: 0.111718\tvalid_1's myFeval: 487.388\n",
      "[3600]\ttraining's l1: 0.105949\ttraining's myFeval: 439.096\tvalid_1's l1: 0.111226\tvalid_1's myFeval: 483.92\n",
      "[3900]\ttraining's l1: 0.105117\ttraining's myFeval: 433.922\tvalid_1's l1: 0.110763\tvalid_1's myFeval: 480.827\n",
      "[4200]\ttraining's l1: 0.104322\ttraining's myFeval: 429.185\tvalid_1's l1: 0.110335\tvalid_1's myFeval: 478.08\n",
      "[4500]\ttraining's l1: 0.103652\ttraining's myFeval: 425.324\tvalid_1's l1: 0.110013\tvalid_1's myFeval: 476.111\n",
      "[4800]\ttraining's l1: 0.103075\ttraining's myFeval: 421.616\tvalid_1's l1: 0.109732\tvalid_1's myFeval: 474.214\n",
      "[5100]\ttraining's l1: 0.102475\ttraining's myFeval: 418.077\tvalid_1's l1: 0.109472\tvalid_1's myFeval: 472.343\n",
      "[5400]\ttraining's l1: 0.101842\ttraining's myFeval: 414.568\tvalid_1's l1: 0.109207\tvalid_1's myFeval: 470.826\n",
      "[5700]\ttraining's l1: 0.1013\ttraining's myFeval: 411.371\tvalid_1's l1: 0.108994\tvalid_1's myFeval: 469.549\n",
      "[6000]\ttraining's l1: 0.1008\ttraining's myFeval: 408.478\tvalid_1's l1: 0.108805\tvalid_1's myFeval: 468.324\n",
      "[6300]\ttraining's l1: 0.100305\ttraining's myFeval: 405.397\tvalid_1's l1: 0.108618\tvalid_1's myFeval: 467.17\n",
      "[6600]\ttraining's l1: 0.0997937\ttraining's myFeval: 402.539\tvalid_1's l1: 0.108423\tvalid_1's myFeval: 466.039\n",
      "[6900]\ttraining's l1: 0.0993789\ttraining's myFeval: 400.204\tvalid_1's l1: 0.108271\tvalid_1's myFeval: 465.076\n",
      "[7200]\ttraining's l1: 0.0989236\ttraining's myFeval: 397.772\tvalid_1's l1: 0.10811\tvalid_1's myFeval: 464.103\n",
      "[7500]\ttraining's l1: 0.0984517\ttraining's myFeval: 395.355\tvalid_1's l1: 0.107958\tvalid_1's myFeval: 463.151\n",
      "[7800]\ttraining's l1: 0.0981139\ttraining's myFeval: 393.462\tvalid_1's l1: 0.10785\tvalid_1's myFeval: 462.439\n",
      "[8100]\ttraining's l1: 0.097711\ttraining's myFeval: 391.436\tvalid_1's l1: 0.107726\tvalid_1's myFeval: 461.612\n",
      "[8400]\ttraining's l1: 0.0973364\ttraining's myFeval: 389.421\tvalid_1's l1: 0.107597\tvalid_1's myFeval: 460.819\n",
      "[8700]\ttraining's l1: 0.0969302\ttraining's myFeval: 387.401\tvalid_1's l1: 0.107463\tvalid_1's myFeval: 460.144\n",
      "[9000]\ttraining's l1: 0.0965852\ttraining's myFeval: 385.628\tvalid_1's l1: 0.107354\tvalid_1's myFeval: 459.515\n",
      "[9300]\ttraining's l1: 0.0962238\ttraining's myFeval: 383.63\tvalid_1's l1: 0.107242\tvalid_1's myFeval: 458.838\n",
      "[9600]\ttraining's l1: 0.0958676\ttraining's myFeval: 381.914\tvalid_1's l1: 0.107154\tvalid_1's myFeval: 458.309\n",
      "[9900]\ttraining's l1: 0.0955841\ttraining's myFeval: 380.445\tvalid_1's l1: 0.107095\tvalid_1's myFeval: 457.954\n",
      "[10200]\ttraining's l1: 0.0952802\ttraining's myFeval: 378.806\tvalid_1's l1: 0.107026\tvalid_1's myFeval: 457.638\n",
      "[10500]\ttraining's l1: 0.0950103\ttraining's myFeval: 377.319\tvalid_1's l1: 0.106955\tvalid_1's myFeval: 457.153\n",
      "[10800]\ttraining's l1: 0.0947131\ttraining's myFeval: 375.922\tvalid_1's l1: 0.106895\tvalid_1's myFeval: 456.75\n",
      "[11100]\ttraining's l1: 0.0944526\ttraining's myFeval: 374.517\tvalid_1's l1: 0.10683\tvalid_1's myFeval: 456.448\n",
      "[11400]\ttraining's l1: 0.0941979\ttraining's myFeval: 373.01\tvalid_1's l1: 0.106788\tvalid_1's myFeval: 456.125\n",
      "[11700]\ttraining's l1: 0.0939234\ttraining's myFeval: 371.571\tvalid_1's l1: 0.106723\tvalid_1's myFeval: 455.78\n",
      "[12000]\ttraining's l1: 0.0936321\ttraining's myFeval: 369.938\tvalid_1's l1: 0.106657\tvalid_1's myFeval: 455.37\n",
      "[12300]\ttraining's l1: 0.0933538\ttraining's myFeval: 368.567\tvalid_1's l1: 0.106594\tvalid_1's myFeval: 454.995\n",
      "[12600]\ttraining's l1: 0.0930989\ttraining's myFeval: 367.302\tvalid_1's l1: 0.106526\tvalid_1's myFeval: 454.653\n",
      "[12900]\ttraining's l1: 0.0928298\ttraining's myFeval: 366.035\tvalid_1's l1: 0.106493\tvalid_1's myFeval: 454.398\n",
      "[13200]\ttraining's l1: 0.0925625\ttraining's myFeval: 364.817\tvalid_1's l1: 0.106439\tvalid_1's myFeval: 454.059\n",
      "[13500]\ttraining's l1: 0.0923388\ttraining's myFeval: 363.536\tvalid_1's l1: 0.106394\tvalid_1's myFeval: 453.708\n",
      "[13800]\ttraining's l1: 0.0921117\ttraining's myFeval: 362.321\tvalid_1's l1: 0.106357\tvalid_1's myFeval: 453.482\n",
      "[14100]\ttraining's l1: 0.0918714\ttraining's myFeval: 361.101\tvalid_1's l1: 0.106303\tvalid_1's myFeval: 453.201\n",
      "[14400]\ttraining's l1: 0.0916697\ttraining's myFeval: 360.034\tvalid_1's l1: 0.106284\tvalid_1's myFeval: 453\n",
      "[14700]\ttraining's l1: 0.0914667\ttraining's myFeval: 359.082\tvalid_1's l1: 0.106243\tvalid_1's myFeval: 452.783\n",
      "[15000]\ttraining's l1: 0.0912599\ttraining's myFeval: 357.905\tvalid_1's l1: 0.106203\tvalid_1's myFeval: 452.482\n",
      "[15300]\ttraining's l1: 0.0910453\ttraining's myFeval: 356.757\tvalid_1's l1: 0.106167\tvalid_1's myFeval: 452.216\n",
      "[15600]\ttraining's l1: 0.0908022\ttraining's myFeval: 355.739\tvalid_1's l1: 0.106128\tvalid_1's myFeval: 451.996\n",
      "[15900]\ttraining's l1: 0.0906063\ttraining's myFeval: 354.833\tvalid_1's l1: 0.106092\tvalid_1's myFeval: 451.815\n",
      "[16200]\ttraining's l1: 0.0903938\ttraining's myFeval: 353.685\tvalid_1's l1: 0.106052\tvalid_1's myFeval: 451.625\n",
      "[16500]\ttraining's l1: 0.090169\ttraining's myFeval: 352.627\tvalid_1's l1: 0.106013\tvalid_1's myFeval: 451.433\n",
      "[16800]\ttraining's l1: 0.0899856\ttraining's myFeval: 351.777\tvalid_1's l1: 0.105971\tvalid_1's myFeval: 451.234\n",
      "[17100]\ttraining's l1: 0.0897965\ttraining's myFeval: 350.885\tvalid_1's l1: 0.105941\tvalid_1's myFeval: 451.073\n",
      "[17400]\ttraining's l1: 0.0896101\ttraining's myFeval: 349.915\tvalid_1's l1: 0.105907\tvalid_1's myFeval: 450.955\n",
      "[17700]\ttraining's l1: 0.0893885\ttraining's myFeval: 348.986\tvalid_1's l1: 0.105866\tvalid_1's myFeval: 450.801\n",
      "[18000]\ttraining's l1: 0.0892091\ttraining's myFeval: 347.914\tvalid_1's l1: 0.10583\tvalid_1's myFeval: 450.555\n",
      "[18300]\ttraining's l1: 0.0890456\ttraining's myFeval: 347.087\tvalid_1's l1: 0.105807\tvalid_1's myFeval: 450.367\n",
      "[18600]\ttraining's l1: 0.0888407\ttraining's myFeval: 346.236\tvalid_1's l1: 0.105777\tvalid_1's myFeval: 450.228\n",
      "[18900]\ttraining's l1: 0.0886595\ttraining's myFeval: 345.451\tvalid_1's l1: 0.105747\tvalid_1's myFeval: 450.108\n",
      "[19200]\ttraining's l1: 0.0884739\ttraining's myFeval: 344.664\tvalid_1's l1: 0.105719\tvalid_1's myFeval: 449.917\n",
      "[19500]\ttraining's l1: 0.0883128\ttraining's myFeval: 343.945\tvalid_1's l1: 0.105695\tvalid_1's myFeval: 449.786\n",
      "[19800]\ttraining's l1: 0.0881468\ttraining's myFeval: 343.192\tvalid_1's l1: 0.105668\tvalid_1's myFeval: 449.664\n",
      "[20100]\ttraining's l1: 0.0879583\ttraining's myFeval: 342.462\tvalid_1's l1: 0.105621\tvalid_1's myFeval: 449.513\n",
      "[20400]\ttraining's l1: 0.0878054\ttraining's myFeval: 341.757\tvalid_1's l1: 0.105604\tvalid_1's myFeval: 449.437\n",
      "[20700]\ttraining's l1: 0.0876578\ttraining's myFeval: 341.03\tvalid_1's l1: 0.105576\tvalid_1's myFeval: 449.259\n",
      "[21000]\ttraining's l1: 0.0874834\ttraining's myFeval: 340.203\tvalid_1's l1: 0.105555\tvalid_1's myFeval: 449.189\n",
      "[21300]\ttraining's l1: 0.087332\ttraining's myFeval: 339.42\tvalid_1's l1: 0.105526\tvalid_1's myFeval: 449.054\n",
      "[21600]\ttraining's l1: 0.0871987\ttraining's myFeval: 338.721\tvalid_1's l1: 0.105497\tvalid_1's myFeval: 448.89\n",
      "[21900]\ttraining's l1: 0.087034\ttraining's myFeval: 338.062\tvalid_1's l1: 0.10548\tvalid_1's myFeval: 448.721\n",
      "[22200]\ttraining's l1: 0.0868474\ttraining's myFeval: 337.165\tvalid_1's l1: 0.105465\tvalid_1's myFeval: 448.608\n",
      "[22500]\ttraining's l1: 0.0866703\ttraining's myFeval: 336.411\tvalid_1's l1: 0.105455\tvalid_1's myFeval: 448.549\n",
      "[22800]\ttraining's l1: 0.0864876\ttraining's myFeval: 335.482\tvalid_1's l1: 0.10544\tvalid_1's myFeval: 448.413\n",
      "[23100]\ttraining's l1: 0.0863392\ttraining's myFeval: 334.739\tvalid_1's l1: 0.105424\tvalid_1's myFeval: 448.346\n",
      "[23400]\ttraining's l1: 0.0861937\ttraining's myFeval: 334.071\tvalid_1's l1: 0.105411\tvalid_1's myFeval: 448.221\n",
      "[23700]\ttraining's l1: 0.0860642\ttraining's myFeval: 333.383\tvalid_1's l1: 0.105385\tvalid_1's myFeval: 448.065\n",
      "[24000]\ttraining's l1: 0.0859184\ttraining's myFeval: 332.708\tvalid_1's l1: 0.105367\tvalid_1's myFeval: 448.031\n",
      "[24300]\ttraining's l1: 0.0857924\ttraining's myFeval: 332.116\tvalid_1's l1: 0.105349\tvalid_1's myFeval: 447.869\n",
      "[24600]\ttraining's l1: 0.0856763\ttraining's myFeval: 331.491\tvalid_1's l1: 0.105329\tvalid_1's myFeval: 447.74\n",
      "[24900]\ttraining's l1: 0.0855343\ttraining's myFeval: 330.903\tvalid_1's l1: 0.105302\tvalid_1's myFeval: 447.561\n",
      "[25200]\ttraining's l1: 0.0853991\ttraining's myFeval: 330.29\tvalid_1's l1: 0.105286\tvalid_1's myFeval: 447.446\n",
      "[25500]\ttraining's l1: 0.085245\ttraining's myFeval: 329.677\tvalid_1's l1: 0.105271\tvalid_1's myFeval: 447.383\n",
      "[25800]\ttraining's l1: 0.0851185\ttraining's myFeval: 329.017\tvalid_1's l1: 0.105253\tvalid_1's myFeval: 447.265\n",
      "[26100]\ttraining's l1: 0.0849683\ttraining's myFeval: 328.321\tvalid_1's l1: 0.105259\tvalid_1's myFeval: 447.243\n",
      "[26400]\ttraining's l1: 0.0848217\ttraining's myFeval: 327.648\tvalid_1's l1: 0.105245\tvalid_1's myFeval: 447.131\n",
      "[26700]\ttraining's l1: 0.0846874\ttraining's myFeval: 327.044\tvalid_1's l1: 0.105229\tvalid_1's myFeval: 446.973\n",
      "[27000]\ttraining's l1: 0.0845498\ttraining's myFeval: 326.424\tvalid_1's l1: 0.105209\tvalid_1's myFeval: 446.93\n",
      "[27300]\ttraining's l1: 0.0844214\ttraining's myFeval: 325.808\tvalid_1's l1: 0.105187\tvalid_1's myFeval: 446.793\n",
      "[27600]\ttraining's l1: 0.0843052\ttraining's myFeval: 325.261\tvalid_1's l1: 0.105172\tvalid_1's myFeval: 446.701\n",
      "[27900]\ttraining's l1: 0.0841949\ttraining's myFeval: 324.761\tvalid_1's l1: 0.105148\tvalid_1's myFeval: 446.598\n",
      "[28200]\ttraining's l1: 0.0840443\ttraining's myFeval: 324.123\tvalid_1's l1: 0.105133\tvalid_1's myFeval: 446.543\n",
      "[28500]\ttraining's l1: 0.0839228\ttraining's myFeval: 323.556\tvalid_1's l1: 0.105124\tvalid_1's myFeval: 446.468\n",
      "[28800]\ttraining's l1: 0.0838023\ttraining's myFeval: 323.034\tvalid_1's l1: 0.105115\tvalid_1's myFeval: 446.413\n",
      "[29100]\ttraining's l1: 0.0836803\ttraining's myFeval: 322.436\tvalid_1's l1: 0.105096\tvalid_1's myFeval: 446.284\n",
      "[29400]\ttraining's l1: 0.0835658\ttraining's myFeval: 321.995\tvalid_1's l1: 0.105087\tvalid_1's myFeval: 446.216\n",
      "[29700]\ttraining's l1: 0.0834496\ttraining's myFeval: 321.484\tvalid_1's l1: 0.105083\tvalid_1's myFeval: 446.136\n",
      "[30000]\ttraining's l1: 0.0833347\ttraining's myFeval: 320.951\tvalid_1's l1: 0.105074\tvalid_1's myFeval: 446.031\n",
      "[30300]\ttraining's l1: 0.0832139\ttraining's myFeval: 320.39\tvalid_1's l1: 0.105063\tvalid_1's myFeval: 445.946\n",
      "[30600]\ttraining's l1: 0.0830924\ttraining's myFeval: 319.885\tvalid_1's l1: 0.105042\tvalid_1's myFeval: 445.863\n",
      "[30900]\ttraining's l1: 0.0829529\ttraining's myFeval: 319.356\tvalid_1's l1: 0.105026\tvalid_1's myFeval: 445.794\n",
      "[31200]\ttraining's l1: 0.0828157\ttraining's myFeval: 318.749\tvalid_1's l1: 0.105016\tvalid_1's myFeval: 445.708\n",
      "[31500]\ttraining's l1: 0.0827168\ttraining's myFeval: 318.309\tvalid_1's l1: 0.105005\tvalid_1's myFeval: 445.674\n",
      "[31800]\ttraining's l1: 0.0825864\ttraining's myFeval: 317.744\tvalid_1's l1: 0.104983\tvalid_1's myFeval: 445.51\n",
      "[32100]\ttraining's l1: 0.0824827\ttraining's myFeval: 317.204\tvalid_1's l1: 0.104971\tvalid_1's myFeval: 445.429\n",
      "[32400]\ttraining's l1: 0.0823876\ttraining's myFeval: 316.738\tvalid_1's l1: 0.104968\tvalid_1's myFeval: 445.368\n",
      "[32700]\ttraining's l1: 0.0822682\ttraining's myFeval: 316.257\tvalid_1's l1: 0.104952\tvalid_1's myFeval: 445.286\n",
      "[33000]\ttraining's l1: 0.0821574\ttraining's myFeval: 315.794\tvalid_1's l1: 0.104944\tvalid_1's myFeval: 445.24\n",
      "[33300]\ttraining's l1: 0.0820375\ttraining's myFeval: 315.301\tvalid_1's l1: 0.104939\tvalid_1's myFeval: 445.176\n",
      "[33600]\ttraining's l1: 0.0819339\ttraining's myFeval: 314.854\tvalid_1's l1: 0.10493\tvalid_1's myFeval: 445.11\n",
      "[33900]\ttraining's l1: 0.0818368\ttraining's myFeval: 314.385\tvalid_1's l1: 0.10492\tvalid_1's myFeval: 445.06\n",
      "[34200]\ttraining's l1: 0.0817453\ttraining's myFeval: 313.995\tvalid_1's l1: 0.104908\tvalid_1's myFeval: 444.999\n",
      "[34500]\ttraining's l1: 0.0816475\ttraining's myFeval: 313.515\tvalid_1's l1: 0.1049\tvalid_1's myFeval: 444.937\n",
      "[34800]\ttraining's l1: 0.0815451\ttraining's myFeval: 312.985\tvalid_1's l1: 0.10489\tvalid_1's myFeval: 444.849\n",
      "[35100]\ttraining's l1: 0.0814351\ttraining's myFeval: 312.516\tvalid_1's l1: 0.104864\tvalid_1's myFeval: 444.761\n",
      "[35400]\ttraining's l1: 0.0813288\ttraining's myFeval: 312.145\tvalid_1's l1: 0.104862\tvalid_1's myFeval: 444.788\n",
      "[35700]\ttraining's l1: 0.0812471\ttraining's myFeval: 311.798\tvalid_1's l1: 0.104845\tvalid_1's myFeval: 444.708\n",
      "[36000]\ttraining's l1: 0.0811662\ttraining's myFeval: 311.417\tvalid_1's l1: 0.10484\tvalid_1's myFeval: 444.665\n",
      "[36300]\ttraining's l1: 0.0810696\ttraining's myFeval: 310.987\tvalid_1's l1: 0.104829\tvalid_1's myFeval: 444.589\n",
      "[36600]\ttraining's l1: 0.0809683\ttraining's myFeval: 310.477\tvalid_1's l1: 0.10481\tvalid_1's myFeval: 444.516\n",
      "[36900]\ttraining's l1: 0.0808615\ttraining's myFeval: 310.068\tvalid_1's l1: 0.104793\tvalid_1's myFeval: 444.49\n",
      "[37200]\ttraining's l1: 0.0807692\ttraining's myFeval: 309.667\tvalid_1's l1: 0.10478\tvalid_1's myFeval: 444.432\n",
      "[37500]\ttraining's l1: 0.0806656\ttraining's myFeval: 309.259\tvalid_1's l1: 0.104783\tvalid_1's myFeval: 444.406\n",
      "Early stopping, best iteration is:\n",
      "[37278]\ttraining's l1: 0.0807413\ttraining's myFeval: 309.566\tvalid_1's l1: 0.104778\tvalid_1's myFeval: 444.412\n",
      "fold n°8\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.043897 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17643\n",
      "[LightGBM] [Info] Number of data points in the train set: 134999, number of used features: 83\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Info] Start training from score 8.086718\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\ttraining's l1: 0.146182\ttraining's myFeval: 758.066\tvalid_1's l1: 0.144407\tvalid_1's myFeval: 775.482\n",
      "[600]\ttraining's l1: 0.128151\ttraining's myFeval: 602.663\tvalid_1's l1: 0.12684\tvalid_1's myFeval: 609.741\n",
      "[900]\ttraining's l1: 0.121149\ttraining's myFeval: 549.153\tvalid_1's l1: 0.120457\tvalid_1's myFeval: 559.011\n",
      "[1200]\ttraining's l1: 0.117263\ttraining's myFeval: 518.367\tvalid_1's l1: 0.117142\tvalid_1's myFeval: 530.606\n",
      "[1500]\ttraining's l1: 0.114882\ttraining's myFeval: 499.673\tvalid_1's l1: 0.115184\tvalid_1's myFeval: 514.199\n",
      "[1800]\ttraining's l1: 0.113\ttraining's myFeval: 484.547\tvalid_1's l1: 0.113728\tvalid_1's myFeval: 501.938\n",
      "[2100]\ttraining's l1: 0.111497\ttraining's myFeval: 473.83\tvalid_1's l1: 0.112584\tvalid_1's myFeval: 493.432\n",
      "[2400]\ttraining's l1: 0.11018\ttraining's myFeval: 465.314\tvalid_1's l1: 0.111687\tvalid_1's myFeval: 487.293\n",
      "[2700]\ttraining's l1: 0.108874\ttraining's myFeval: 457.208\tvalid_1's l1: 0.110875\tvalid_1's myFeval: 481.811\n",
      "[3000]\ttraining's l1: 0.107794\ttraining's myFeval: 450.571\tvalid_1's l1: 0.110207\tvalid_1's myFeval: 477.391\n",
      "[3300]\ttraining's l1: 0.106857\ttraining's myFeval: 444.897\tvalid_1's l1: 0.109646\tvalid_1's myFeval: 473.97\n",
      "[3600]\ttraining's l1: 0.105962\ttraining's myFeval: 439.04\tvalid_1's l1: 0.109165\tvalid_1's myFeval: 470.891\n",
      "[3900]\ttraining's l1: 0.105196\ttraining's myFeval: 434.443\tvalid_1's l1: 0.108769\tvalid_1's myFeval: 468.467\n",
      "[4200]\ttraining's l1: 0.104429\ttraining's myFeval: 429.809\tvalid_1's l1: 0.10838\tvalid_1's myFeval: 466.087\n",
      "[4500]\ttraining's l1: 0.103684\ttraining's myFeval: 425.297\tvalid_1's l1: 0.108031\tvalid_1's myFeval: 464.003\n",
      "[4800]\ttraining's l1: 0.103072\ttraining's myFeval: 421.93\tvalid_1's l1: 0.107805\tvalid_1's myFeval: 462.657\n",
      "[5100]\ttraining's l1: 0.102456\ttraining's myFeval: 418.267\tvalid_1's l1: 0.107541\tvalid_1's myFeval: 461.235\n",
      "[5400]\ttraining's l1: 0.101903\ttraining's myFeval: 415.029\tvalid_1's l1: 0.107308\tvalid_1's myFeval: 459.742\n",
      "[5700]\ttraining's l1: 0.101411\ttraining's myFeval: 411.882\tvalid_1's l1: 0.107103\tvalid_1's myFeval: 458.376\n",
      "[6000]\ttraining's l1: 0.100881\ttraining's myFeval: 409.115\tvalid_1's l1: 0.10688\tvalid_1's myFeval: 457.233\n",
      "[6300]\ttraining's l1: 0.100412\ttraining's myFeval: 406.162\tvalid_1's l1: 0.10672\tvalid_1's myFeval: 456.153\n",
      "[6600]\ttraining's l1: 0.0998964\ttraining's myFeval: 403.469\tvalid_1's l1: 0.106554\tvalid_1's myFeval: 455.096\n",
      "[6900]\ttraining's l1: 0.0994951\ttraining's myFeval: 400.966\tvalid_1's l1: 0.106399\tvalid_1's myFeval: 454.154\n",
      "[7200]\ttraining's l1: 0.0990051\ttraining's myFeval: 398.325\tvalid_1's l1: 0.106229\tvalid_1's myFeval: 453.337\n",
      "[7500]\ttraining's l1: 0.0986166\ttraining's myFeval: 396.163\tvalid_1's l1: 0.106112\tvalid_1's myFeval: 452.571\n",
      "[7800]\ttraining's l1: 0.0982368\ttraining's myFeval: 394.199\tvalid_1's l1: 0.105994\tvalid_1's myFeval: 451.909\n",
      "[8100]\ttraining's l1: 0.097791\ttraining's myFeval: 391.851\tvalid_1's l1: 0.105884\tvalid_1's myFeval: 451.14\n",
      "[8400]\ttraining's l1: 0.097416\ttraining's myFeval: 389.777\tvalid_1's l1: 0.105776\tvalid_1's myFeval: 450.582\n",
      "[8700]\ttraining's l1: 0.0970323\ttraining's myFeval: 387.812\tvalid_1's l1: 0.10567\tvalid_1's myFeval: 449.946\n",
      "[9000]\ttraining's l1: 0.0966914\ttraining's myFeval: 385.392\tvalid_1's l1: 0.105583\tvalid_1's myFeval: 449.412\n",
      "[9300]\ttraining's l1: 0.096363\ttraining's myFeval: 383.787\tvalid_1's l1: 0.105499\tvalid_1's myFeval: 448.938\n",
      "[9600]\ttraining's l1: 0.0960322\ttraining's myFeval: 381.956\tvalid_1's l1: 0.105424\tvalid_1's myFeval: 448.566\n",
      "[9900]\ttraining's l1: 0.0956785\ttraining's myFeval: 380.275\tvalid_1's l1: 0.105332\tvalid_1's myFeval: 448.005\n",
      "[10200]\ttraining's l1: 0.0953684\ttraining's myFeval: 378.829\tvalid_1's l1: 0.105249\tvalid_1's myFeval: 447.647\n",
      "[10500]\ttraining's l1: 0.0950508\ttraining's myFeval: 377.027\tvalid_1's l1: 0.10517\tvalid_1's myFeval: 447.156\n",
      "[10800]\ttraining's l1: 0.0947333\ttraining's myFeval: 375.563\tvalid_1's l1: 0.105104\tvalid_1's myFeval: 446.835\n",
      "[11100]\ttraining's l1: 0.0944506\ttraining's myFeval: 374.022\tvalid_1's l1: 0.105044\tvalid_1's myFeval: 446.467\n",
      "[11400]\ttraining's l1: 0.0941176\ttraining's myFeval: 372.149\tvalid_1's l1: 0.104988\tvalid_1's myFeval: 446.16\n",
      "[11700]\ttraining's l1: 0.093794\ttraining's myFeval: 370.537\tvalid_1's l1: 0.10492\tvalid_1's myFeval: 445.867\n",
      "[12000]\ttraining's l1: 0.0935326\ttraining's myFeval: 369.145\tvalid_1's l1: 0.104879\tvalid_1's myFeval: 445.631\n",
      "[12300]\ttraining's l1: 0.093263\ttraining's myFeval: 367.781\tvalid_1's l1: 0.104832\tvalid_1's myFeval: 445.323\n",
      "[12600]\ttraining's l1: 0.0929998\ttraining's myFeval: 366.427\tvalid_1's l1: 0.104787\tvalid_1's myFeval: 445.033\n",
      "[12900]\ttraining's l1: 0.0927246\ttraining's myFeval: 365.082\tvalid_1's l1: 0.104731\tvalid_1's myFeval: 444.742\n",
      "[13200]\ttraining's l1: 0.0924873\ttraining's myFeval: 363.92\tvalid_1's l1: 0.104686\tvalid_1's myFeval: 444.456\n",
      "[13500]\ttraining's l1: 0.0922541\ttraining's myFeval: 362.773\tvalid_1's l1: 0.10464\tvalid_1's myFeval: 444.292\n",
      "[13800]\ttraining's l1: 0.0919967\ttraining's myFeval: 361.458\tvalid_1's l1: 0.104594\tvalid_1's myFeval: 444.023\n",
      "[14100]\ttraining's l1: 0.0917642\ttraining's myFeval: 360.235\tvalid_1's l1: 0.104571\tvalid_1's myFeval: 443.883\n",
      "[14400]\ttraining's l1: 0.0915251\ttraining's myFeval: 359.044\tvalid_1's l1: 0.104538\tvalid_1's myFeval: 443.639\n",
      "[14700]\ttraining's l1: 0.0912822\ttraining's myFeval: 357.85\tvalid_1's l1: 0.104503\tvalid_1's myFeval: 443.424\n",
      "[15000]\ttraining's l1: 0.0910319\ttraining's myFeval: 356.574\tvalid_1's l1: 0.104477\tvalid_1's myFeval: 443.246\n",
      "[15300]\ttraining's l1: 0.0908621\ttraining's myFeval: 355.701\tvalid_1's l1: 0.104434\tvalid_1's myFeval: 443.043\n",
      "[15600]\ttraining's l1: 0.0906344\ttraining's myFeval: 354.65\tvalid_1's l1: 0.104404\tvalid_1's myFeval: 442.889\n",
      "[15900]\ttraining's l1: 0.0904592\ttraining's myFeval: 353.744\tvalid_1's l1: 0.104377\tvalid_1's myFeval: 442.724\n",
      "[16200]\ttraining's l1: 0.0902602\ttraining's myFeval: 352.806\tvalid_1's l1: 0.104334\tvalid_1's myFeval: 442.48\n",
      "[16500]\ttraining's l1: 0.0900429\ttraining's myFeval: 351.819\tvalid_1's l1: 0.104308\tvalid_1's myFeval: 442.361\n",
      "[16800]\ttraining's l1: 0.0898439\ttraining's myFeval: 350.877\tvalid_1's l1: 0.104278\tvalid_1's myFeval: 442.091\n",
      "[17100]\ttraining's l1: 0.0896197\ttraining's myFeval: 349.813\tvalid_1's l1: 0.104249\tvalid_1's myFeval: 441.931\n",
      "[17400]\ttraining's l1: 0.0894058\ttraining's myFeval: 348.672\tvalid_1's l1: 0.104211\tvalid_1's myFeval: 441.765\n",
      "[17700]\ttraining's l1: 0.0892077\ttraining's myFeval: 347.755\tvalid_1's l1: 0.104174\tvalid_1's myFeval: 441.576\n",
      "[18000]\ttraining's l1: 0.0890155\ttraining's myFeval: 346.801\tvalid_1's l1: 0.104154\tvalid_1's myFeval: 441.505\n",
      "[18300]\ttraining's l1: 0.0888418\ttraining's myFeval: 345.922\tvalid_1's l1: 0.104128\tvalid_1's myFeval: 441.386\n",
      "[18600]\ttraining's l1: 0.0887011\ttraining's myFeval: 345.164\tvalid_1's l1: 0.104115\tvalid_1's myFeval: 441.303\n",
      "[18900]\ttraining's l1: 0.0885481\ttraining's myFeval: 344.468\tvalid_1's l1: 0.104091\tvalid_1's myFeval: 441.233\n",
      "[19200]\ttraining's l1: 0.0883686\ttraining's myFeval: 343.67\tvalid_1's l1: 0.104073\tvalid_1's myFeval: 441.138\n",
      "[19500]\ttraining's l1: 0.0882258\ttraining's myFeval: 342.941\tvalid_1's l1: 0.104046\tvalid_1's myFeval: 440.923\n",
      "[19800]\ttraining's l1: 0.0880113\ttraining's myFeval: 341.997\tvalid_1's l1: 0.104006\tvalid_1's myFeval: 440.701\n",
      "[20100]\ttraining's l1: 0.0878356\ttraining's myFeval: 341.207\tvalid_1's l1: 0.103979\tvalid_1's myFeval: 440.585\n",
      "[20400]\ttraining's l1: 0.0876555\ttraining's myFeval: 340.321\tvalid_1's l1: 0.103972\tvalid_1's myFeval: 440.507\n",
      "[20700]\ttraining's l1: 0.0874738\ttraining's myFeval: 339.492\tvalid_1's l1: 0.103949\tvalid_1's myFeval: 440.407\n",
      "[21000]\ttraining's l1: 0.0873081\ttraining's myFeval: 338.762\tvalid_1's l1: 0.103928\tvalid_1's myFeval: 440.303\n",
      "[21300]\ttraining's l1: 0.0871435\ttraining's myFeval: 337.911\tvalid_1's l1: 0.10391\tvalid_1's myFeval: 440.202\n",
      "[21600]\ttraining's l1: 0.0869882\ttraining's myFeval: 337.151\tvalid_1's l1: 0.103898\tvalid_1's myFeval: 440.089\n",
      "[21900]\ttraining's l1: 0.0868378\ttraining's myFeval: 336.523\tvalid_1's l1: 0.103879\tvalid_1's myFeval: 439.96\n",
      "[22200]\ttraining's l1: 0.0866997\ttraining's myFeval: 335.88\tvalid_1's l1: 0.103856\tvalid_1's myFeval: 439.85\n",
      "[22500]\ttraining's l1: 0.0865554\ttraining's myFeval: 335.233\tvalid_1's l1: 0.103848\tvalid_1's myFeval: 439.795\n",
      "[22800]\ttraining's l1: 0.0864047\ttraining's myFeval: 334.447\tvalid_1's l1: 0.10383\tvalid_1's myFeval: 439.669\n",
      "[23100]\ttraining's l1: 0.0862536\ttraining's myFeval: 333.752\tvalid_1's l1: 0.103809\tvalid_1's myFeval: 439.563\n",
      "[23400]\ttraining's l1: 0.0860863\ttraining's myFeval: 332.99\tvalid_1's l1: 0.10378\tvalid_1's myFeval: 439.429\n",
      "[23700]\ttraining's l1: 0.0859282\ttraining's myFeval: 332.304\tvalid_1's l1: 0.103765\tvalid_1's myFeval: 439.367\n",
      "[24000]\ttraining's l1: 0.0857961\ttraining's myFeval: 331.684\tvalid_1's l1: 0.103752\tvalid_1's myFeval: 439.314\n",
      "[24300]\ttraining's l1: 0.0856435\ttraining's myFeval: 331.019\tvalid_1's l1: 0.103735\tvalid_1's myFeval: 439.281\n",
      "[24600]\ttraining's l1: 0.0854941\ttraining's myFeval: 330.405\tvalid_1's l1: 0.103725\tvalid_1's myFeval: 439.229\n",
      "[24900]\ttraining's l1: 0.0853468\ttraining's myFeval: 329.719\tvalid_1's l1: 0.103706\tvalid_1's myFeval: 439.13\n",
      "[25200]\ttraining's l1: 0.0852235\ttraining's myFeval: 329.198\tvalid_1's l1: 0.103687\tvalid_1's myFeval: 439.113\n",
      "[25500]\ttraining's l1: 0.0851145\ttraining's myFeval: 328.637\tvalid_1's l1: 0.103672\tvalid_1's myFeval: 439.1\n",
      "[25800]\ttraining's l1: 0.0849927\ttraining's myFeval: 328.034\tvalid_1's l1: 0.103662\tvalid_1's myFeval: 439.037\n",
      "[26100]\ttraining's l1: 0.0848287\ttraining's myFeval: 327.292\tvalid_1's l1: 0.103652\tvalid_1's myFeval: 438.926\n",
      "[26400]\ttraining's l1: 0.0847122\ttraining's myFeval: 326.716\tvalid_1's l1: 0.103637\tvalid_1's myFeval: 438.871\n",
      "[26700]\ttraining's l1: 0.0845824\ttraining's myFeval: 326.207\tvalid_1's l1: 0.103618\tvalid_1's myFeval: 438.785\n",
      "[27000]\ttraining's l1: 0.0844415\ttraining's myFeval: 325.678\tvalid_1's l1: 0.103607\tvalid_1's myFeval: 438.776\n",
      "Early stopping, best iteration is:\n",
      "[26828]\ttraining's l1: 0.084527\ttraining's myFeval: 325.999\tvalid_1's l1: 0.103613\tvalid_1's myFeval: 438.75\n",
      "fold n°9\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.041741 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17640\n",
      "[LightGBM] [Info] Number of data points in the train set: 134999, number of used features: 83\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Info] Start training from score 8.086718\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\ttraining's l1: 0.146174\ttraining's myFeval: 760.639\tvalid_1's l1: 0.147592\tvalid_1's myFeval: 746.888\n",
      "[600]\ttraining's l1: 0.12766\ttraining's myFeval: 601.045\tvalid_1's l1: 0.129671\tvalid_1's myFeval: 595.716\n",
      "[900]\ttraining's l1: 0.120776\ttraining's myFeval: 546.988\tvalid_1's l1: 0.123305\tvalid_1's myFeval: 548.608\n",
      "[1200]\ttraining's l1: 0.116952\ttraining's myFeval: 517.253\tvalid_1's l1: 0.12004\tvalid_1's myFeval: 524.966\n",
      "[1500]\ttraining's l1: 0.114464\ttraining's myFeval: 498.618\tvalid_1's l1: 0.118008\tvalid_1's myFeval: 510.794\n",
      "[1800]\ttraining's l1: 0.112437\ttraining's myFeval: 483.146\tvalid_1's l1: 0.116493\tvalid_1's myFeval: 499.757\n",
      "[2100]\ttraining's l1: 0.110917\ttraining's myFeval: 472.506\tvalid_1's l1: 0.11539\tvalid_1's myFeval: 492.693\n",
      "[2400]\ttraining's l1: 0.109685\ttraining's myFeval: 464.064\tvalid_1's l1: 0.114545\tvalid_1's myFeval: 487.415\n",
      "[2700]\ttraining's l1: 0.108609\ttraining's myFeval: 456.719\tvalid_1's l1: 0.113842\tvalid_1's myFeval: 483.089\n",
      "[3000]\ttraining's l1: 0.10752\ttraining's myFeval: 449.931\tvalid_1's l1: 0.113214\tvalid_1's myFeval: 479.569\n",
      "[3300]\ttraining's l1: 0.106547\ttraining's myFeval: 443.717\tvalid_1's l1: 0.112667\tvalid_1's myFeval: 476.594\n",
      "[3600]\ttraining's l1: 0.105691\ttraining's myFeval: 438.229\tvalid_1's l1: 0.1122\tvalid_1's myFeval: 474.049\n",
      "[3900]\ttraining's l1: 0.104894\ttraining's myFeval: 433.484\tvalid_1's l1: 0.111784\tvalid_1's myFeval: 471.877\n",
      "[4200]\ttraining's l1: 0.104242\ttraining's myFeval: 429.299\tvalid_1's l1: 0.111449\tvalid_1's myFeval: 469.978\n",
      "[4500]\ttraining's l1: 0.103482\ttraining's myFeval: 425.215\tvalid_1's l1: 0.111078\tvalid_1's myFeval: 468.105\n",
      "[4800]\ttraining's l1: 0.102854\ttraining's myFeval: 421.505\tvalid_1's l1: 0.110781\tvalid_1's myFeval: 466.476\n",
      "[5100]\ttraining's l1: 0.102281\ttraining's myFeval: 417.902\tvalid_1's l1: 0.110508\tvalid_1's myFeval: 464.932\n",
      "[5400]\ttraining's l1: 0.101712\ttraining's myFeval: 414.44\tvalid_1's l1: 0.110251\tvalid_1's myFeval: 463.491\n",
      "[5700]\ttraining's l1: 0.101191\ttraining's myFeval: 411.321\tvalid_1's l1: 0.110031\tvalid_1's myFeval: 462.182\n",
      "[6000]\ttraining's l1: 0.100728\ttraining's myFeval: 408.587\tvalid_1's l1: 0.109831\tvalid_1's myFeval: 461.297\n",
      "[6300]\ttraining's l1: 0.10021\ttraining's myFeval: 405.722\tvalid_1's l1: 0.109642\tvalid_1's myFeval: 460.286\n",
      "[6600]\ttraining's l1: 0.0996756\ttraining's myFeval: 402.771\tvalid_1's l1: 0.109422\tvalid_1's myFeval: 459.331\n",
      "[6900]\ttraining's l1: 0.0992325\ttraining's myFeval: 400.197\tvalid_1's l1: 0.109254\tvalid_1's myFeval: 458.483\n",
      "[7200]\ttraining's l1: 0.098812\ttraining's myFeval: 397.906\tvalid_1's l1: 0.109115\tvalid_1's myFeval: 457.704\n",
      "[7500]\ttraining's l1: 0.0983253\ttraining's myFeval: 395.537\tvalid_1's l1: 0.108918\tvalid_1's myFeval: 456.869\n",
      "[7800]\ttraining's l1: 0.0979515\ttraining's myFeval: 393.482\tvalid_1's l1: 0.108807\tvalid_1's myFeval: 456.25\n",
      "[8100]\ttraining's l1: 0.0975813\ttraining's myFeval: 391.276\tvalid_1's l1: 0.108676\tvalid_1's myFeval: 455.542\n",
      "[8400]\ttraining's l1: 0.0972351\ttraining's myFeval: 389.374\tvalid_1's l1: 0.108557\tvalid_1's myFeval: 454.881\n",
      "[8700]\ttraining's l1: 0.0968569\ttraining's myFeval: 387.486\tvalid_1's l1: 0.108426\tvalid_1's myFeval: 454.287\n",
      "[9000]\ttraining's l1: 0.0964842\ttraining's myFeval: 385.482\tvalid_1's l1: 0.10832\tvalid_1's myFeval: 453.7\n",
      "[9300]\ttraining's l1: 0.0961405\ttraining's myFeval: 383.57\tvalid_1's l1: 0.108224\tvalid_1's myFeval: 453.161\n",
      "[9600]\ttraining's l1: 0.0957792\ttraining's myFeval: 381.585\tvalid_1's l1: 0.108143\tvalid_1's myFeval: 452.804\n",
      "[9900]\ttraining's l1: 0.0954722\ttraining's myFeval: 379.86\tvalid_1's l1: 0.108067\tvalid_1's myFeval: 452.41\n",
      "[10200]\ttraining's l1: 0.0951262\ttraining's myFeval: 377.943\tvalid_1's l1: 0.107964\tvalid_1's myFeval: 451.879\n",
      "[10500]\ttraining's l1: 0.0948134\ttraining's myFeval: 376.356\tvalid_1's l1: 0.107869\tvalid_1's myFeval: 451.481\n",
      "[10800]\ttraining's l1: 0.0945028\ttraining's myFeval: 374.655\tvalid_1's l1: 0.107779\tvalid_1's myFeval: 451.103\n",
      "[11100]\ttraining's l1: 0.0941715\ttraining's myFeval: 373.072\tvalid_1's l1: 0.1077\tvalid_1's myFeval: 450.712\n",
      "[11400]\ttraining's l1: 0.0939088\ttraining's myFeval: 371.693\tvalid_1's l1: 0.107644\tvalid_1's myFeval: 450.454\n",
      "[11700]\ttraining's l1: 0.0936236\ttraining's myFeval: 370.281\tvalid_1's l1: 0.107561\tvalid_1's myFeval: 450.14\n",
      "[12000]\ttraining's l1: 0.0933545\ttraining's myFeval: 369.005\tvalid_1's l1: 0.107502\tvalid_1's myFeval: 449.936\n",
      "[12300]\ttraining's l1: 0.0930749\ttraining's myFeval: 367.679\tvalid_1's l1: 0.107452\tvalid_1's myFeval: 449.818\n",
      "[12600]\ttraining's l1: 0.0927837\ttraining's myFeval: 366.218\tvalid_1's l1: 0.107391\tvalid_1's myFeval: 449.524\n",
      "[12900]\ttraining's l1: 0.0925227\ttraining's myFeval: 364.928\tvalid_1's l1: 0.107329\tvalid_1's myFeval: 449.25\n",
      "[13200]\ttraining's l1: 0.0922585\ttraining's myFeval: 363.597\tvalid_1's l1: 0.107287\tvalid_1's myFeval: 449.047\n",
      "[13500]\ttraining's l1: 0.092\ttraining's myFeval: 362.437\tvalid_1's l1: 0.107245\tvalid_1's myFeval: 448.828\n",
      "[13800]\ttraining's l1: 0.0917634\ttraining's myFeval: 361.249\tvalid_1's l1: 0.107198\tvalid_1's myFeval: 448.566\n",
      "[14100]\ttraining's l1: 0.0914987\ttraining's myFeval: 360.014\tvalid_1's l1: 0.107159\tvalid_1's myFeval: 448.442\n",
      "[14400]\ttraining's l1: 0.0912589\ttraining's myFeval: 358.811\tvalid_1's l1: 0.107106\tvalid_1's myFeval: 448.075\n",
      "[14700]\ttraining's l1: 0.0910165\ttraining's myFeval: 357.572\tvalid_1's l1: 0.107056\tvalid_1's myFeval: 447.826\n",
      "[15000]\ttraining's l1: 0.0907798\ttraining's myFeval: 356.442\tvalid_1's l1: 0.107035\tvalid_1's myFeval: 447.706\n",
      "[15300]\ttraining's l1: 0.0905421\ttraining's myFeval: 355.242\tvalid_1's l1: 0.107011\tvalid_1's myFeval: 447.662\n",
      "[15600]\ttraining's l1: 0.0902987\ttraining's myFeval: 354.106\tvalid_1's l1: 0.10697\tvalid_1's myFeval: 447.465\n",
      "[15900]\ttraining's l1: 0.090085\ttraining's myFeval: 353.218\tvalid_1's l1: 0.106928\tvalid_1's myFeval: 447.299\n",
      "[16200]\ttraining's l1: 0.0898663\ttraining's myFeval: 352.327\tvalid_1's l1: 0.10689\tvalid_1's myFeval: 447.147\n",
      "[16500]\ttraining's l1: 0.0896865\ttraining's myFeval: 351.438\tvalid_1's l1: 0.106857\tvalid_1's myFeval: 446.964\n",
      "[16800]\ttraining's l1: 0.0894812\ttraining's myFeval: 350.384\tvalid_1's l1: 0.106818\tvalid_1's myFeval: 446.748\n",
      "[17100]\ttraining's l1: 0.0892665\ttraining's myFeval: 349.392\tvalid_1's l1: 0.106777\tvalid_1's myFeval: 446.606\n",
      "[17400]\ttraining's l1: 0.0890551\ttraining's myFeval: 348.49\tvalid_1's l1: 0.106735\tvalid_1's myFeval: 446.324\n",
      "[17700]\ttraining's l1: 0.0888905\ttraining's myFeval: 347.676\tvalid_1's l1: 0.106715\tvalid_1's myFeval: 446.213\n",
      "[18000]\ttraining's l1: 0.088698\ttraining's myFeval: 346.78\tvalid_1's l1: 0.106676\tvalid_1's myFeval: 446.01\n",
      "[18300]\ttraining's l1: 0.088523\ttraining's myFeval: 345.786\tvalid_1's l1: 0.106659\tvalid_1's myFeval: 445.934\n",
      "[18600]\ttraining's l1: 0.0883435\ttraining's myFeval: 344.949\tvalid_1's l1: 0.106629\tvalid_1's myFeval: 445.821\n",
      "[18900]\ttraining's l1: 0.0881818\ttraining's myFeval: 344.182\tvalid_1's l1: 0.106605\tvalid_1's myFeval: 445.708\n",
      "[19200]\ttraining's l1: 0.0879995\ttraining's myFeval: 343.364\tvalid_1's l1: 0.106598\tvalid_1's myFeval: 445.648\n",
      "Early stopping, best iteration is:\n",
      "[19031]\ttraining's l1: 0.0881093\ttraining's myFeval: 343.839\tvalid_1's l1: 0.106596\tvalid_1's myFeval: 445.617\n",
      "fold n°10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.044218 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17641\n",
      "[LightGBM] [Info] Number of data points in the train set: 135000, number of used features: 83\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Info] Start training from score 8.086718\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\ttraining's l1: 0.14586\ttraining's myFeval: 754.375\tvalid_1's l1: 0.147292\tvalid_1's myFeval: 781.16\n",
      "[600]\ttraining's l1: 0.127798\ttraining's myFeval: 598.878\tvalid_1's l1: 0.130011\tvalid_1's myFeval: 631.651\n",
      "[900]\ttraining's l1: 0.120858\ttraining's myFeval: 545.531\tvalid_1's l1: 0.12355\tvalid_1's myFeval: 579.055\n",
      "[1200]\ttraining's l1: 0.117105\ttraining's myFeval: 516.972\tvalid_1's l1: 0.120228\tvalid_1's myFeval: 551.225\n",
      "[1500]\ttraining's l1: 0.11452\ttraining's myFeval: 498.398\tvalid_1's l1: 0.11809\tvalid_1's myFeval: 534.426\n",
      "[1800]\ttraining's l1: 0.112685\ttraining's myFeval: 484.366\tvalid_1's l1: 0.11664\tvalid_1's myFeval: 522.534\n",
      "[2100]\ttraining's l1: 0.111067\ttraining's myFeval: 473.736\tvalid_1's l1: 0.115382\tvalid_1's myFeval: 513.864\n",
      "[2400]\ttraining's l1: 0.109719\ttraining's myFeval: 464.796\tvalid_1's l1: 0.114425\tvalid_1's myFeval: 506.757\n",
      "[2700]\ttraining's l1: 0.108579\ttraining's myFeval: 457.108\tvalid_1's l1: 0.113621\tvalid_1's myFeval: 500.635\n",
      "[3000]\ttraining's l1: 0.107462\ttraining's myFeval: 449.662\tvalid_1's l1: 0.112866\tvalid_1's myFeval: 495.437\n",
      "[3300]\ttraining's l1: 0.106433\ttraining's myFeval: 443.562\tvalid_1's l1: 0.11222\tvalid_1's myFeval: 491.151\n",
      "[3600]\ttraining's l1: 0.105556\ttraining's myFeval: 438.356\tvalid_1's l1: 0.111686\tvalid_1's myFeval: 487.743\n",
      "[3900]\ttraining's l1: 0.104785\ttraining's myFeval: 433.295\tvalid_1's l1: 0.11122\tvalid_1's myFeval: 484.501\n",
      "[4200]\ttraining's l1: 0.104152\ttraining's myFeval: 429.06\tvalid_1's l1: 0.11087\tvalid_1's myFeval: 481.901\n",
      "[4500]\ttraining's l1: 0.103478\ttraining's myFeval: 424.947\tvalid_1's l1: 0.110541\tvalid_1's myFeval: 479.729\n",
      "[4800]\ttraining's l1: 0.102781\ttraining's myFeval: 420.889\tvalid_1's l1: 0.110196\tvalid_1's myFeval: 477.864\n",
      "[5100]\ttraining's l1: 0.102146\ttraining's myFeval: 417.458\tvalid_1's l1: 0.109911\tvalid_1's myFeval: 476.247\n",
      "[5400]\ttraining's l1: 0.101542\ttraining's myFeval: 414.231\tvalid_1's l1: 0.109622\tvalid_1's myFeval: 474.731\n",
      "[5700]\ttraining's l1: 0.100969\ttraining's myFeval: 410.871\tvalid_1's l1: 0.109379\tvalid_1's myFeval: 473.257\n",
      "[6000]\ttraining's l1: 0.100489\ttraining's myFeval: 408.066\tvalid_1's l1: 0.10919\tvalid_1's myFeval: 472.083\n",
      "[6300]\ttraining's l1: 0.100007\ttraining's myFeval: 405.614\tvalid_1's l1: 0.109033\tvalid_1's myFeval: 471.093\n",
      "[6600]\ttraining's l1: 0.0995468\ttraining's myFeval: 403.155\tvalid_1's l1: 0.108856\tvalid_1's myFeval: 470.025\n",
      "[6900]\ttraining's l1: 0.0991369\ttraining's myFeval: 400.866\tvalid_1's l1: 0.108702\tvalid_1's myFeval: 469.156\n",
      "[7200]\ttraining's l1: 0.0987406\ttraining's myFeval: 398.415\tvalid_1's l1: 0.108555\tvalid_1's myFeval: 468.304\n",
      "[7500]\ttraining's l1: 0.0983561\ttraining's myFeval: 396.272\tvalid_1's l1: 0.108414\tvalid_1's myFeval: 467.581\n",
      "[7800]\ttraining's l1: 0.0979895\ttraining's myFeval: 394.168\tvalid_1's l1: 0.108284\tvalid_1's myFeval: 466.837\n",
      "[8100]\ttraining's l1: 0.0976356\ttraining's myFeval: 391.987\tvalid_1's l1: 0.108158\tvalid_1's myFeval: 465.98\n",
      "[8400]\ttraining's l1: 0.0972658\ttraining's myFeval: 389.994\tvalid_1's l1: 0.108042\tvalid_1's myFeval: 465.228\n",
      "[8700]\ttraining's l1: 0.0968891\ttraining's myFeval: 388.178\tvalid_1's l1: 0.107918\tvalid_1's myFeval: 464.666\n",
      "[9000]\ttraining's l1: 0.0965276\ttraining's myFeval: 386.289\tvalid_1's l1: 0.107814\tvalid_1's myFeval: 464.018\n",
      "[9300]\ttraining's l1: 0.0962212\ttraining's myFeval: 384.663\tvalid_1's l1: 0.107725\tvalid_1's myFeval: 463.627\n",
      "[9600]\ttraining's l1: 0.0959026\ttraining's myFeval: 382.998\tvalid_1's l1: 0.10763\tvalid_1's myFeval: 463.085\n",
      "[9900]\ttraining's l1: 0.0955607\ttraining's myFeval: 381.272\tvalid_1's l1: 0.107525\tvalid_1's myFeval: 462.604\n",
      "[10200]\ttraining's l1: 0.0952717\ttraining's myFeval: 379.717\tvalid_1's l1: 0.107434\tvalid_1's myFeval: 462.019\n",
      "[10500]\ttraining's l1: 0.0949873\ttraining's myFeval: 378.204\tvalid_1's l1: 0.107379\tvalid_1's myFeval: 461.755\n",
      "[10800]\ttraining's l1: 0.0947204\ttraining's myFeval: 376.66\tvalid_1's l1: 0.107303\tvalid_1's myFeval: 461.319\n",
      "[11100]\ttraining's l1: 0.0944119\ttraining's myFeval: 375.147\tvalid_1's l1: 0.107225\tvalid_1's myFeval: 461.033\n",
      "[11400]\ttraining's l1: 0.0941246\ttraining's myFeval: 373.791\tvalid_1's l1: 0.107155\tvalid_1's myFeval: 460.679\n",
      "[11700]\ttraining's l1: 0.0938298\ttraining's myFeval: 372.234\tvalid_1's l1: 0.107086\tvalid_1's myFeval: 460.258\n",
      "[12000]\ttraining's l1: 0.0935377\ttraining's myFeval: 370.961\tvalid_1's l1: 0.107015\tvalid_1's myFeval: 459.852\n",
      "[12300]\ttraining's l1: 0.0932595\ttraining's myFeval: 369.621\tvalid_1's l1: 0.10696\tvalid_1's myFeval: 459.58\n",
      "[12600]\ttraining's l1: 0.0930012\ttraining's myFeval: 368.236\tvalid_1's l1: 0.106893\tvalid_1's myFeval: 459.193\n",
      "[12900]\ttraining's l1: 0.0927499\ttraining's myFeval: 367.041\tvalid_1's l1: 0.106841\tvalid_1's myFeval: 458.921\n",
      "[13200]\ttraining's l1: 0.0924902\ttraining's myFeval: 365.81\tvalid_1's l1: 0.106769\tvalid_1's myFeval: 458.549\n",
      "[13500]\ttraining's l1: 0.0922334\ttraining's myFeval: 364.475\tvalid_1's l1: 0.106703\tvalid_1's myFeval: 458.185\n",
      "[13800]\ttraining's l1: 0.092021\ttraining's myFeval: 363.269\tvalid_1's l1: 0.10667\tvalid_1's myFeval: 457.839\n",
      "[14100]\ttraining's l1: 0.0918081\ttraining's myFeval: 362.055\tvalid_1's l1: 0.106621\tvalid_1's myFeval: 457.431\n",
      "[14400]\ttraining's l1: 0.0916002\ttraining's myFeval: 360.874\tvalid_1's l1: 0.106574\tvalid_1's myFeval: 457.122\n",
      "[14700]\ttraining's l1: 0.0913645\ttraining's myFeval: 359.655\tvalid_1's l1: 0.106543\tvalid_1's myFeval: 456.872\n",
      "[15000]\ttraining's l1: 0.0911075\ttraining's myFeval: 358.557\tvalid_1's l1: 0.106476\tvalid_1's myFeval: 456.623\n",
      "[15300]\ttraining's l1: 0.0908947\ttraining's myFeval: 357.323\tvalid_1's l1: 0.106446\tvalid_1's myFeval: 456.393\n",
      "[15600]\ttraining's l1: 0.0907131\ttraining's myFeval: 356.287\tvalid_1's l1: 0.106411\tvalid_1's myFeval: 456.219\n",
      "[15900]\ttraining's l1: 0.0904869\ttraining's myFeval: 355.067\tvalid_1's l1: 0.106373\tvalid_1's myFeval: 455.945\n",
      "[16200]\ttraining's l1: 0.0903068\ttraining's myFeval: 353.992\tvalid_1's l1: 0.106334\tvalid_1's myFeval: 455.682\n",
      "[16500]\ttraining's l1: 0.0901056\ttraining's myFeval: 353.005\tvalid_1's l1: 0.10629\tvalid_1's myFeval: 455.488\n",
      "[16800]\ttraining's l1: 0.0898756\ttraining's myFeval: 351.722\tvalid_1's l1: 0.106256\tvalid_1's myFeval: 455.226\n",
      "[17100]\ttraining's l1: 0.0896284\ttraining's myFeval: 350.511\tvalid_1's l1: 0.106207\tvalid_1's myFeval: 454.984\n",
      "[17400]\ttraining's l1: 0.0894424\ttraining's myFeval: 349.657\tvalid_1's l1: 0.106178\tvalid_1's myFeval: 454.825\n",
      "[17700]\ttraining's l1: 0.0892135\ttraining's myFeval: 348.638\tvalid_1's l1: 0.106134\tvalid_1's myFeval: 454.648\n",
      "[18000]\ttraining's l1: 0.0890431\ttraining's myFeval: 347.69\tvalid_1's l1: 0.106109\tvalid_1's myFeval: 454.437\n",
      "[18300]\ttraining's l1: 0.0888648\ttraining's myFeval: 346.756\tvalid_1's l1: 0.106064\tvalid_1's myFeval: 454.33\n",
      "[18600]\ttraining's l1: 0.0886552\ttraining's myFeval: 345.856\tvalid_1's l1: 0.106027\tvalid_1's myFeval: 454.144\n",
      "[18900]\ttraining's l1: 0.0884969\ttraining's myFeval: 345.053\tvalid_1's l1: 0.106012\tvalid_1's myFeval: 454.053\n",
      "[19200]\ttraining's l1: 0.0883248\ttraining's myFeval: 344.258\tvalid_1's l1: 0.105998\tvalid_1's myFeval: 453.865\n",
      "[19500]\ttraining's l1: 0.0881784\ttraining's myFeval: 343.477\tvalid_1's l1: 0.105969\tvalid_1's myFeval: 453.678\n",
      "[19800]\ttraining's l1: 0.08796\ttraining's myFeval: 342.631\tvalid_1's l1: 0.105946\tvalid_1's myFeval: 453.6\n",
      "[20100]\ttraining's l1: 0.0877739\ttraining's myFeval: 341.875\tvalid_1's l1: 0.105919\tvalid_1's myFeval: 453.447\n",
      "[20400]\ttraining's l1: 0.0876091\ttraining's myFeval: 341.056\tvalid_1's l1: 0.10589\tvalid_1's myFeval: 453.219\n",
      "[20700]\ttraining's l1: 0.0874368\ttraining's myFeval: 340.348\tvalid_1's l1: 0.105882\tvalid_1's myFeval: 453.187\n",
      "[21000]\ttraining's l1: 0.0873005\ttraining's myFeval: 339.693\tvalid_1's l1: 0.105861\tvalid_1's myFeval: 453.034\n",
      "[21300]\ttraining's l1: 0.0871501\ttraining's myFeval: 338.862\tvalid_1's l1: 0.105833\tvalid_1's myFeval: 452.844\n",
      "[21600]\ttraining's l1: 0.0870132\ttraining's myFeval: 338.198\tvalid_1's l1: 0.105816\tvalid_1's myFeval: 452.726\n",
      "[21900]\ttraining's l1: 0.0868645\ttraining's myFeval: 337.43\tvalid_1's l1: 0.105803\tvalid_1's myFeval: 452.623\n",
      "[22200]\ttraining's l1: 0.0867113\ttraining's myFeval: 336.569\tvalid_1's l1: 0.10578\tvalid_1's myFeval: 452.539\n",
      "[22500]\ttraining's l1: 0.0865495\ttraining's myFeval: 335.814\tvalid_1's l1: 0.105764\tvalid_1's myFeval: 452.378\n",
      "[22800]\ttraining's l1: 0.0864109\ttraining's myFeval: 335.048\tvalid_1's l1: 0.105752\tvalid_1's myFeval: 452.307\n",
      "[23100]\ttraining's l1: 0.0862689\ttraining's myFeval: 334.281\tvalid_1's l1: 0.105723\tvalid_1's myFeval: 452.149\n",
      "[23400]\ttraining's l1: 0.0861601\ttraining's myFeval: 333.683\tvalid_1's l1: 0.105718\tvalid_1's myFeval: 452.092\n",
      "[23700]\ttraining's l1: 0.0860267\ttraining's myFeval: 333.066\tvalid_1's l1: 0.105706\tvalid_1's myFeval: 452.011\n",
      "[24000]\ttraining's l1: 0.0858994\ttraining's myFeval: 332.381\tvalid_1's l1: 0.105685\tvalid_1's myFeval: 451.844\n",
      "[24300]\ttraining's l1: 0.085767\ttraining's myFeval: 331.72\tvalid_1's l1: 0.105676\tvalid_1's myFeval: 451.722\n",
      "[24600]\ttraining's l1: 0.085631\ttraining's myFeval: 331.059\tvalid_1's l1: 0.105653\tvalid_1's myFeval: 451.586\n",
      "[24900]\ttraining's l1: 0.0854843\ttraining's myFeval: 330.365\tvalid_1's l1: 0.10564\tvalid_1's myFeval: 451.459\n",
      "[25200]\ttraining's l1: 0.0853668\ttraining's myFeval: 329.865\tvalid_1's l1: 0.105617\tvalid_1's myFeval: 451.325\n",
      "[25500]\ttraining's l1: 0.0852337\ttraining's myFeval: 329.291\tvalid_1's l1: 0.105597\tvalid_1's myFeval: 451.306\n",
      "[25800]\ttraining's l1: 0.0851016\ttraining's myFeval: 328.735\tvalid_1's l1: 0.105575\tvalid_1's myFeval: 451.212\n",
      "[26100]\ttraining's l1: 0.084962\ttraining's myFeval: 328.109\tvalid_1's l1: 0.105558\tvalid_1's myFeval: 451.094\n",
      "[26400]\ttraining's l1: 0.0848009\ttraining's myFeval: 327.412\tvalid_1's l1: 0.105548\tvalid_1's myFeval: 450.962\n",
      "[26700]\ttraining's l1: 0.0846665\ttraining's myFeval: 326.809\tvalid_1's l1: 0.105539\tvalid_1's myFeval: 450.868\n",
      "[27000]\ttraining's l1: 0.0845467\ttraining's myFeval: 326.236\tvalid_1's l1: 0.105517\tvalid_1's myFeval: 450.735\n",
      "[27300]\ttraining's l1: 0.0844218\ttraining's myFeval: 325.662\tvalid_1's l1: 0.10551\tvalid_1's myFeval: 450.677\n",
      "[27600]\ttraining's l1: 0.0843069\ttraining's myFeval: 325.103\tvalid_1's l1: 0.105498\tvalid_1's myFeval: 450.645\n",
      "[27900]\ttraining's l1: 0.0841923\ttraining's myFeval: 324.596\tvalid_1's l1: 0.105494\tvalid_1's myFeval: 450.595\n",
      "[28200]\ttraining's l1: 0.0840781\ttraining's myFeval: 324.101\tvalid_1's l1: 0.105492\tvalid_1's myFeval: 450.516\n",
      "[28500]\ttraining's l1: 0.0839557\ttraining's myFeval: 323.56\tvalid_1's l1: 0.105481\tvalid_1's myFeval: 450.428\n",
      "[28800]\ttraining's l1: 0.0838258\ttraining's myFeval: 322.977\tvalid_1's l1: 0.105479\tvalid_1's myFeval: 450.356\n",
      "[29100]\ttraining's l1: 0.0837134\ttraining's myFeval: 322.486\tvalid_1's l1: 0.105452\tvalid_1's myFeval: 450.237\n",
      "[29400]\ttraining's l1: 0.0835874\ttraining's myFeval: 322.004\tvalid_1's l1: 0.105435\tvalid_1's myFeval: 450.172\n",
      "[29700]\ttraining's l1: 0.083476\ttraining's myFeval: 321.537\tvalid_1's l1: 0.105422\tvalid_1's myFeval: 450.114\n",
      "[30000]\ttraining's l1: 0.0833338\ttraining's myFeval: 321.013\tvalid_1's l1: 0.105423\tvalid_1's myFeval: 450.113\n",
      "Early stopping, best iteration is:\n",
      "[29766]\ttraining's l1: 0.0834475\ttraining's myFeval: 321.406\tvalid_1's l1: 0.105417\tvalid_1's myFeval: 450.093\n",
      "lightgbm score: 447.01148723\n"
     ]
    }
   ],
   "source": [
    "## 基础工具\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.model_selection import KFold, RepeatedKFold\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn import linear_model\n",
    "\n",
    "## 读取树模型数据\n",
    "# path = os.path.abspath(os.path.dirname(os.getcwd()) + os.path.sep + \".\")\n",
    "path = os.getcwd()\n",
    "tree_data_path = os.path.join(path, \"user_data\")\n",
    "Train_data = pd.read_csv(os.path.join(tree_data_path, \"train_tree.csv\"), sep=\" \")\n",
    "TestA_data = pd.read_csv(os.path.join(tree_data_path, \"test_tree.csv\"), sep=\" \")\n",
    "\n",
    "numerical_cols = Train_data.columns\n",
    "feature_cols = [col for col in numerical_cols if col not in [\"price\", \"SaleID\"]]\n",
    "## 提前特征列，标签列构造训练样本和测试样本\n",
    "X_data = Train_data[feature_cols]\n",
    "X_test = TestA_data[feature_cols]\n",
    "print(X_data.shape)\n",
    "print(X_test.shape)\n",
    "\n",
    "X_data = np.array(X_data)\n",
    "X_test = np.array(X_test)\n",
    "Y_data = np.array(Train_data[\"price\"])\n",
    "\n",
    "\"\"\"\n",
    "lightgbm\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# 自定义损失函数\n",
    "def myFeval(preds, xgbtrain):\n",
    "    label = xgbtrain.get_label()\n",
    "    score = mean_absolute_error(np.expm1(label), np.expm1(preds))\n",
    "    return \"myFeval\", score, False\n",
    "param = {\n",
    "    \"boosting_type\": \"gbdt\",         # 使用 GBDT（梯度提升树）作为提升方法，是 LightGBM 默认的方式\n",
    "    \"num_leaves\": 31,                # 每棵树的最大叶子节点数，值越大模型越复杂，越容易过拟合\n",
    "    \"max_depth\": -1,                 # 不限制树的最大深度，通常与 num_leaves 联合控制复杂度\n",
    "    \"lambda_l2\": 2,                  # L2 正则化系数，用于防止模型过拟合\n",
    "    \"min_data_in_leaf\": 20,          # 每个叶子节点最少的数据量，防止分裂出只有少量样本的叶子，减少过拟合风险\n",
    "    \"objective\": \"regression_l1\",    # 目标函数为 L1 回归（最小绝对误差 MAE），对异常值更稳健\n",
    "    \"learning_rate\": 0.02,           # 学习率，值小代表每棵树学习得更慢，训练更稳定但耗时更长\n",
    "    \"min_child_samples\": 20,         # 与 min_data_in_leaf 类似，叶子节点所需的最小样本数，防止过拟合\n",
    "    \"feature_fraction\": 0.8,         # 每棵树训练时，随机使用 80% 的特征，有助于减少特征间的共线性和过拟合\n",
    "    \"bagging_freq\": 1,               # 每 1 次迭代执行一次数据采样（bagging）\n",
    "    \"bagging_fraction\": 0.8,         # 每次训练时只用 80% 的训练数据进行采样，防止模型过拟合\n",
    "    \"bagging_seed\": 11,              # 控制 bagging 的随机性，使得结果可复现\n",
    "    \"metric\": \"mae\",                 # 使用 MAE（平均绝对误差）作为验证集评估指标，衡量预测误差\n",
    "}\n",
    "\n",
    "folds = KFold(n_splits=2, shuffle=True)\n",
    "oof_lgb = np.zeros(len(X_data))\n",
    "predictions_lgb = np.zeros(len(X_test))\n",
    "predictions_train_lgb = np.zeros(len(X_data))\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_data, Y_data)):\n",
    "    print(\"fold n°{}\".format(fold_ + 1))\n",
    "    trn_data = lgb.Dataset(X_data[trn_idx], Y_data[trn_idx])\n",
    "    val_data = lgb.Dataset(X_data[val_idx], Y_data[val_idx])\n",
    "\n",
    "    num_round = 1000000\n",
    "    # 添加回调函数\n",
    "    callbacks = [\n",
    "        lgb.callback.log_evaluation(period=300),\n",
    "        lgb.callback.early_stopping(stopping_rounds=300),\n",
    "    ]\n",
    "    clf = lgb.train(\n",
    "        param,\n",
    "        trn_data,\n",
    "        num_round,\n",
    "        valid_sets=[trn_data, val_data],\n",
    "        feval=myFeval,\n",
    "        callbacks=callbacks,\n",
    "    )\n",
    "    oof_lgb[val_idx] = clf.predict(X_data[val_idx], num_iteration=clf.best_iteration)\n",
    "    predictions_lgb += (\n",
    "        clf.predict(X_test, num_iteration=clf.best_iteration) / folds.n_splits\n",
    "    )\n",
    "    predictions_train_lgb += (\n",
    "        clf.predict(X_data, num_iteration=clf.best_iteration) / folds.n_splits\n",
    "    )\n",
    "\n",
    "print(\n",
    "    \"lightgbm score: {:<8.8f}\".format(\n",
    "        mean_absolute_error(np.expm1(oof_lgb), np.expm1(Y_data))\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = os.path.join(path, \"user_data\")\n",
    "# 测试集输出\n",
    "predictions = predictions_lgb\n",
    "predictions[predictions < 0] = 0\n",
    "sub = pd.DataFrame()\n",
    "sub[\"SaleID\"] = TestA_data.SaleID\n",
    "sub[\"price\"] = predictions\n",
    "sub.to_csv(os.path.join(output_path, \"test_lgb.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 验证集输出\n",
    "oof_lgb[oof_lgb < 0] = 0\n",
    "sub = pd.DataFrame()\n",
    "sub[\"SaleID\"] = Train_data.SaleID\n",
    "sub[\"price\"] = oof_lgb\n",
    "sub.to_csv(os.path.join(output_path, \"train_lgb.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n°1\n",
      "0:\tlearn: 0.9748379\ttest: 0.9794465\tbest: 0.9794465 (0)\ttotal: 184ms\tremaining: 2d 3h 1m 34s\n",
      "300:\tlearn: 0.1702876\ttest: 0.1754260\tbest: 0.1754260 (300)\ttotal: 9.63s\tremaining: 8h 53m 17s\n",
      "600:\tlearn: 0.1443512\ttest: 0.1493608\tbest: 0.1493608 (600)\ttotal: 19.1s\tremaining: 8h 49m 2s\n",
      "900:\tlearn: 0.1330670\ttest: 0.1380017\tbest: 0.1380017 (900)\ttotal: 28.5s\tremaining: 8h 47m 18s\n",
      "1200:\tlearn: 0.1269843\ttest: 0.1322234\tbest: 0.1322234 (1200)\ttotal: 37.9s\tremaining: 8h 45m 5s\n",
      "1500:\tlearn: 0.1230593\ttest: 0.1285772\tbest: 0.1285772 (1500)\ttotal: 47.3s\tremaining: 8h 44m 21s\n",
      "1800:\tlearn: 0.1201550\ttest: 0.1259519\tbest: 0.1259519 (1800)\ttotal: 56.7s\tremaining: 8h 43m 47s\n",
      "2100:\tlearn: 0.1179963\ttest: 0.1240175\tbest: 0.1240175 (2100)\ttotal: 1m 6s\tremaining: 8h 43m 10s\n",
      "2400:\tlearn: 0.1162220\ttest: 0.1225035\tbest: 0.1225035 (2400)\ttotal: 1m 14s\tremaining: 8h 39m 2s\n",
      "2700:\tlearn: 0.1147293\ttest: 0.1212625\tbest: 0.1212625 (2700)\ttotal: 1m 23s\tremaining: 8h 35m 47s\n",
      "3000:\tlearn: 0.1134367\ttest: 0.1202235\tbest: 0.1202235 (3000)\ttotal: 1m 32s\tremaining: 8h 33m 43s\n",
      "3300:\tlearn: 0.1123036\ttest: 0.1192570\tbest: 0.1192570 (3300)\ttotal: 1m 41s\tremaining: 8h 32m 6s\n",
      "3600:\tlearn: 0.1112742\ttest: 0.1184329\tbest: 0.1184329 (3600)\ttotal: 1m 50s\tremaining: 8h 30m 21s\n",
      "3900:\tlearn: 0.1103646\ttest: 0.1177418\tbest: 0.1177418 (3900)\ttotal: 1m 59s\tremaining: 8h 29m 6s\n",
      "4200:\tlearn: 0.1095051\ttest: 0.1170828\tbest: 0.1170828 (4200)\ttotal: 2m 8s\tremaining: 8h 27m 29s\n",
      "4500:\tlearn: 0.1087917\ttest: 0.1165405\tbest: 0.1165405 (4500)\ttotal: 2m 17s\tremaining: 8h 25m 55s\n",
      "4800:\tlearn: 0.1080875\ttest: 0.1160367\tbest: 0.1160367 (4800)\ttotal: 2m 26s\tremaining: 8h 25m 2s\n",
      "5100:\tlearn: 0.1074503\ttest: 0.1155986\tbest: 0.1155986 (5100)\ttotal: 2m 35s\tremaining: 8h 24m 14s\n",
      "5400:\tlearn: 0.1068394\ttest: 0.1151780\tbest: 0.1151780 (5400)\ttotal: 2m 43s\tremaining: 8h 23m 14s\n",
      "5700:\tlearn: 0.1062968\ttest: 0.1148012\tbest: 0.1148012 (5700)\ttotal: 2m 52s\tremaining: 8h 22m 16s\n",
      "6000:\tlearn: 0.1057783\ttest: 0.1144563\tbest: 0.1144563 (6000)\ttotal: 3m 1s\tremaining: 8h 21m 51s\n",
      "6300:\tlearn: 0.1053071\ttest: 0.1141436\tbest: 0.1141436 (6300)\ttotal: 3m 10s\tremaining: 8h 21m 20s\n",
      "6600:\tlearn: 0.1048226\ttest: 0.1138351\tbest: 0.1138351 (6600)\ttotal: 3m 19s\tremaining: 8h 20m 27s\n",
      "6900:\tlearn: 0.1044186\ttest: 0.1135643\tbest: 0.1135643 (6900)\ttotal: 3m 28s\tremaining: 8h 19m 55s\n",
      "7200:\tlearn: 0.1040309\ttest: 0.1133264\tbest: 0.1133264 (7200)\ttotal: 3m 37s\tremaining: 8h 19m 33s\n",
      "7500:\tlearn: 0.1036616\ttest: 0.1131160\tbest: 0.1131160 (7500)\ttotal: 3m 46s\tremaining: 8h 18m 58s\n",
      "7800:\tlearn: 0.1033008\ttest: 0.1129051\tbest: 0.1129051 (7800)\ttotal: 3m 55s\tremaining: 8h 18m 35s\n",
      "8100:\tlearn: 0.1029620\ttest: 0.1127332\tbest: 0.1127332 (8100)\ttotal: 4m 4s\tremaining: 8h 18m 10s\n",
      "8400:\tlearn: 0.1026276\ttest: 0.1125452\tbest: 0.1125452 (8400)\ttotal: 4m 12s\tremaining: 8h 17m 34s\n",
      "8700:\tlearn: 0.1023137\ttest: 0.1123847\tbest: 0.1123845 (8699)\ttotal: 4m 21s\tremaining: 8h 17m\n",
      "9000:\tlearn: 0.1020078\ttest: 0.1122344\tbest: 0.1122344 (9000)\ttotal: 4m 30s\tremaining: 8h 16m 31s\n",
      "9300:\tlearn: 0.1017399\ttest: 0.1120751\tbest: 0.1120751 (9300)\ttotal: 4m 39s\tremaining: 8h 16m 6s\n",
      "9600:\tlearn: 0.1014424\ttest: 0.1119112\tbest: 0.1119112 (9600)\ttotal: 4m 48s\tremaining: 8h 15m 44s\n",
      "9900:\tlearn: 0.1011773\ttest: 0.1117691\tbest: 0.1117675 (9899)\ttotal: 4m 57s\tremaining: 8h 15m 31s\n",
      "10200:\tlearn: 0.1008925\ttest: 0.1116498\tbest: 0.1116487 (10199)\ttotal: 5m 6s\tremaining: 8h 15m\n",
      "10500:\tlearn: 0.1006374\ttest: 0.1115206\tbest: 0.1115206 (10500)\ttotal: 5m 15s\tremaining: 8h 14m 43s\n",
      "10800:\tlearn: 0.1003827\ttest: 0.1114139\tbest: 0.1114139 (10800)\ttotal: 5m 23s\tremaining: 8h 14m 21s\n",
      "11100:\tlearn: 0.1001400\ttest: 0.1112830\tbest: 0.1112830 (11100)\ttotal: 5m 32s\tremaining: 8h 14m 4s\n",
      "11400:\tlearn: 0.0999311\ttest: 0.1111948\tbest: 0.1111944 (11397)\ttotal: 5m 41s\tremaining: 8h 13m 34s\n",
      "11700:\tlearn: 0.0996763\ttest: 0.1110613\tbest: 0.1110613 (11700)\ttotal: 5m 50s\tremaining: 8h 13m 5s\n",
      "12000:\tlearn: 0.0994432\ttest: 0.1109565\tbest: 0.1109558 (11998)\ttotal: 5m 59s\tremaining: 8h 12m 53s\n",
      "12300:\tlearn: 0.0992173\ttest: 0.1108646\tbest: 0.1108646 (12300)\ttotal: 6m 8s\tremaining: 8h 12m 33s\n",
      "12600:\tlearn: 0.0990070\ttest: 0.1107807\tbest: 0.1107807 (12600)\ttotal: 6m 16s\tremaining: 8h 12m 16s\n",
      "12900:\tlearn: 0.0987942\ttest: 0.1107013\tbest: 0.1107013 (12900)\ttotal: 6m 25s\tremaining: 8h 12m 1s\n",
      "13200:\tlearn: 0.0985812\ttest: 0.1106196\tbest: 0.1106196 (13200)\ttotal: 6m 34s\tremaining: 8h 11m 42s\n",
      "13500:\tlearn: 0.0983923\ttest: 0.1105472\tbest: 0.1105472 (13500)\ttotal: 6m 43s\tremaining: 8h 11m 21s\n",
      "13800:\tlearn: 0.0981846\ttest: 0.1104716\tbest: 0.1104716 (13800)\ttotal: 6m 52s\tremaining: 8h 11m 1s\n",
      "14100:\tlearn: 0.0980005\ttest: 0.1103976\tbest: 0.1103976 (14100)\ttotal: 7m 1s\tremaining: 8h 10m 44s\n",
      "14400:\tlearn: 0.0978065\ttest: 0.1103171\tbest: 0.1103164 (14398)\ttotal: 7m 9s\tremaining: 8h 10m 26s\n",
      "14700:\tlearn: 0.0976324\ttest: 0.1102603\tbest: 0.1102583 (14689)\ttotal: 7m 18s\tremaining: 8h 9m 46s\n",
      "15000:\tlearn: 0.0974648\ttest: 0.1101976\tbest: 0.1101976 (15000)\ttotal: 7m 26s\tremaining: 8h 9m 2s\n",
      "15300:\tlearn: 0.0973017\ttest: 0.1101289\tbest: 0.1101289 (15300)\ttotal: 7m 35s\tremaining: 8h 8m 18s\n",
      "15600:\tlearn: 0.0971457\ttest: 0.1100843\tbest: 0.1100843 (15600)\ttotal: 7m 43s\tremaining: 8h 7m 38s\n",
      "15900:\tlearn: 0.0969562\ttest: 0.1100320\tbest: 0.1100316 (15897)\ttotal: 7m 52s\tremaining: 8h 6m 59s\n",
      "16200:\tlearn: 0.0967886\ttest: 0.1099611\tbest: 0.1099611 (16200)\ttotal: 8m\tremaining: 8h 6m 29s\n",
      "16500:\tlearn: 0.0966412\ttest: 0.1099028\tbest: 0.1099028 (16500)\ttotal: 8m 9s\tremaining: 8h 6m 8s\n",
      "16800:\tlearn: 0.0964900\ttest: 0.1098455\tbest: 0.1098455 (16800)\ttotal: 8m 17s\tremaining: 8h 5m 39s\n",
      "17100:\tlearn: 0.0963311\ttest: 0.1097955\tbest: 0.1097955 (17100)\ttotal: 8m 26s\tremaining: 8h 5m 5s\n",
      "17400:\tlearn: 0.0961718\ttest: 0.1097559\tbest: 0.1097555 (17386)\ttotal: 8m 34s\tremaining: 8h 4m 27s\n",
      "17700:\tlearn: 0.0960226\ttest: 0.1097160\tbest: 0.1097159 (17698)\ttotal: 8m 43s\tremaining: 8h 3m 50s\n",
      "18000:\tlearn: 0.0958699\ttest: 0.1096749\tbest: 0.1096742 (17968)\ttotal: 8m 51s\tremaining: 8h 3m 13s\n",
      "18300:\tlearn: 0.0957323\ttest: 0.1096292\tbest: 0.1096292 (18300)\ttotal: 8m 59s\tremaining: 8h 2m 42s\n",
      "18600:\tlearn: 0.0955707\ttest: 0.1095753\tbest: 0.1095753 (18600)\ttotal: 9m 8s\tremaining: 8h 2m 5s\n",
      "18900:\tlearn: 0.0954391\ttest: 0.1095301\tbest: 0.1095301 (18900)\ttotal: 9m 16s\tremaining: 8h 1m 30s\n",
      "19200:\tlearn: 0.0953044\ttest: 0.1094919\tbest: 0.1094918 (19198)\ttotal: 9m 24s\tremaining: 8h 57s\n",
      "19500:\tlearn: 0.0951705\ttest: 0.1094354\tbest: 0.1094354 (19500)\ttotal: 9m 33s\tremaining: 8h 30s\n",
      "19800:\tlearn: 0.0950437\ttest: 0.1093964\tbest: 0.1093964 (19800)\ttotal: 9m 41s\tremaining: 7h 59m 54s\n",
      "20100:\tlearn: 0.0949204\ttest: 0.1093504\tbest: 0.1093504 (20100)\ttotal: 9m 50s\tremaining: 7h 59m 23s\n",
      "20400:\tlearn: 0.0947797\ttest: 0.1093164\tbest: 0.1093164 (20400)\ttotal: 9m 58s\tremaining: 7h 58m 46s\n",
      "20700:\tlearn: 0.0946622\ttest: 0.1092924\tbest: 0.1092904 (20698)\ttotal: 10m 6s\tremaining: 7h 58m 18s\n",
      "21000:\tlearn: 0.0945430\ttest: 0.1092715\tbest: 0.1092715 (21000)\ttotal: 10m 15s\tremaining: 7h 57m 52s\n",
      "21300:\tlearn: 0.0944113\ttest: 0.1092301\tbest: 0.1092301 (21300)\ttotal: 10m 23s\tremaining: 7h 57m 32s\n",
      "21600:\tlearn: 0.0942800\ttest: 0.1092032\tbest: 0.1092031 (21599)\ttotal: 10m 32s\tremaining: 7h 57m 12s\n",
      "21900:\tlearn: 0.0941620\ttest: 0.1091735\tbest: 0.1091735 (21900)\ttotal: 10m 40s\tremaining: 7h 56m 48s\n",
      "22200:\tlearn: 0.0940429\ttest: 0.1091485\tbest: 0.1091468 (22164)\ttotal: 10m 48s\tremaining: 7h 56m 21s\n",
      "22500:\tlearn: 0.0939245\ttest: 0.1091209\tbest: 0.1091209 (22500)\ttotal: 10m 57s\tremaining: 7h 55m 54s\n",
      "22800:\tlearn: 0.0938212\ttest: 0.1091048\tbest: 0.1091016 (22785)\ttotal: 11m 5s\tremaining: 7h 55m 25s\n",
      "23100:\tlearn: 0.0937044\ttest: 0.1090796\tbest: 0.1090796 (23100)\ttotal: 11m 13s\tremaining: 7h 55m 1s\n",
      "23400:\tlearn: 0.0935890\ttest: 0.1090453\tbest: 0.1090448 (23388)\ttotal: 11m 22s\tremaining: 7h 54m 37s\n",
      "23700:\tlearn: 0.0934830\ttest: 0.1090132\tbest: 0.1090130 (23699)\ttotal: 11m 30s\tremaining: 7h 54m 11s\n",
      "24000:\tlearn: 0.0933702\ttest: 0.1089967\tbest: 0.1089961 (23902)\ttotal: 11m 38s\tremaining: 7h 53m 44s\n",
      "24300:\tlearn: 0.0932620\ttest: 0.1089684\tbest: 0.1089671 (24292)\ttotal: 11m 47s\tremaining: 7h 53m 20s\n",
      "24600:\tlearn: 0.0931511\ttest: 0.1089533\tbest: 0.1089499 (24557)\ttotal: 11m 55s\tremaining: 7h 52m 54s\n",
      "24900:\tlearn: 0.0930476\ttest: 0.1089391\tbest: 0.1089365 (24877)\ttotal: 12m 4s\tremaining: 7h 52m 31s\n",
      "25200:\tlearn: 0.0929389\ttest: 0.1089199\tbest: 0.1089199 (25200)\ttotal: 12m 12s\tremaining: 7h 52m 6s\n",
      "25500:\tlearn: 0.0928460\ttest: 0.1089020\tbest: 0.1089019 (25495)\ttotal: 12m 20s\tremaining: 7h 51m 49s\n",
      "25800:\tlearn: 0.0927489\ttest: 0.1088780\tbest: 0.1088770 (25785)\ttotal: 12m 29s\tremaining: 7h 51m 26s\n",
      "26100:\tlearn: 0.0926493\ttest: 0.1088505\tbest: 0.1088503 (26096)\ttotal: 12m 37s\tremaining: 7h 51m 2s\n",
      "26400:\tlearn: 0.0925461\ttest: 0.1088340\tbest: 0.1088322 (26392)\ttotal: 12m 46s\tremaining: 7h 50m 50s\n",
      "26700:\tlearn: 0.0924557\ttest: 0.1088278\tbest: 0.1088255 (26682)\ttotal: 12m 54s\tremaining: 7h 50m 26s\n",
      "27000:\tlearn: 0.0923521\ttest: 0.1088217\tbest: 0.1088186 (26994)\ttotal: 13m 2s\tremaining: 7h 50m 2s\n",
      "27300:\tlearn: 0.0922586\ttest: 0.1088007\tbest: 0.1087991 (27298)\ttotal: 13m 10s\tremaining: 7h 49m 40s\n",
      "27600:\tlearn: 0.0921552\ttest: 0.1087863\tbest: 0.1087863 (27600)\ttotal: 13m 19s\tremaining: 7h 49m 21s\n",
      "27900:\tlearn: 0.0920656\ttest: 0.1087755\tbest: 0.1087745 (27895)\ttotal: 13m 27s\tremaining: 7h 49m\n",
      "28200:\tlearn: 0.0919763\ttest: 0.1087598\tbest: 0.1087595 (28195)\ttotal: 13m 36s\tremaining: 7h 48m 41s\n",
      "28500:\tlearn: 0.0918858\ttest: 0.1087453\tbest: 0.1087447 (28477)\ttotal: 13m 44s\tremaining: 7h 48m 19s\n",
      "28800:\tlearn: 0.0917932\ttest: 0.1087268\tbest: 0.1087267 (28798)\ttotal: 13m 52s\tremaining: 7h 47m 57s\n",
      "29100:\tlearn: 0.0916922\ttest: 0.1086985\tbest: 0.1086985 (29100)\ttotal: 14m 1s\tremaining: 7h 47m 38s\n",
      "29400:\tlearn: 0.0916107\ttest: 0.1086840\tbest: 0.1086822 (29382)\ttotal: 14m 9s\tremaining: 7h 47m 17s\n",
      "29700:\tlearn: 0.0915104\ttest: 0.1086676\tbest: 0.1086646 (29682)\ttotal: 14m 17s\tremaining: 7h 46m 57s\n",
      "30000:\tlearn: 0.0914152\ttest: 0.1086572\tbest: 0.1086567 (29969)\ttotal: 14m 25s\tremaining: 7h 46m 38s\n",
      "30300:\tlearn: 0.0913297\ttest: 0.1086393\tbest: 0.1086393 (30300)\ttotal: 14m 34s\tremaining: 7h 46m 20s\n",
      "30600:\tlearn: 0.0912444\ttest: 0.1086266\tbest: 0.1086259 (30593)\ttotal: 14m 42s\tremaining: 7h 46m\n",
      "30900:\tlearn: 0.0911535\ttest: 0.1086048\tbest: 0.1086045 (30898)\ttotal: 14m 50s\tremaining: 7h 45m 41s\n",
      "31200:\tlearn: 0.0910722\ttest: 0.1085853\tbest: 0.1085832 (31180)\ttotal: 14m 59s\tremaining: 7h 45m 23s\n",
      "31500:\tlearn: 0.0909915\ttest: 0.1085735\tbest: 0.1085735 (31500)\ttotal: 15m 7s\tremaining: 7h 45m 6s\n",
      "31800:\tlearn: 0.0909063\ttest: 0.1085680\tbest: 0.1085653 (31771)\ttotal: 15m 16s\tremaining: 7h 44m 48s\n",
      "32100:\tlearn: 0.0908286\ttest: 0.1085576\tbest: 0.1085557 (32063)\ttotal: 15m 24s\tremaining: 7h 44m 28s\n",
      "32400:\tlearn: 0.0907383\ttest: 0.1085390\tbest: 0.1085377 (32382)\ttotal: 15m 32s\tremaining: 7h 44m 10s\n",
      "32700:\tlearn: 0.0906584\ttest: 0.1085136\tbest: 0.1085136 (32700)\ttotal: 15m 40s\tremaining: 7h 43m 51s\n",
      "33000:\tlearn: 0.0905764\ttest: 0.1085069\tbest: 0.1085046 (32988)\ttotal: 15m 49s\tremaining: 7h 43m 31s\n",
      "33300:\tlearn: 0.0905085\ttest: 0.1084909\tbest: 0.1084908 (33298)\ttotal: 15m 57s\tremaining: 7h 43m 13s\n",
      "33600:\tlearn: 0.0904324\ttest: 0.1084665\tbest: 0.1084649 (33591)\ttotal: 16m 5s\tremaining: 7h 42m 55s\n",
      "33900:\tlearn: 0.0903606\ttest: 0.1084578\tbest: 0.1084569 (33873)\ttotal: 16m 14s\tremaining: 7h 42m 38s\n",
      "34200:\tlearn: 0.0902857\ttest: 0.1084364\tbest: 0.1084362 (34174)\ttotal: 16m 22s\tremaining: 7h 42m 21s\n",
      "34500:\tlearn: 0.0902097\ttest: 0.1084277\tbest: 0.1084276 (34494)\ttotal: 16m 30s\tremaining: 7h 42m 4s\n",
      "34800:\tlearn: 0.0901386\ttest: 0.1084185\tbest: 0.1084161 (34737)\ttotal: 16m 39s\tremaining: 7h 41m 49s\n",
      "35100:\tlearn: 0.0900609\ttest: 0.1084047\tbest: 0.1084039 (35097)\ttotal: 16m 47s\tremaining: 7h 41m 30s\n",
      "35400:\tlearn: 0.0899841\ttest: 0.1083880\tbest: 0.1083867 (35384)\ttotal: 16m 55s\tremaining: 7h 41m 13s\n",
      "35700:\tlearn: 0.0899146\ttest: 0.1083829\tbest: 0.1083760 (35674)\ttotal: 17m 3s\tremaining: 7h 40m 57s\n",
      "36000:\tlearn: 0.0898420\ttest: 0.1083621\tbest: 0.1083621 (36000)\ttotal: 17m 12s\tremaining: 7h 40m 40s\n",
      "36300:\tlearn: 0.0897617\ttest: 0.1083562\tbest: 0.1083560 (36296)\ttotal: 17m 20s\tremaining: 7h 40m 27s\n",
      "36600:\tlearn: 0.0896842\ttest: 0.1083442\tbest: 0.1083438 (36527)\ttotal: 17m 28s\tremaining: 7h 40m 10s\n",
      "36900:\tlearn: 0.0896097\ttest: 0.1083325\tbest: 0.1083325 (36900)\ttotal: 17m 37s\tremaining: 7h 39m 54s\n",
      "37200:\tlearn: 0.0895335\ttest: 0.1083077\tbest: 0.1083069 (37191)\ttotal: 17m 45s\tremaining: 7h 39m 39s\n",
      "37500:\tlearn: 0.0894570\ttest: 0.1083011\tbest: 0.1082992 (37488)\ttotal: 17m 54s\tremaining: 7h 39m 26s\n",
      "37800:\tlearn: 0.0893912\ttest: 0.1082915\tbest: 0.1082915 (37800)\ttotal: 18m 2s\tremaining: 7h 39m 11s\n",
      "38100:\tlearn: 0.0893145\ttest: 0.1082793\tbest: 0.1082793 (38100)\ttotal: 18m 10s\tremaining: 7h 38m 55s\n",
      "38400:\tlearn: 0.0892460\ttest: 0.1082809\tbest: 0.1082771 (38148)\ttotal: 18m 18s\tremaining: 7h 38m 39s\n",
      "Stopped by overfitting detector  (300 iterations wait)\n",
      "\n",
      "bestTest = 0.1082770725\n",
      "bestIteration = 38148\n",
      "\n",
      "Shrink model to first 38149 iterations.\n",
      "fold n°2\n",
      "0:\tlearn: 0.9752512\ttest: 0.9768811\tbest: 0.9768811 (0)\ttotal: 32.2ms\tremaining: 8h 56m 40s\n",
      "300:\tlearn: 0.1708690\ttest: 0.1689712\tbest: 0.1689712 (300)\ttotal: 8.8s\tremaining: 8h 6m 59s\n",
      "600:\tlearn: 0.1449353\ttest: 0.1439112\tbest: 0.1439112 (600)\ttotal: 17.4s\tremaining: 8h 1m 31s\n",
      "900:\tlearn: 0.1335130\ttest: 0.1331335\tbest: 0.1331335 (900)\ttotal: 26s\tremaining: 8h 38s\n",
      "1200:\tlearn: 0.1275550\ttest: 0.1277636\tbest: 0.1277636 (1200)\ttotal: 34.6s\tremaining: 7h 59m 31s\n",
      "1500:\tlearn: 0.1235384\ttest: 0.1242477\tbest: 0.1242477 (1500)\ttotal: 43.3s\tremaining: 7h 59m 36s\n",
      "1800:\tlearn: 0.1206146\ttest: 0.1217320\tbest: 0.1217320 (1800)\ttotal: 51.9s\tremaining: 7h 59m 52s\n",
      "2100:\tlearn: 0.1183763\ttest: 0.1198781\tbest: 0.1198781 (2100)\ttotal: 1m\tremaining: 7h 59m 58s\n",
      "2400:\tlearn: 0.1165645\ttest: 0.1183973\tbest: 0.1183973 (2400)\ttotal: 1m 9s\tremaining: 7h 59m 37s\n",
      "2700:\tlearn: 0.1150397\ttest: 0.1171229\tbest: 0.1171229 (2700)\ttotal: 1m 17s\tremaining: 7h 59m 57s\n",
      "3000:\tlearn: 0.1137948\ttest: 0.1161062\tbest: 0.1161062 (3000)\ttotal: 1m 26s\tremaining: 8h 4s\n",
      "3300:\tlearn: 0.1125651\ttest: 0.1151254\tbest: 0.1151254 (3300)\ttotal: 1m 35s\tremaining: 8h 7s\n",
      "3600:\tlearn: 0.1115574\ttest: 0.1143288\tbest: 0.1143288 (3600)\ttotal: 1m 44s\tremaining: 8h 27s\n",
      "3900:\tlearn: 0.1106880\ttest: 0.1136553\tbest: 0.1136553 (3900)\ttotal: 1m 53s\tremaining: 8h 55s\n",
      "4200:\tlearn: 0.1097838\ttest: 0.1129507\tbest: 0.1129507 (4200)\ttotal: 2m 1s\tremaining: 8h 1m 6s\n",
      "4500:\tlearn: 0.1090826\ttest: 0.1124384\tbest: 0.1124384 (4500)\ttotal: 2m 10s\tremaining: 8h 44s\n",
      "4800:\tlearn: 0.1083939\ttest: 0.1119213\tbest: 0.1119213 (4800)\ttotal: 2m 19s\tremaining: 8h 33s\n",
      "5100:\tlearn: 0.1077791\ttest: 0.1114758\tbest: 0.1114758 (5100)\ttotal: 2m 27s\tremaining: 8h 27s\n",
      "5400:\tlearn: 0.1071571\ttest: 0.1110711\tbest: 0.1110711 (5400)\ttotal: 2m 36s\tremaining: 8h 1s\n",
      "5700:\tlearn: 0.1066628\ttest: 0.1107468\tbest: 0.1107468 (5700)\ttotal: 2m 45s\tremaining: 7h 59m 57s\n",
      "6000:\tlearn: 0.1061573\ttest: 0.1104042\tbest: 0.1104042 (6000)\ttotal: 2m 53s\tremaining: 7h 59m 49s\n",
      "6300:\tlearn: 0.1056489\ttest: 0.1100620\tbest: 0.1100620 (6300)\ttotal: 3m 2s\tremaining: 7h 59m 32s\n",
      "6600:\tlearn: 0.1052085\ttest: 0.1097990\tbest: 0.1097990 (6600)\ttotal: 3m 11s\tremaining: 7h 59m 30s\n",
      "6900:\tlearn: 0.1047792\ttest: 0.1095429\tbest: 0.1095429 (6900)\ttotal: 3m 19s\tremaining: 7h 59m 16s\n",
      "7200:\tlearn: 0.1043752\ttest: 0.1092811\tbest: 0.1092811 (7200)\ttotal: 3m 28s\tremaining: 7h 58m 56s\n",
      "7500:\tlearn: 0.1040036\ttest: 0.1090548\tbest: 0.1090548 (7500)\ttotal: 3m 37s\tremaining: 7h 58m 51s\n",
      "7800:\tlearn: 0.1036660\ttest: 0.1088440\tbest: 0.1088440 (7800)\ttotal: 3m 45s\tremaining: 7h 58m 46s\n",
      "8100:\tlearn: 0.1033024\ttest: 0.1086637\tbest: 0.1086637 (8100)\ttotal: 3m 54s\tremaining: 7h 58m 30s\n",
      "8400:\tlearn: 0.1029745\ttest: 0.1084908\tbest: 0.1084908 (8400)\ttotal: 4m 3s\tremaining: 7h 58m 23s\n",
      "8700:\tlearn: 0.1026452\ttest: 0.1083245\tbest: 0.1083245 (8700)\ttotal: 4m 11s\tremaining: 7h 58m 13s\n",
      "9000:\tlearn: 0.1023664\ttest: 0.1081880\tbest: 0.1081880 (9000)\ttotal: 4m 20s\tremaining: 7h 58m 22s\n",
      "9300:\tlearn: 0.1021086\ttest: 0.1080315\tbest: 0.1080315 (9300)\ttotal: 4m 29s\tremaining: 7h 58m 28s\n",
      "9600:\tlearn: 0.1017943\ttest: 0.1078681\tbest: 0.1078681 (9600)\ttotal: 4m 38s\tremaining: 7h 58m 41s\n",
      "9900:\tlearn: 0.1015268\ttest: 0.1077289\tbest: 0.1077289 (9900)\ttotal: 4m 47s\tremaining: 7h 58m 29s\n",
      "10200:\tlearn: 0.1012636\ttest: 0.1075957\tbest: 0.1075957 (10200)\ttotal: 4m 55s\tremaining: 7h 58m 18s\n",
      "10500:\tlearn: 0.1010028\ttest: 0.1074748\tbest: 0.1074748 (10500)\ttotal: 5m 4s\tremaining: 7h 58m\n",
      "10800:\tlearn: 0.1007544\ttest: 0.1073636\tbest: 0.1073615 (10798)\ttotal: 5m 13s\tremaining: 7h 57m 48s\n",
      "11100:\tlearn: 0.1005243\ttest: 0.1072321\tbest: 0.1072321 (11100)\ttotal: 5m 21s\tremaining: 7h 57m 41s\n",
      "11400:\tlearn: 0.1002904\ttest: 0.1071415\tbest: 0.1071409 (11397)\ttotal: 5m 30s\tremaining: 7h 57m 27s\n",
      "11700:\tlearn: 0.1000765\ttest: 0.1070397\tbest: 0.1070394 (11697)\ttotal: 5m 39s\tremaining: 7h 57m 15s\n",
      "12000:\tlearn: 0.0998382\ttest: 0.1069359\tbest: 0.1069359 (12000)\ttotal: 5m 47s\tremaining: 7h 57m 3s\n",
      "12300:\tlearn: 0.0996068\ttest: 0.1068322\tbest: 0.1068322 (12300)\ttotal: 5m 56s\tremaining: 7h 56m 53s\n",
      "12600:\tlearn: 0.0993812\ttest: 0.1067425\tbest: 0.1067425 (12600)\ttotal: 6m 4s\tremaining: 7h 56m 34s\n",
      "12900:\tlearn: 0.0991840\ttest: 0.1066571\tbest: 0.1066571 (12900)\ttotal: 6m 13s\tremaining: 7h 56m 19s\n",
      "13200:\tlearn: 0.0989643\ttest: 0.1065700\tbest: 0.1065682 (13193)\ttotal: 6m 22s\tremaining: 7h 56m 3s\n",
      "13500:\tlearn: 0.0987761\ttest: 0.1064971\tbest: 0.1064971 (13499)\ttotal: 6m 30s\tremaining: 7h 55m 44s\n",
      "13800:\tlearn: 0.0985701\ttest: 0.1064271\tbest: 0.1064271 (13800)\ttotal: 6m 39s\tremaining: 7h 55m 28s\n",
      "14100:\tlearn: 0.0983904\ttest: 0.1063519\tbest: 0.1063519 (14100)\ttotal: 6m 47s\tremaining: 7h 55m 22s\n",
      "14400:\tlearn: 0.0981887\ttest: 0.1062752\tbest: 0.1062752 (14400)\ttotal: 6m 56s\tremaining: 7h 55m 6s\n",
      "14700:\tlearn: 0.0980061\ttest: 0.1062019\tbest: 0.1062019 (14700)\ttotal: 7m 5s\tremaining: 7h 54m 54s\n",
      "15000:\tlearn: 0.0978348\ttest: 0.1061440\tbest: 0.1061440 (15000)\ttotal: 7m 13s\tremaining: 7h 54m 42s\n",
      "15300:\tlearn: 0.0976437\ttest: 0.1060742\tbest: 0.1060742 (15300)\ttotal: 7m 22s\tremaining: 7h 54m 25s\n",
      "15600:\tlearn: 0.0974630\ttest: 0.1060231\tbest: 0.1060231 (15600)\ttotal: 7m 30s\tremaining: 7h 54m 10s\n",
      "15900:\tlearn: 0.0972960\ttest: 0.1059612\tbest: 0.1059612 (15900)\ttotal: 7m 39s\tremaining: 7h 53m 59s\n",
      "16200:\tlearn: 0.0971222\ttest: 0.1058914\tbest: 0.1058905 (16196)\ttotal: 7m 48s\tremaining: 7h 53m 51s\n",
      "16500:\tlearn: 0.0969542\ttest: 0.1058342\tbest: 0.1058342 (16500)\ttotal: 7m 56s\tremaining: 7h 53m 38s\n",
      "16800:\tlearn: 0.0968094\ttest: 0.1057778\tbest: 0.1057778 (16800)\ttotal: 8m 5s\tremaining: 7h 53m 32s\n",
      "17100:\tlearn: 0.0966555\ttest: 0.1057424\tbest: 0.1057423 (17079)\ttotal: 8m 14s\tremaining: 7h 53m 16s\n",
      "17400:\tlearn: 0.0964828\ttest: 0.1056845\tbest: 0.1056838 (17394)\ttotal: 8m 22s\tremaining: 7h 53m 4s\n",
      "17700:\tlearn: 0.0963097\ttest: 0.1056443\tbest: 0.1056442 (17698)\ttotal: 8m 31s\tremaining: 7h 52m 54s\n",
      "18000:\tlearn: 0.0961640\ttest: 0.1056033\tbest: 0.1056033 (17999)\ttotal: 8m 39s\tremaining: 7h 52m 47s\n",
      "18300:\tlearn: 0.0960212\ttest: 0.1055438\tbest: 0.1055438 (18300)\ttotal: 8m 48s\tremaining: 7h 52m 37s\n",
      "18600:\tlearn: 0.0958810\ttest: 0.1055042\tbest: 0.1055029 (18594)\ttotal: 8m 57s\tremaining: 7h 52m 26s\n",
      "18900:\tlearn: 0.0957455\ttest: 0.1054515\tbest: 0.1054512 (18898)\ttotal: 9m 5s\tremaining: 7h 52m 13s\n",
      "19200:\tlearn: 0.0956182\ttest: 0.1054094\tbest: 0.1054094 (19200)\ttotal: 9m 14s\tremaining: 7h 52m 1s\n",
      "19500:\tlearn: 0.0954899\ttest: 0.1053761\tbest: 0.1053761 (19500)\ttotal: 9m 23s\tremaining: 7h 51m 50s\n",
      "19800:\tlearn: 0.0953548\ttest: 0.1053410\tbest: 0.1053410 (19799)\ttotal: 9m 31s\tremaining: 7h 51m 39s\n",
      "20100:\tlearn: 0.0952369\ttest: 0.1053116\tbest: 0.1053097 (20068)\ttotal: 9m 40s\tremaining: 7h 51m 28s\n",
      "20400:\tlearn: 0.0951098\ttest: 0.1052776\tbest: 0.1052773 (20389)\ttotal: 9m 48s\tremaining: 7h 51m 19s\n",
      "20700:\tlearn: 0.0949705\ttest: 0.1052429\tbest: 0.1052385 (20670)\ttotal: 9m 57s\tremaining: 7h 51m 7s\n",
      "21000:\tlearn: 0.0948256\ttest: 0.1052024\tbest: 0.1052015 (20987)\ttotal: 10m 6s\tremaining: 7h 50m 55s\n",
      "21300:\tlearn: 0.0947050\ttest: 0.1051766\tbest: 0.1051765 (21293)\ttotal: 10m 14s\tremaining: 7h 50m 45s\n",
      "21600:\tlearn: 0.0945778\ttest: 0.1051418\tbest: 0.1051398 (21587)\ttotal: 10m 23s\tremaining: 7h 50m 36s\n",
      "21900:\tlearn: 0.0944561\ttest: 0.1051170\tbest: 0.1051166 (21899)\ttotal: 10m 32s\tremaining: 7h 50m 25s\n",
      "22200:\tlearn: 0.0943385\ttest: 0.1050824\tbest: 0.1050823 (22195)\ttotal: 10m 40s\tremaining: 7h 50m 15s\n",
      "22500:\tlearn: 0.0942254\ttest: 0.1050548\tbest: 0.1050548 (22499)\ttotal: 10m 49s\tremaining: 7h 50m 4s\n",
      "22800:\tlearn: 0.0941050\ttest: 0.1050266\tbest: 0.1050266 (22800)\ttotal: 10m 57s\tremaining: 7h 49m 54s\n",
      "23100:\tlearn: 0.0939770\ttest: 0.1049945\tbest: 0.1049945 (23100)\ttotal: 11m 6s\tremaining: 7h 49m 39s\n",
      "23400:\tlearn: 0.0938668\ttest: 0.1049685\tbest: 0.1049685 (23400)\ttotal: 11m 15s\tremaining: 7h 49m 30s\n",
      "23700:\tlearn: 0.0937757\ttest: 0.1049514\tbest: 0.1049512 (23697)\ttotal: 11m 23s\tremaining: 7h 49m 18s\n",
      "24000:\tlearn: 0.0936692\ttest: 0.1049314\tbest: 0.1049314 (24000)\ttotal: 11m 32s\tremaining: 7h 49m 7s\n",
      "24300:\tlearn: 0.0935636\ttest: 0.1048938\tbest: 0.1048923 (24274)\ttotal: 11m 40s\tremaining: 7h 48m 54s\n",
      "24600:\tlearn: 0.0934707\ttest: 0.1048611\tbest: 0.1048611 (24600)\ttotal: 11m 49s\tremaining: 7h 48m 43s\n",
      "24900:\tlearn: 0.0933669\ttest: 0.1048442\tbest: 0.1048424 (24880)\ttotal: 11m 57s\tremaining: 7h 48m 30s\n",
      "25200:\tlearn: 0.0932676\ttest: 0.1048129\tbest: 0.1048129 (25189)\ttotal: 12m 6s\tremaining: 7h 48m 18s\n",
      "25500:\tlearn: 0.0931531\ttest: 0.1047956\tbest: 0.1047951 (25459)\ttotal: 12m 15s\tremaining: 7h 48m 7s\n",
      "25800:\tlearn: 0.0930600\ttest: 0.1047826\tbest: 0.1047826 (25800)\ttotal: 12m 23s\tremaining: 7h 47m 54s\n",
      "26100:\tlearn: 0.0929604\ttest: 0.1047663\tbest: 0.1047661 (26091)\ttotal: 12m 32s\tremaining: 7h 47m 40s\n",
      "26400:\tlearn: 0.0928414\ttest: 0.1047572\tbest: 0.1047572 (26400)\ttotal: 12m 40s\tremaining: 7h 47m 32s\n",
      "26700:\tlearn: 0.0927353\ttest: 0.1047348\tbest: 0.1047328 (26688)\ttotal: 12m 49s\tremaining: 7h 47m 21s\n",
      "27000:\tlearn: 0.0926328\ttest: 0.1047194\tbest: 0.1047194 (27000)\ttotal: 12m 57s\tremaining: 7h 47m 11s\n",
      "27300:\tlearn: 0.0925365\ttest: 0.1047032\tbest: 0.1047031 (27289)\ttotal: 13m 6s\tremaining: 7h 47m 3s\n",
      "27600:\tlearn: 0.0924477\ttest: 0.1046824\tbest: 0.1046822 (27593)\ttotal: 13m 15s\tremaining: 7h 46m 52s\n",
      "27900:\tlearn: 0.0923441\ttest: 0.1046576\tbest: 0.1046576 (27900)\ttotal: 13m 23s\tremaining: 7h 46m 43s\n",
      "28200:\tlearn: 0.0922511\ttest: 0.1046335\tbest: 0.1046332 (28195)\ttotal: 13m 32s\tremaining: 7h 46m 36s\n",
      "28500:\tlearn: 0.0921605\ttest: 0.1046064\tbest: 0.1046064 (28500)\ttotal: 13m 41s\tremaining: 7h 46m 32s\n",
      "28800:\tlearn: 0.0920827\ttest: 0.1045936\tbest: 0.1045910 (28782)\ttotal: 13m 49s\tremaining: 7h 46m 21s\n",
      "29100:\tlearn: 0.0919898\ttest: 0.1045730\tbest: 0.1045705 (29086)\ttotal: 13m 58s\tremaining: 7h 46m 11s\n",
      "29400:\tlearn: 0.0918979\ttest: 0.1045530\tbest: 0.1045529 (29397)\ttotal: 14m 6s\tremaining: 7h 46m\n",
      "29700:\tlearn: 0.0918114\ttest: 0.1045477\tbest: 0.1045452 (29668)\ttotal: 14m 15s\tremaining: 7h 45m 48s\n",
      "30000:\tlearn: 0.0917226\ttest: 0.1045298\tbest: 0.1045298 (29999)\ttotal: 14m 24s\tremaining: 7h 45m 35s\n",
      "30300:\tlearn: 0.0916254\ttest: 0.1045266\tbest: 0.1045236 (30234)\ttotal: 14m 32s\tremaining: 7h 45m 25s\n",
      "30600:\tlearn: 0.0915494\ttest: 0.1045197\tbest: 0.1045189 (30485)\ttotal: 14m 41s\tremaining: 7h 45m 11s\n",
      "30900:\tlearn: 0.0914557\ttest: 0.1045010\tbest: 0.1045010 (30900)\ttotal: 14m 49s\tremaining: 7h 45m 2s\n",
      "31200:\tlearn: 0.0913794\ttest: 0.1044853\tbest: 0.1044848 (31199)\ttotal: 14m 58s\tremaining: 7h 44m 51s\n",
      "31500:\tlearn: 0.0912883\ttest: 0.1044734\tbest: 0.1044734 (31500)\ttotal: 15m 6s\tremaining: 7h 44m 41s\n",
      "31800:\tlearn: 0.0911976\ttest: 0.1044604\tbest: 0.1044579 (31778)\ttotal: 15m 15s\tremaining: 7h 44m 30s\n",
      "32100:\tlearn: 0.0911219\ttest: 0.1044467\tbest: 0.1044450 (32060)\ttotal: 15m 23s\tremaining: 7h 44m 18s\n",
      "32400:\tlearn: 0.0910420\ttest: 0.1044418\tbest: 0.1044408 (32352)\ttotal: 15m 32s\tremaining: 7h 44m 6s\n",
      "32700:\tlearn: 0.0909654\ttest: 0.1044257\tbest: 0.1044235 (32629)\ttotal: 15m 40s\tremaining: 7h 43m 54s\n",
      "33000:\tlearn: 0.0908924\ttest: 0.1044113\tbest: 0.1044113 (33000)\ttotal: 15m 49s\tremaining: 7h 43m 42s\n",
      "33300:\tlearn: 0.0908213\ttest: 0.1044039\tbest: 0.1044039 (33300)\ttotal: 15m 58s\tremaining: 7h 43m 31s\n",
      "33600:\tlearn: 0.0907443\ttest: 0.1043941\tbest: 0.1043932 (33597)\ttotal: 16m 6s\tremaining: 7h 43m 19s\n",
      "33900:\tlearn: 0.0906560\ttest: 0.1043783\tbest: 0.1043767 (33897)\ttotal: 16m 15s\tremaining: 7h 43m 9s\n",
      "34200:\tlearn: 0.0905777\ttest: 0.1043705\tbest: 0.1043691 (34129)\ttotal: 16m 23s\tremaining: 7h 42m 58s\n",
      "34500:\tlearn: 0.0905113\ttest: 0.1043610\tbest: 0.1043609 (34494)\ttotal: 16m 32s\tremaining: 7h 42m 47s\n",
      "34800:\tlearn: 0.0904306\ttest: 0.1043565\tbest: 0.1043565 (34800)\ttotal: 16m 40s\tremaining: 7h 42m 36s\n",
      "35100:\tlearn: 0.0903555\ttest: 0.1043432\tbest: 0.1043429 (35099)\ttotal: 16m 49s\tremaining: 7h 42m 25s\n",
      "35400:\tlearn: 0.0902689\ttest: 0.1043346\tbest: 0.1043342 (35391)\ttotal: 16m 57s\tremaining: 7h 42m 13s\n",
      "35700:\tlearn: 0.0901931\ttest: 0.1043286\tbest: 0.1043266 (35608)\ttotal: 17m 6s\tremaining: 7h 42m 1s\n",
      "36000:\tlearn: 0.0901247\ttest: 0.1043137\tbest: 0.1043131 (35991)\ttotal: 17m 14s\tremaining: 7h 41m 51s\n",
      "36300:\tlearn: 0.0900529\ttest: 0.1043061\tbest: 0.1043025 (36261)\ttotal: 17m 23s\tremaining: 7h 41m 41s\n",
      "36600:\tlearn: 0.0899770\ttest: 0.1042964\tbest: 0.1042951 (36589)\ttotal: 17m 32s\tremaining: 7h 41m 32s\n",
      "36900:\tlearn: 0.0899083\ttest: 0.1042866\tbest: 0.1042866 (36890)\ttotal: 17m 40s\tremaining: 7h 41m 21s\n",
      "37200:\tlearn: 0.0898355\ttest: 0.1042721\tbest: 0.1042685 (37176)\ttotal: 17m 49s\tremaining: 7h 41m 11s\n",
      "37500:\tlearn: 0.0897689\ttest: 0.1042688\tbest: 0.1042657 (37382)\ttotal: 17m 57s\tremaining: 7h 41m 1s\n",
      "37800:\tlearn: 0.0897027\ttest: 0.1042631\tbest: 0.1042597 (37640)\ttotal: 18m 6s\tremaining: 7h 40m 51s\n",
      "38100:\tlearn: 0.0896330\ttest: 0.1042556\tbest: 0.1042544 (38003)\ttotal: 18m 15s\tremaining: 7h 40m 49s\n",
      "38400:\tlearn: 0.0895668\ttest: 0.1042551\tbest: 0.1042510 (38366)\ttotal: 18m 23s\tremaining: 7h 40m 44s\n",
      "38700:\tlearn: 0.0895025\ttest: 0.1042375\tbest: 0.1042375 (38700)\ttotal: 18m 32s\tremaining: 7h 40m 34s\n",
      "39000:\tlearn: 0.0894391\ttest: 0.1042324\tbest: 0.1042297 (38770)\ttotal: 18m 41s\tremaining: 7h 40m 22s\n",
      "Stopped by overfitting detector  (300 iterations wait)\n",
      "\n",
      "bestTest = 0.1042296713\n",
      "bestIteration = 38770\n",
      "\n",
      "Shrink model to first 38771 iterations.\n",
      "fold n°3\n",
      "0:\tlearn: 0.9754326\ttest: 0.9741973\tbest: 0.9741973 (0)\ttotal: 31.2ms\tremaining: 8h 39m 11s\n",
      "300:\tlearn: 0.1703328\ttest: 0.1731365\tbest: 0.1731365 (300)\ttotal: 8.84s\tremaining: 8h 9m 19s\n",
      "600:\tlearn: 0.1444306\ttest: 0.1480348\tbest: 0.1480348 (600)\ttotal: 17.5s\tremaining: 8h 5m 51s\n",
      "900:\tlearn: 0.1335414\ttest: 0.1375311\tbest: 0.1375311 (900)\ttotal: 26.3s\tremaining: 8h 6m 55s\n",
      "1200:\tlearn: 0.1271621\ttest: 0.1313784\tbest: 0.1313784 (1200)\ttotal: 35.2s\tremaining: 8h 7m 59s\n",
      "1500:\tlearn: 0.1231596\ttest: 0.1275667\tbest: 0.1275667 (1500)\ttotal: 43.9s\tremaining: 8h 6m 18s\n",
      "1800:\tlearn: 0.1203061\ttest: 0.1248635\tbest: 0.1248635 (1800)\ttotal: 52.5s\tremaining: 8h 4m 52s\n",
      "2100:\tlearn: 0.1180797\ttest: 0.1227720\tbest: 0.1227720 (2100)\ttotal: 1m 1s\tremaining: 8h 3m 59s\n",
      "2400:\tlearn: 0.1162387\ttest: 0.1210618\tbest: 0.1210618 (2400)\ttotal: 1m 9s\tremaining: 8h 3m\n",
      "2700:\tlearn: 0.1146791\ttest: 0.1196468\tbest: 0.1196468 (2700)\ttotal: 1m 18s\tremaining: 8h 2m 35s\n",
      "3000:\tlearn: 0.1133596\ttest: 0.1184303\tbest: 0.1184303 (3000)\ttotal: 1m 27s\tremaining: 8h 1m 52s\n",
      "3300:\tlearn: 0.1122538\ttest: 0.1174373\tbest: 0.1174373 (3300)\ttotal: 1m 35s\tremaining: 8h 1m 38s\n",
      "3600:\tlearn: 0.1112592\ttest: 0.1165460\tbest: 0.1165460 (3600)\ttotal: 1m 44s\tremaining: 8h 1m 34s\n",
      "3900:\tlearn: 0.1103981\ttest: 0.1158264\tbest: 0.1158264 (3900)\ttotal: 1m 53s\tremaining: 8h 1m 6s\n",
      "4200:\tlearn: 0.1096147\ttest: 0.1151639\tbest: 0.1151639 (4200)\ttotal: 2m 1s\tremaining: 8h 54s\n",
      "4500:\tlearn: 0.1089004\ttest: 0.1145756\tbest: 0.1145756 (4500)\ttotal: 2m 10s\tremaining: 8h 15s\n",
      "4800:\tlearn: 0.1082231\ttest: 0.1140339\tbest: 0.1140339 (4800)\ttotal: 2m 19s\tremaining: 8h 13s\n",
      "5100:\tlearn: 0.1075983\ttest: 0.1135130\tbest: 0.1135130 (5100)\ttotal: 2m 27s\tremaining: 8h 5s\n",
      "5400:\tlearn: 0.1070102\ttest: 0.1130585\tbest: 0.1130585 (5400)\ttotal: 2m 36s\tremaining: 8h 15s\n",
      "5700:\tlearn: 0.1064658\ttest: 0.1126645\tbest: 0.1126645 (5700)\ttotal: 2m 45s\tremaining: 8h 45s\n",
      "6000:\tlearn: 0.1059867\ttest: 0.1123280\tbest: 0.1123280 (6000)\ttotal: 2m 54s\tremaining: 8h 51s\n",
      "6300:\tlearn: 0.1055049\ttest: 0.1119627\tbest: 0.1119627 (6300)\ttotal: 3m 2s\tremaining: 8h 30s\n",
      "6600:\tlearn: 0.1050848\ttest: 0.1116669\tbest: 0.1116669 (6600)\ttotal: 3m 11s\tremaining: 8h 5s\n",
      "6900:\tlearn: 0.1046812\ttest: 0.1113920\tbest: 0.1113920 (6900)\ttotal: 3m 20s\tremaining: 7h 59m 48s\n",
      "7200:\tlearn: 0.1042722\ttest: 0.1111216\tbest: 0.1111216 (7200)\ttotal: 3m 28s\tremaining: 7h 59m 22s\n",
      "7500:\tlearn: 0.1038587\ttest: 0.1108683\tbest: 0.1108683 (7500)\ttotal: 3m 37s\tremaining: 7h 59m 12s\n",
      "7800:\tlearn: 0.1035083\ttest: 0.1106476\tbest: 0.1106476 (7800)\ttotal: 3m 45s\tremaining: 7h 58m 55s\n",
      "8100:\tlearn: 0.1031805\ttest: 0.1104303\tbest: 0.1104303 (8100)\ttotal: 3m 54s\tremaining: 7h 58m 36s\n",
      "8400:\tlearn: 0.1028564\ttest: 0.1102510\tbest: 0.1102510 (8400)\ttotal: 4m 3s\tremaining: 7h 58m 25s\n",
      "8700:\tlearn: 0.1025435\ttest: 0.1100558\tbest: 0.1100558 (8700)\ttotal: 4m 11s\tremaining: 7h 58m 5s\n",
      "9000:\tlearn: 0.1022121\ttest: 0.1098607\tbest: 0.1098607 (9000)\ttotal: 4m 20s\tremaining: 7h 57m 49s\n",
      "9300:\tlearn: 0.1019216\ttest: 0.1096993\tbest: 0.1096993 (9300)\ttotal: 4m 29s\tremaining: 7h 57m 35s\n",
      "9600:\tlearn: 0.1016552\ttest: 0.1095545\tbest: 0.1095545 (9600)\ttotal: 4m 37s\tremaining: 7h 57m 36s\n",
      "9900:\tlearn: 0.1013805\ttest: 0.1093954\tbest: 0.1093954 (9900)\ttotal: 4m 46s\tremaining: 7h 57m 18s\n",
      "10200:\tlearn: 0.1011038\ttest: 0.1092357\tbest: 0.1092357 (10200)\ttotal: 4m 55s\tremaining: 7h 57m 6s\n",
      "10500:\tlearn: 0.1008440\ttest: 0.1091051\tbest: 0.1091050 (10499)\ttotal: 5m 3s\tremaining: 7h 56m 47s\n",
      "10800:\tlearn: 0.1006114\ttest: 0.1089951\tbest: 0.1089950 (10799)\ttotal: 5m 12s\tremaining: 7h 56m 30s\n",
      "11100:\tlearn: 0.1003663\ttest: 0.1088734\tbest: 0.1088734 (11099)\ttotal: 5m 20s\tremaining: 7h 56m 16s\n",
      "11400:\tlearn: 0.1001163\ttest: 0.1087450\tbest: 0.1087450 (11400)\ttotal: 5m 29s\tremaining: 7h 56m 9s\n",
      "11700:\tlearn: 0.0998806\ttest: 0.1086402\tbest: 0.1086401 (11699)\ttotal: 5m 38s\tremaining: 7h 56m 5s\n",
      "12000:\tlearn: 0.0996559\ttest: 0.1085328\tbest: 0.1085328 (12000)\ttotal: 5m 46s\tremaining: 7h 56m 1s\n",
      "12300:\tlearn: 0.0994477\ttest: 0.1084344\tbest: 0.1084338 (12299)\ttotal: 5m 55s\tremaining: 7h 55m 38s\n",
      "12600:\tlearn: 0.0992572\ttest: 0.1083502\tbest: 0.1083502 (12600)\ttotal: 6m 3s\tremaining: 7h 55m 21s\n",
      "12900:\tlearn: 0.0990589\ttest: 0.1082664\tbest: 0.1082664 (12900)\ttotal: 6m 12s\tremaining: 7h 55m 9s\n",
      "13200:\tlearn: 0.0988700\ttest: 0.1081801\tbest: 0.1081801 (13194)\ttotal: 6m 21s\tremaining: 7h 54m 54s\n",
      "13500:\tlearn: 0.0986639\ttest: 0.1080957\tbest: 0.1080956 (13499)\ttotal: 6m 29s\tremaining: 7h 54m 42s\n",
      "13800:\tlearn: 0.0984627\ttest: 0.1080082\tbest: 0.1080082 (13800)\ttotal: 6m 38s\tremaining: 7h 54m 26s\n",
      "14100:\tlearn: 0.0982951\ttest: 0.1079373\tbest: 0.1079372 (14099)\ttotal: 6m 46s\tremaining: 7h 54m 7s\n",
      "14400:\tlearn: 0.0981002\ttest: 0.1078469\tbest: 0.1078469 (14399)\ttotal: 6m 55s\tremaining: 7h 53m 48s\n",
      "14700:\tlearn: 0.0979267\ttest: 0.1077691\tbest: 0.1077691 (14700)\ttotal: 7m 3s\tremaining: 7h 53m 37s\n",
      "15000:\tlearn: 0.0977562\ttest: 0.1077066\tbest: 0.1077065 (14999)\ttotal: 7m 12s\tremaining: 7h 53m 22s\n",
      "15300:\tlearn: 0.0975689\ttest: 0.1076373\tbest: 0.1076373 (15300)\ttotal: 7m 21s\tremaining: 7h 53m 9s\n",
      "15600:\tlearn: 0.0973981\ttest: 0.1075734\tbest: 0.1075734 (15600)\ttotal: 7m 29s\tremaining: 7h 52m 59s\n",
      "15900:\tlearn: 0.0972192\ttest: 0.1074933\tbest: 0.1074933 (15899)\ttotal: 7m 38s\tremaining: 7h 52m 44s\n",
      "16200:\tlearn: 0.0970576\ttest: 0.1074371\tbest: 0.1074371 (16200)\ttotal: 7m 46s\tremaining: 7h 52m 31s\n",
      "16500:\tlearn: 0.0969048\ttest: 0.1073843\tbest: 0.1073843 (16500)\ttotal: 7m 55s\tremaining: 7h 52m 26s\n",
      "16800:\tlearn: 0.0967633\ttest: 0.1073298\tbest: 0.1073298 (16800)\ttotal: 8m 4s\tremaining: 7h 52m 14s\n",
      "17100:\tlearn: 0.0966014\ttest: 0.1072686\tbest: 0.1072686 (17100)\ttotal: 8m 12s\tremaining: 7h 52m 1s\n",
      "17400:\tlearn: 0.0964550\ttest: 0.1072229\tbest: 0.1072229 (17400)\ttotal: 8m 21s\tremaining: 7h 51m 50s\n",
      "17700:\tlearn: 0.0963161\ttest: 0.1071868\tbest: 0.1071868 (17700)\ttotal: 8m 29s\tremaining: 7h 51m 35s\n",
      "18000:\tlearn: 0.0961653\ttest: 0.1071338\tbest: 0.1071338 (18000)\ttotal: 8m 38s\tremaining: 7h 51m 17s\n",
      "18300:\tlearn: 0.0960229\ttest: 0.1070992\tbest: 0.1070987 (18294)\ttotal: 8m 46s\tremaining: 7h 51m 1s\n",
      "18600:\tlearn: 0.0958803\ttest: 0.1070556\tbest: 0.1070555 (18599)\ttotal: 8m 55s\tremaining: 7h 50m 47s\n",
      "18900:\tlearn: 0.0957268\ttest: 0.1070326\tbest: 0.1070312 (18889)\ttotal: 9m 3s\tremaining: 7h 50m 35s\n",
      "19200:\tlearn: 0.0955972\ttest: 0.1069870\tbest: 0.1069863 (19195)\ttotal: 9m 12s\tremaining: 7h 50m 23s\n",
      "19500:\tlearn: 0.0954457\ttest: 0.1069498\tbest: 0.1069497 (19498)\ttotal: 9m 21s\tremaining: 7h 50m 12s\n",
      "19800:\tlearn: 0.0953078\ttest: 0.1069226\tbest: 0.1069226 (19794)\ttotal: 9m 29s\tremaining: 7h 49m 58s\n",
      "20100:\tlearn: 0.0951921\ttest: 0.1068843\tbest: 0.1068842 (20099)\ttotal: 9m 38s\tremaining: 7h 49m 45s\n",
      "20400:\tlearn: 0.0950562\ttest: 0.1068474\tbest: 0.1068470 (20373)\ttotal: 9m 46s\tremaining: 7h 49m 35s\n",
      "20700:\tlearn: 0.0949366\ttest: 0.1068031\tbest: 0.1068028 (20698)\ttotal: 9m 55s\tremaining: 7h 49m 23s\n",
      "21000:\tlearn: 0.0948133\ttest: 0.1067747\tbest: 0.1067716 (20982)\ttotal: 10m 3s\tremaining: 7h 49m 12s\n",
      "21300:\tlearn: 0.0946927\ttest: 0.1067324\tbest: 0.1067324 (21300)\ttotal: 10m 12s\tremaining: 7h 48m 59s\n",
      "21600:\tlearn: 0.0945595\ttest: 0.1066998\tbest: 0.1066998 (21597)\ttotal: 10m 21s\tremaining: 7h 48m 50s\n",
      "21900:\tlearn: 0.0944420\ttest: 0.1066665\tbest: 0.1066662 (21899)\ttotal: 10m 29s\tremaining: 7h 48m 37s\n",
      "22200:\tlearn: 0.0943189\ttest: 0.1066257\tbest: 0.1066226 (22162)\ttotal: 10m 38s\tremaining: 7h 48m 27s\n",
      "22500:\tlearn: 0.0941971\ttest: 0.1065820\tbest: 0.1065820 (22500)\ttotal: 10m 46s\tremaining: 7h 48m 18s\n",
      "22800:\tlearn: 0.0940955\ttest: 0.1065619\tbest: 0.1065612 (22798)\ttotal: 10m 55s\tremaining: 7h 48m 7s\n",
      "23100:\tlearn: 0.0939770\ttest: 0.1065199\tbest: 0.1065199 (23100)\ttotal: 11m 3s\tremaining: 7h 47m 54s\n",
      "23400:\tlearn: 0.0938631\ttest: 0.1064908\tbest: 0.1064900 (23386)\ttotal: 11m 12s\tremaining: 7h 47m 43s\n",
      "23700:\tlearn: 0.0937412\ttest: 0.1064531\tbest: 0.1064531 (23700)\ttotal: 11m 21s\tremaining: 7h 47m 33s\n",
      "24000:\tlearn: 0.0936353\ttest: 0.1064339\tbest: 0.1064308 (23897)\ttotal: 11m 29s\tremaining: 7h 47m 23s\n",
      "24300:\tlearn: 0.0935263\ttest: 0.1064029\tbest: 0.1064029 (24300)\ttotal: 11m 38s\tremaining: 7h 47m 10s\n",
      "24600:\tlearn: 0.0934410\ttest: 0.1063756\tbest: 0.1063756 (24600)\ttotal: 11m 46s\tremaining: 7h 46m 58s\n",
      "24900:\tlearn: 0.0933393\ttest: 0.1063472\tbest: 0.1063472 (24900)\ttotal: 11m 55s\tremaining: 7h 46m 45s\n",
      "25200:\tlearn: 0.0932319\ttest: 0.1063237\tbest: 0.1063236 (25197)\ttotal: 12m 3s\tremaining: 7h 46m 36s\n",
      "25500:\tlearn: 0.0931205\ttest: 0.1063062\tbest: 0.1063062 (25500)\ttotal: 12m 12s\tremaining: 7h 46m 29s\n",
      "25800:\tlearn: 0.0930168\ttest: 0.1062845\tbest: 0.1062831 (25773)\ttotal: 12m 21s\tremaining: 7h 46m 20s\n",
      "26100:\tlearn: 0.0929197\ttest: 0.1062641\tbest: 0.1062641 (26100)\ttotal: 12m 29s\tremaining: 7h 46m 11s\n",
      "26400:\tlearn: 0.0928134\ttest: 0.1062335\tbest: 0.1062325 (26390)\ttotal: 12m 38s\tremaining: 7h 46m\n",
      "26700:\tlearn: 0.0927093\ttest: 0.1062089\tbest: 0.1062085 (26699)\ttotal: 12m 46s\tremaining: 7h 45m 45s\n",
      "27000:\tlearn: 0.0925983\ttest: 0.1061889\tbest: 0.1061888 (26999)\ttotal: 12m 55s\tremaining: 7h 45m 30s\n",
      "27300:\tlearn: 0.0925071\ttest: 0.1061596\tbest: 0.1061596 (27300)\ttotal: 13m 3s\tremaining: 7h 45m 18s\n",
      "27600:\tlearn: 0.0924036\ttest: 0.1061291\tbest: 0.1061290 (27599)\ttotal: 13m 12s\tremaining: 7h 45m 5s\n",
      "27900:\tlearn: 0.0923088\ttest: 0.1061065\tbest: 0.1061064 (27895)\ttotal: 13m 20s\tremaining: 7h 44m 50s\n",
      "28200:\tlearn: 0.0922135\ttest: 0.1060804\tbest: 0.1060791 (28196)\ttotal: 13m 29s\tremaining: 7h 44m 41s\n",
      "28500:\tlearn: 0.0921215\ttest: 0.1060540\tbest: 0.1060540 (28499)\ttotal: 13m 37s\tremaining: 7h 44m 28s\n",
      "28800:\tlearn: 0.0920269\ttest: 0.1060149\tbest: 0.1060148 (28798)\ttotal: 13m 46s\tremaining: 7h 44m 17s\n",
      "29100:\tlearn: 0.0919440\ttest: 0.1059983\tbest: 0.1059966 (29072)\ttotal: 13m 54s\tremaining: 7h 44m 6s\n",
      "29400:\tlearn: 0.0918512\ttest: 0.1059774\tbest: 0.1059764 (29375)\ttotal: 14m 3s\tremaining: 7h 43m 57s\n",
      "29700:\tlearn: 0.0917748\ttest: 0.1059625\tbest: 0.1059625 (29700)\ttotal: 14m 11s\tremaining: 7h 43m 46s\n",
      "30000:\tlearn: 0.0916887\ttest: 0.1059437\tbest: 0.1059435 (29992)\ttotal: 14m 20s\tremaining: 7h 43m 32s\n",
      "30300:\tlearn: 0.0916088\ttest: 0.1059305\tbest: 0.1059305 (30300)\ttotal: 14m 28s\tremaining: 7h 43m 17s\n",
      "30600:\tlearn: 0.0915280\ttest: 0.1059075\tbest: 0.1059075 (30600)\ttotal: 14m 37s\tremaining: 7h 43m 6s\n",
      "30900:\tlearn: 0.0914422\ttest: 0.1058926\tbest: 0.1058916 (30872)\ttotal: 14m 45s\tremaining: 7h 42m 51s\n",
      "31200:\tlearn: 0.0913608\ttest: 0.1058727\tbest: 0.1058723 (31198)\ttotal: 14m 54s\tremaining: 7h 42m 39s\n",
      "31500:\tlearn: 0.0912828\ttest: 0.1058668\tbest: 0.1058668 (31500)\ttotal: 15m 2s\tremaining: 7h 42m 35s\n",
      "31800:\tlearn: 0.0911921\ttest: 0.1058406\tbest: 0.1058403 (31786)\ttotal: 15m 11s\tremaining: 7h 42m 30s\n",
      "32100:\tlearn: 0.0911161\ttest: 0.1058318\tbest: 0.1058318 (32100)\ttotal: 15m 20s\tremaining: 7h 42m 23s\n",
      "32400:\tlearn: 0.0910396\ttest: 0.1058277\tbest: 0.1058273 (32393)\ttotal: 15m 28s\tremaining: 7h 42m 13s\n",
      "32700:\tlearn: 0.0909634\ttest: 0.1058126\tbest: 0.1058106 (32634)\ttotal: 15m 37s\tremaining: 7h 42m 1s\n",
      "33000:\tlearn: 0.0908814\ttest: 0.1057980\tbest: 0.1057973 (32998)\ttotal: 15m 45s\tremaining: 7h 41m 49s\n",
      "33300:\tlearn: 0.0908007\ttest: 0.1057870\tbest: 0.1057867 (33161)\ttotal: 15m 54s\tremaining: 7h 41m 38s\n",
      "33600:\tlearn: 0.0907161\ttest: 0.1057761\tbest: 0.1057761 (33600)\ttotal: 16m 2s\tremaining: 7h 41m 28s\n",
      "33900:\tlearn: 0.0906311\ttest: 0.1057736\tbest: 0.1057696 (33740)\ttotal: 16m 11s\tremaining: 7h 41m 17s\n",
      "34200:\tlearn: 0.0905565\ttest: 0.1057614\tbest: 0.1057606 (34178)\ttotal: 16m 19s\tremaining: 7h 41m 4s\n",
      "34500:\tlearn: 0.0904873\ttest: 0.1057391\tbest: 0.1057391 (34500)\ttotal: 16m 28s\tremaining: 7h 40m 55s\n",
      "34800:\tlearn: 0.0903972\ttest: 0.1057146\tbest: 0.1057138 (34784)\ttotal: 16m 36s\tremaining: 7h 40m 44s\n",
      "35100:\tlearn: 0.0903355\ttest: 0.1056980\tbest: 0.1056980 (35098)\ttotal: 16m 45s\tremaining: 7h 40m 33s\n",
      "35400:\tlearn: 0.0902620\ttest: 0.1056793\tbest: 0.1056780 (35373)\ttotal: 16m 53s\tremaining: 7h 40m 22s\n",
      "35700:\tlearn: 0.0901808\ttest: 0.1056609\tbest: 0.1056588 (35640)\ttotal: 17m 2s\tremaining: 7h 40m 10s\n",
      "36000:\tlearn: 0.0901131\ttest: 0.1056583\tbest: 0.1056552 (35937)\ttotal: 17m 10s\tremaining: 7h 40m\n",
      "36300:\tlearn: 0.0900429\ttest: 0.1056382\tbest: 0.1056379 (36297)\ttotal: 17m 19s\tremaining: 7h 39m 51s\n",
      "36600:\tlearn: 0.0899822\ttest: 0.1056249\tbest: 0.1056249 (36600)\ttotal: 17m 27s\tremaining: 7h 39m 40s\n",
      "36900:\tlearn: 0.0899143\ttest: 0.1056111\tbest: 0.1056104 (36881)\ttotal: 17m 36s\tremaining: 7h 39m 30s\n",
      "37200:\tlearn: 0.0898473\ttest: 0.1055967\tbest: 0.1055967 (37200)\ttotal: 17m 44s\tremaining: 7h 39m 19s\n",
      "37500:\tlearn: 0.0897746\ttest: 0.1055871\tbest: 0.1055837 (37427)\ttotal: 17m 53s\tremaining: 7h 39m 11s\n",
      "37800:\tlearn: 0.0897065\ttest: 0.1055713\tbest: 0.1055705 (37791)\ttotal: 18m 2s\tremaining: 7h 39m 2s\n",
      "38100:\tlearn: 0.0896299\ttest: 0.1055590\tbest: 0.1055590 (38100)\ttotal: 18m 10s\tremaining: 7h 38m 51s\n",
      "38400:\tlearn: 0.0895653\ttest: 0.1055401\tbest: 0.1055398 (38394)\ttotal: 18m 18s\tremaining: 7h 38m 39s\n",
      "38700:\tlearn: 0.0895062\ttest: 0.1055363\tbest: 0.1055343 (38616)\ttotal: 18m 27s\tremaining: 7h 38m 28s\n",
      "39000:\tlearn: 0.0894411\ttest: 0.1055325\tbest: 0.1055312 (38977)\ttotal: 18m 35s\tremaining: 7h 38m 17s\n",
      "39300:\tlearn: 0.0893692\ttest: 0.1055132\tbest: 0.1055131 (39295)\ttotal: 18m 44s\tremaining: 7h 38m 6s\n",
      "39600:\tlearn: 0.0893146\ttest: 0.1055061\tbest: 0.1055055 (39465)\ttotal: 18m 52s\tremaining: 7h 37m 55s\n",
      "39900:\tlearn: 0.0892536\ttest: 0.1054973\tbest: 0.1054937 (39843)\ttotal: 19m 1s\tremaining: 7h 37m 47s\n",
      "40200:\tlearn: 0.0891861\ttest: 0.1054847\tbest: 0.1054847 (40200)\ttotal: 19m 9s\tremaining: 7h 37m 35s\n",
      "40500:\tlearn: 0.0891128\ttest: 0.1054598\tbest: 0.1054598 (40500)\ttotal: 19m 18s\tremaining: 7h 37m 25s\n",
      "40800:\tlearn: 0.0890307\ttest: 0.1054536\tbest: 0.1054511 (40656)\ttotal: 19m 27s\tremaining: 7h 37m 24s\n",
      "41100:\tlearn: 0.0889718\ttest: 0.1054436\tbest: 0.1054406 (41062)\ttotal: 19m 35s\tremaining: 7h 37m 12s\n",
      "Stopped by overfitting detector  (300 iterations wait)\n",
      "\n",
      "bestTest = 0.1054405699\n",
      "bestIteration = 41062\n",
      "\n",
      "Shrink model to first 41063 iterations.\n",
      "fold n°4\n",
      "0:\tlearn: 0.9756721\ttest: 0.9720402\tbest: 0.9720402 (0)\ttotal: 32ms\tremaining: 8h 54m 7s\n",
      "300:\tlearn: 0.1698311\ttest: 0.1720564\tbest: 0.1720564 (300)\ttotal: 8.91s\tremaining: 8h 13m 16s\n",
      "600:\tlearn: 0.1441136\ttest: 0.1460725\tbest: 0.1460725 (600)\ttotal: 17.5s\tremaining: 8h 4m 56s\n",
      "900:\tlearn: 0.1338232\ttest: 0.1359209\tbest: 0.1359209 (900)\ttotal: 26.2s\tremaining: 8h 4m 33s\n",
      "1200:\tlearn: 0.1272937\ttest: 0.1295715\tbest: 0.1295715 (1200)\ttotal: 35s\tremaining: 8h 4m 25s\n",
      "1500:\tlearn: 0.1233825\ttest: 0.1258742\tbest: 0.1258742 (1500)\ttotal: 43.6s\tremaining: 8h 3m 38s\n",
      "1800:\tlearn: 0.1204531\ttest: 0.1230807\tbest: 0.1230807 (1800)\ttotal: 52.4s\tremaining: 8h 3m 39s\n",
      "2100:\tlearn: 0.1181937\ttest: 0.1209359\tbest: 0.1209359 (2100)\ttotal: 1m 1s\tremaining: 8h 4m 2s\n",
      "2400:\tlearn: 0.1164113\ttest: 0.1193264\tbest: 0.1193264 (2400)\ttotal: 1m 9s\tremaining: 8h 4m 18s\n",
      "2700:\tlearn: 0.1149524\ttest: 0.1180288\tbest: 0.1180288 (2700)\ttotal: 1m 18s\tremaining: 8h 4m 24s\n",
      "3000:\tlearn: 0.1136070\ttest: 0.1168238\tbest: 0.1168238 (3000)\ttotal: 1m 27s\tremaining: 8h 4m 19s\n",
      "3300:\tlearn: 0.1124613\ttest: 0.1158182\tbest: 0.1158182 (3300)\ttotal: 1m 36s\tremaining: 8h 4m 41s\n",
      "3600:\tlearn: 0.1114980\ttest: 0.1150232\tbest: 0.1150232 (3600)\ttotal: 1m 45s\tremaining: 8h 4m 26s\n",
      "3900:\tlearn: 0.1106469\ttest: 0.1143155\tbest: 0.1143155 (3900)\ttotal: 1m 53s\tremaining: 8h 4m 6s\n",
      "4200:\tlearn: 0.1098710\ttest: 0.1136605\tbest: 0.1136605 (4200)\ttotal: 2m 2s\tremaining: 8h 3m 45s\n",
      "4500:\tlearn: 0.1091771\ttest: 0.1130877\tbest: 0.1130877 (4500)\ttotal: 2m 11s\tremaining: 8h 3m 57s\n",
      "4800:\tlearn: 0.1085144\ttest: 0.1125590\tbest: 0.1125590 (4800)\ttotal: 2m 19s\tremaining: 8h 3m 38s\n",
      "5100:\tlearn: 0.1079174\ttest: 0.1121117\tbest: 0.1121117 (5100)\ttotal: 2m 28s\tremaining: 8h 3m 28s\n",
      "5400:\tlearn: 0.1073610\ttest: 0.1116881\tbest: 0.1116881 (5400)\ttotal: 2m 37s\tremaining: 8h 3m 34s\n",
      "5700:\tlearn: 0.1067898\ttest: 0.1112660\tbest: 0.1112660 (5700)\ttotal: 2m 46s\tremaining: 8h 3m 20s\n",
      "6000:\tlearn: 0.1062761\ttest: 0.1108927\tbest: 0.1108927 (6000)\ttotal: 2m 55s\tremaining: 8h 3m 12s\n",
      "6300:\tlearn: 0.1058030\ttest: 0.1105361\tbest: 0.1105361 (6300)\ttotal: 3m 3s\tremaining: 8h 2m 59s\n",
      "6600:\tlearn: 0.1053594\ttest: 0.1102373\tbest: 0.1102373 (6600)\ttotal: 3m 12s\tremaining: 8h 2m 56s\n",
      "6900:\tlearn: 0.1049012\ttest: 0.1099450\tbest: 0.1099450 (6900)\ttotal: 3m 21s\tremaining: 8h 2m 50s\n",
      "7200:\tlearn: 0.1044992\ttest: 0.1096872\tbest: 0.1096872 (7200)\ttotal: 3m 30s\tremaining: 8h 2m 45s\n",
      "7500:\tlearn: 0.1041129\ttest: 0.1094275\tbest: 0.1094274 (7499)\ttotal: 3m 38s\tremaining: 8h 2m 37s\n",
      "7800:\tlearn: 0.1037454\ttest: 0.1091760\tbest: 0.1091760 (7800)\ttotal: 3m 47s\tremaining: 8h 2m 30s\n",
      "8100:\tlearn: 0.1033963\ttest: 0.1089631\tbest: 0.1089631 (8100)\ttotal: 3m 56s\tremaining: 8h 2m 27s\n",
      "8400:\tlearn: 0.1030579\ttest: 0.1087669\tbest: 0.1087669 (8400)\ttotal: 4m 5s\tremaining: 8h 2m 23s\n",
      "8700:\tlearn: 0.1027526\ttest: 0.1086019\tbest: 0.1086019 (8700)\ttotal: 4m 14s\tremaining: 8h 2m 22s\n",
      "9000:\tlearn: 0.1024403\ttest: 0.1084141\tbest: 0.1084141 (9000)\ttotal: 4m 22s\tremaining: 8h 2m 14s\n",
      "9300:\tlearn: 0.1021507\ttest: 0.1082351\tbest: 0.1082351 (9300)\ttotal: 4m 31s\tremaining: 8h 2m 4s\n",
      "9600:\tlearn: 0.1018569\ttest: 0.1080728\tbest: 0.1080728 (9600)\ttotal: 4m 40s\tremaining: 8h 1m 54s\n",
      "9900:\tlearn: 0.1015818\ttest: 0.1079299\tbest: 0.1079299 (9900)\ttotal: 4m 48s\tremaining: 8h 1m 37s\n",
      "10200:\tlearn: 0.1013328\ttest: 0.1077895\tbest: 0.1077895 (10200)\ttotal: 4m 57s\tremaining: 8h 1m 30s\n",
      "10500:\tlearn: 0.1010879\ttest: 0.1076505\tbest: 0.1076505 (10500)\ttotal: 5m 6s\tremaining: 8h 1m 27s\n",
      "10800:\tlearn: 0.1008517\ttest: 0.1075427\tbest: 0.1075427 (10800)\ttotal: 5m 15s\tremaining: 8h 1m 15s\n",
      "11100:\tlearn: 0.1006365\ttest: 0.1074379\tbest: 0.1074379 (11100)\ttotal: 5m 24s\tremaining: 8h 1m 7s\n",
      "11400:\tlearn: 0.1004073\ttest: 0.1073312\tbest: 0.1073286 (11389)\ttotal: 5m 32s\tremaining: 8h 54s\n",
      "11700:\tlearn: 0.1001838\ttest: 0.1072222\tbest: 0.1072222 (11700)\ttotal: 5m 41s\tremaining: 8h 42s\n",
      "12000:\tlearn: 0.0999587\ttest: 0.1071199\tbest: 0.1071199 (12000)\ttotal: 5m 50s\tremaining: 8h 33s\n",
      "12300:\tlearn: 0.0997174\ttest: 0.1070190\tbest: 0.1070190 (12300)\ttotal: 5m 58s\tremaining: 8h 18s\n",
      "12600:\tlearn: 0.0994971\ttest: 0.1069160\tbest: 0.1069160 (12600)\ttotal: 6m 7s\tremaining: 8h 9s\n",
      "12900:\tlearn: 0.0993040\ttest: 0.1068119\tbest: 0.1068119 (12900)\ttotal: 6m 16s\tremaining: 7h 59m 55s\n",
      "13200:\tlearn: 0.0990942\ttest: 0.1067058\tbest: 0.1067058 (13200)\ttotal: 6m 25s\tremaining: 7h 59m 44s\n",
      "13500:\tlearn: 0.0988767\ttest: 0.1066130\tbest: 0.1066130 (13500)\ttotal: 6m 33s\tremaining: 7h 59m 31s\n",
      "13800:\tlearn: 0.0986940\ttest: 0.1065300\tbest: 0.1065300 (13800)\ttotal: 6m 42s\tremaining: 7h 59m 22s\n",
      "14100:\tlearn: 0.0984916\ttest: 0.1064454\tbest: 0.1064454 (14100)\ttotal: 6m 51s\tremaining: 7h 59m 12s\n",
      "14400:\tlearn: 0.0982981\ttest: 0.1063774\tbest: 0.1063773 (14398)\ttotal: 6m 59s\tremaining: 7h 58m 57s\n",
      "14700:\tlearn: 0.0981225\ttest: 0.1063232\tbest: 0.1063232 (14700)\ttotal: 7m 8s\tremaining: 7h 58m 50s\n",
      "15000:\tlearn: 0.0979451\ttest: 0.1062441\tbest: 0.1062441 (15000)\ttotal: 7m 17s\tremaining: 7h 58m 39s\n",
      "15300:\tlearn: 0.0977829\ttest: 0.1061847\tbest: 0.1061845 (15299)\ttotal: 7m 26s\tremaining: 7h 58m 27s\n",
      "15600:\tlearn: 0.0976149\ttest: 0.1061195\tbest: 0.1061195 (15598)\ttotal: 7m 34s\tremaining: 7h 58m 17s\n",
      "15900:\tlearn: 0.0974540\ttest: 0.1060561\tbest: 0.1060561 (15900)\ttotal: 7m 43s\tremaining: 7h 57m 58s\n",
      "16200:\tlearn: 0.0972999\ttest: 0.1059956\tbest: 0.1059956 (16200)\ttotal: 7m 52s\tremaining: 7h 57m 46s\n",
      "16500:\tlearn: 0.0971435\ttest: 0.1059356\tbest: 0.1059338 (16494)\ttotal: 8m\tremaining: 7h 57m 36s\n",
      "16800:\tlearn: 0.0969738\ttest: 0.1058605\tbest: 0.1058605 (16800)\ttotal: 8m 9s\tremaining: 7h 57m 21s\n",
      "17100:\tlearn: 0.0967945\ttest: 0.1058115\tbest: 0.1058115 (17100)\ttotal: 8m 18s\tremaining: 7h 57m 13s\n",
      "17400:\tlearn: 0.0966274\ttest: 0.1057568\tbest: 0.1057568 (17400)\ttotal: 8m 26s\tremaining: 7h 57m 4s\n",
      "17700:\tlearn: 0.0964709\ttest: 0.1057225\tbest: 0.1057210 (17667)\ttotal: 8m 35s\tremaining: 7h 56m 50s\n",
      "18000:\tlearn: 0.0963357\ttest: 0.1056680\tbest: 0.1056680 (18000)\ttotal: 8m 44s\tremaining: 7h 56m 42s\n",
      "18300:\tlearn: 0.0962031\ttest: 0.1056249\tbest: 0.1056249 (18300)\ttotal: 8m 52s\tremaining: 7h 56m 31s\n",
      "18600:\tlearn: 0.0960515\ttest: 0.1055665\tbest: 0.1055655 (18593)\ttotal: 9m 1s\tremaining: 7h 56m 20s\n",
      "18900:\tlearn: 0.0959035\ttest: 0.1055246\tbest: 0.1055246 (18900)\ttotal: 9m 10s\tremaining: 7h 56m 7s\n",
      "19200:\tlearn: 0.0957587\ttest: 0.1054888\tbest: 0.1054867 (19185)\ttotal: 9m 19s\tremaining: 7h 55m 59s\n",
      "19500:\tlearn: 0.0956233\ttest: 0.1054322\tbest: 0.1054322 (19500)\ttotal: 9m 27s\tremaining: 7h 55m 46s\n",
      "19800:\tlearn: 0.0954861\ttest: 0.1053841\tbest: 0.1053840 (19799)\ttotal: 9m 36s\tremaining: 7h 55m 35s\n",
      "20100:\tlearn: 0.0953654\ttest: 0.1053520\tbest: 0.1053516 (20096)\ttotal: 9m 45s\tremaining: 7h 55m 23s\n",
      "20400:\tlearn: 0.0952410\ttest: 0.1053140\tbest: 0.1053099 (20374)\ttotal: 9m 53s\tremaining: 7h 55m 14s\n",
      "20700:\tlearn: 0.0951098\ttest: 0.1052737\tbest: 0.1052730 (20697)\ttotal: 10m 2s\tremaining: 7h 55m 16s\n",
      "21000:\tlearn: 0.0949709\ttest: 0.1052312\tbest: 0.1052283 (20972)\ttotal: 10m 11s\tremaining: 7h 55m 17s\n",
      "21300:\tlearn: 0.0948529\ttest: 0.1051840\tbest: 0.1051836 (21295)\ttotal: 10m 20s\tremaining: 7h 55m 10s\n",
      "21600:\tlearn: 0.0947309\ttest: 0.1051452\tbest: 0.1051442 (21573)\ttotal: 10m 29s\tremaining: 7h 54m 59s\n",
      "21900:\tlearn: 0.0946123\ttest: 0.1051181\tbest: 0.1051170 (21867)\ttotal: 10m 37s\tremaining: 7h 54m 47s\n",
      "22200:\tlearn: 0.0944738\ttest: 0.1050720\tbest: 0.1050720 (22200)\ttotal: 10m 46s\tremaining: 7h 54m 36s\n",
      "22500:\tlearn: 0.0943516\ttest: 0.1050357\tbest: 0.1050357 (22499)\ttotal: 10m 55s\tremaining: 7h 54m 24s\n",
      "22800:\tlearn: 0.0942370\ttest: 0.1049945\tbest: 0.1049945 (22800)\ttotal: 11m 3s\tremaining: 7h 54m 11s\n",
      "23100:\tlearn: 0.0941121\ttest: 0.1049537\tbest: 0.1049527 (23097)\ttotal: 11m 12s\tremaining: 7h 54m 2s\n",
      "23400:\tlearn: 0.0939967\ttest: 0.1049465\tbest: 0.1049443 (23356)\ttotal: 11m 21s\tremaining: 7h 53m 56s\n",
      "23700:\tlearn: 0.0938862\ttest: 0.1049212\tbest: 0.1049211 (23697)\ttotal: 11m 30s\tremaining: 7h 53m 44s\n",
      "24000:\tlearn: 0.0937746\ttest: 0.1048866\tbest: 0.1048852 (23987)\ttotal: 11m 38s\tremaining: 7h 53m 30s\n",
      "24300:\tlearn: 0.0936567\ttest: 0.1048637\tbest: 0.1048637 (24300)\ttotal: 11m 47s\tremaining: 7h 53m 18s\n",
      "24600:\tlearn: 0.0935368\ttest: 0.1048423\tbest: 0.1048423 (24598)\ttotal: 11m 55s\tremaining: 7h 53m 4s\n",
      "24900:\tlearn: 0.0934251\ttest: 0.1048143\tbest: 0.1048143 (24899)\ttotal: 12m 4s\tremaining: 7h 52m 57s\n",
      "25200:\tlearn: 0.0933205\ttest: 0.1047800\tbest: 0.1047800 (25200)\ttotal: 12m 13s\tremaining: 7h 52m 50s\n",
      "25500:\tlearn: 0.0932167\ttest: 0.1047483\tbest: 0.1047474 (25490)\ttotal: 12m 22s\tremaining: 7h 52m 38s\n",
      "25800:\tlearn: 0.0931064\ttest: 0.1047181\tbest: 0.1047157 (25784)\ttotal: 12m 30s\tremaining: 7h 52m 26s\n",
      "26100:\tlearn: 0.0930157\ttest: 0.1047011\tbest: 0.1046994 (26057)\ttotal: 12m 39s\tremaining: 7h 52m 15s\n",
      "26400:\tlearn: 0.0929158\ttest: 0.1046906\tbest: 0.1046886 (26366)\ttotal: 12m 47s\tremaining: 7h 52m\n",
      "26700:\tlearn: 0.0928114\ttest: 0.1046737\tbest: 0.1046737 (26700)\ttotal: 12m 56s\tremaining: 7h 51m 46s\n",
      "27000:\tlearn: 0.0927222\ttest: 0.1046540\tbest: 0.1046536 (26999)\ttotal: 13m 5s\tremaining: 7h 51m 33s\n",
      "27300:\tlearn: 0.0926347\ttest: 0.1046473\tbest: 0.1046472 (27297)\ttotal: 13m 13s\tremaining: 7h 51m 22s\n",
      "27600:\tlearn: 0.0925386\ttest: 0.1046422\tbest: 0.1046419 (27597)\ttotal: 13m 22s\tremaining: 7h 51m 9s\n",
      "27900:\tlearn: 0.0924401\ttest: 0.1046111\tbest: 0.1046110 (27899)\ttotal: 13m 31s\tremaining: 7h 50m 58s\n",
      "28200:\tlearn: 0.0923527\ttest: 0.1045841\tbest: 0.1045837 (28190)\ttotal: 13m 39s\tremaining: 7h 50m 44s\n",
      "28500:\tlearn: 0.0922593\ttest: 0.1045654\tbest: 0.1045647 (28487)\ttotal: 13m 48s\tremaining: 7h 50m 30s\n",
      "28800:\tlearn: 0.0921634\ttest: 0.1045406\tbest: 0.1045404 (28799)\ttotal: 13m 56s\tremaining: 7h 50m 17s\n",
      "29100:\tlearn: 0.0920640\ttest: 0.1045275\tbest: 0.1045270 (29065)\ttotal: 14m 5s\tremaining: 7h 50m 5s\n",
      "29400:\tlearn: 0.0919761\ttest: 0.1045181\tbest: 0.1045172 (29395)\ttotal: 14m 14s\tremaining: 7h 49m 57s\n",
      "29700:\tlearn: 0.0918792\ttest: 0.1044980\tbest: 0.1044976 (29695)\ttotal: 14m 22s\tremaining: 7h 49m 48s\n",
      "30000:\tlearn: 0.0917860\ttest: 0.1044840\tbest: 0.1044827 (29958)\ttotal: 14m 31s\tremaining: 7h 49m 36s\n",
      "30300:\tlearn: 0.0916978\ttest: 0.1044665\tbest: 0.1044658 (30278)\ttotal: 14m 40s\tremaining: 7h 49m 30s\n",
      "30600:\tlearn: 0.0916106\ttest: 0.1044581\tbest: 0.1044581 (30600)\ttotal: 14m 48s\tremaining: 7h 49m 20s\n",
      "30900:\tlearn: 0.0915310\ttest: 0.1044439\tbest: 0.1044439 (30899)\ttotal: 14m 57s\tremaining: 7h 49m 9s\n",
      "31200:\tlearn: 0.0914502\ttest: 0.1044462\tbest: 0.1044423 (31030)\ttotal: 15m 6s\tremaining: 7h 49m\n",
      "31500:\tlearn: 0.0913681\ttest: 0.1044282\tbest: 0.1044282 (31500)\ttotal: 15m 14s\tremaining: 7h 48m 49s\n",
      "31800:\tlearn: 0.0912806\ttest: 0.1044105\tbest: 0.1044101 (31795)\ttotal: 15m 23s\tremaining: 7h 48m 39s\n",
      "32100:\tlearn: 0.0911960\ttest: 0.1044004\tbest: 0.1044004 (32100)\ttotal: 15m 32s\tremaining: 7h 48m 26s\n",
      "32400:\tlearn: 0.0911113\ttest: 0.1043924\tbest: 0.1043923 (32158)\ttotal: 15m 40s\tremaining: 7h 48m 14s\n",
      "32700:\tlearn: 0.0910285\ttest: 0.1043785\tbest: 0.1043780 (32698)\ttotal: 15m 49s\tremaining: 7h 48m 2s\n",
      "33000:\tlearn: 0.0909437\ttest: 0.1043729\tbest: 0.1043665 (32925)\ttotal: 15m 58s\tremaining: 7h 47m 53s\n",
      "33300:\tlearn: 0.0908570\ttest: 0.1043606\tbest: 0.1043589 (33258)\ttotal: 16m 6s\tremaining: 7h 47m 40s\n",
      "33600:\tlearn: 0.0907908\ttest: 0.1043516\tbest: 0.1043513 (33578)\ttotal: 16m 15s\tremaining: 7h 47m 31s\n",
      "33900:\tlearn: 0.0907074\ttest: 0.1043545\tbest: 0.1043474 (33755)\ttotal: 16m 23s\tremaining: 7h 47m 16s\n",
      "34200:\tlearn: 0.0906302\ttest: 0.1043390\tbest: 0.1043386 (34181)\ttotal: 16m 32s\tremaining: 7h 47m 3s\n",
      "34500:\tlearn: 0.0905585\ttest: 0.1043284\tbest: 0.1043262 (34491)\ttotal: 16m 40s\tremaining: 7h 46m 51s\n",
      "34800:\tlearn: 0.0904810\ttest: 0.1043102\tbest: 0.1043102 (34800)\ttotal: 16m 49s\tremaining: 7h 46m 38s\n",
      "35100:\tlearn: 0.0903972\ttest: 0.1042988\tbest: 0.1042971 (35074)\ttotal: 16m 58s\tremaining: 7h 46m 30s\n",
      "35400:\tlearn: 0.0903240\ttest: 0.1042896\tbest: 0.1042896 (35390)\ttotal: 17m 6s\tremaining: 7h 46m 18s\n",
      "35700:\tlearn: 0.0902556\ttest: 0.1042872\tbest: 0.1042869 (35670)\ttotal: 17m 15s\tremaining: 7h 46m 5s\n",
      "36000:\tlearn: 0.0901890\ttest: 0.1042765\tbest: 0.1042739 (35971)\ttotal: 17m 23s\tremaining: 7h 45m 55s\n",
      "36300:\tlearn: 0.0901133\ttest: 0.1042650\tbest: 0.1042648 (36293)\ttotal: 17m 32s\tremaining: 7h 45m 45s\n",
      "36600:\tlearn: 0.0900381\ttest: 0.1042486\tbest: 0.1042486 (36600)\ttotal: 17m 41s\tremaining: 7h 45m 33s\n",
      "36900:\tlearn: 0.0899548\ttest: 0.1042302\tbest: 0.1042294 (36894)\ttotal: 17m 49s\tremaining: 7h 45m 22s\n",
      "37200:\tlearn: 0.0898826\ttest: 0.1042172\tbest: 0.1042167 (37195)\ttotal: 17m 58s\tremaining: 7h 45m 13s\n",
      "37500:\tlearn: 0.0898005\ttest: 0.1042047\tbest: 0.1042013 (37471)\ttotal: 18m 7s\tremaining: 7h 45m 1s\n",
      "37800:\tlearn: 0.0897351\ttest: 0.1041994\tbest: 0.1041979 (37780)\ttotal: 18m 15s\tremaining: 7h 44m 50s\n",
      "38100:\tlearn: 0.0896635\ttest: 0.1041966\tbest: 0.1041958 (38087)\ttotal: 18m 24s\tremaining: 7h 44m 38s\n",
      "38400:\tlearn: 0.0895839\ttest: 0.1041866\tbest: 0.1041866 (38400)\ttotal: 18m 32s\tremaining: 7h 44m 29s\n",
      "38700:\tlearn: 0.0895093\ttest: 0.1041748\tbest: 0.1041723 (38584)\ttotal: 18m 41s\tremaining: 7h 44m 23s\n",
      "39000:\tlearn: 0.0894397\ttest: 0.1041695\tbest: 0.1041673 (38965)\ttotal: 18m 50s\tremaining: 7h 44m 12s\n",
      "39300:\tlearn: 0.0893738\ttest: 0.1041564\tbest: 0.1041564 (39299)\ttotal: 18m 58s\tremaining: 7h 44m 1s\n",
      "39600:\tlearn: 0.0893102\ttest: 0.1041556\tbest: 0.1041528 (39573)\ttotal: 19m 7s\tremaining: 7h 43m 48s\n",
      "39900:\tlearn: 0.0892392\ttest: 0.1041509\tbest: 0.1041491 (39888)\ttotal: 19m 16s\tremaining: 7h 43m 37s\n",
      "40200:\tlearn: 0.0891691\ttest: 0.1041470\tbest: 0.1041442 (39954)\ttotal: 19m 24s\tremaining: 7h 43m 26s\n",
      "Stopped by overfitting detector  (300 iterations wait)\n",
      "\n",
      "bestTest = 0.1041441562\n",
      "bestIteration = 39954\n",
      "\n",
      "Shrink model to first 39955 iterations.\n",
      "fold n°5\n",
      "0:\tlearn: 0.9753444\ttest: 0.9755118\tbest: 0.9755118 (0)\ttotal: 30.5ms\tremaining: 8h 27m 57s\n",
      "300:\tlearn: 0.1706205\ttest: 0.1708984\tbest: 0.1708984 (300)\ttotal: 8.88s\tremaining: 8h 11m 16s\n",
      "600:\tlearn: 0.1447667\ttest: 0.1452855\tbest: 0.1452855 (600)\ttotal: 17.4s\tremaining: 8h 3m 33s\n",
      "900:\tlearn: 0.1338319\ttest: 0.1348392\tbest: 0.1348392 (900)\ttotal: 26.1s\tremaining: 8h 3m 1s\n",
      "1200:\tlearn: 0.1276059\ttest: 0.1288737\tbest: 0.1288737 (1200)\ttotal: 34.8s\tremaining: 8h 2m 44s\n",
      "1500:\tlearn: 0.1236302\ttest: 0.1251120\tbest: 0.1251120 (1500)\ttotal: 43.6s\tremaining: 8h 3m\n",
      "1800:\tlearn: 0.1207186\ttest: 0.1224285\tbest: 0.1224285 (1800)\ttotal: 52.3s\tremaining: 8h 3m 13s\n",
      "2100:\tlearn: 0.1183595\ttest: 0.1202679\tbest: 0.1202679 (2100)\ttotal: 1m 1s\tremaining: 8h 3m 13s\n",
      "2400:\tlearn: 0.1165718\ttest: 0.1186844\tbest: 0.1186844 (2400)\ttotal: 1m 9s\tremaining: 8h 2m 42s\n",
      "2700:\tlearn: 0.1150526\ttest: 0.1174252\tbest: 0.1174252 (2700)\ttotal: 1m 18s\tremaining: 8h 2m 39s\n",
      "3000:\tlearn: 0.1136743\ttest: 0.1162807\tbest: 0.1162807 (3000)\ttotal: 1m 27s\tremaining: 8h 2m 51s\n",
      "3300:\tlearn: 0.1125657\ttest: 0.1153585\tbest: 0.1153585 (3300)\ttotal: 1m 36s\tremaining: 8h 3m 28s\n",
      "3600:\tlearn: 0.1115955\ttest: 0.1145937\tbest: 0.1145937 (3600)\ttotal: 1m 44s\tremaining: 8h 3m 41s\n",
      "3900:\tlearn: 0.1106421\ttest: 0.1138648\tbest: 0.1138648 (3900)\ttotal: 1m 53s\tremaining: 8h 3m 49s\n",
      "4200:\tlearn: 0.1098209\ttest: 0.1132397\tbest: 0.1132397 (4200)\ttotal: 2m 2s\tremaining: 8h 4m 4s\n",
      "4500:\tlearn: 0.1091094\ttest: 0.1127161\tbest: 0.1127161 (4500)\ttotal: 2m 11s\tremaining: 8h 3m 49s\n",
      "4800:\tlearn: 0.1084725\ttest: 0.1122307\tbest: 0.1122307 (4800)\ttotal: 2m 20s\tremaining: 8h 3m 49s\n",
      "5100:\tlearn: 0.1078458\ttest: 0.1118023\tbest: 0.1118023 (5100)\ttotal: 2m 28s\tremaining: 8h 3m 54s\n",
      "5400:\tlearn: 0.1073355\ttest: 0.1114582\tbest: 0.1114582 (5400)\ttotal: 2m 37s\tremaining: 8h 3m 54s\n",
      "5700:\tlearn: 0.1068293\ttest: 0.1111163\tbest: 0.1111163 (5700)\ttotal: 2m 46s\tremaining: 8h 3m 50s\n",
      "6000:\tlearn: 0.1063174\ttest: 0.1107610\tbest: 0.1107610 (6000)\ttotal: 2m 55s\tremaining: 8h 3m 40s\n",
      "6300:\tlearn: 0.1058585\ttest: 0.1104524\tbest: 0.1104524 (6300)\ttotal: 3m 3s\tremaining: 8h 3m 32s\n",
      "6600:\tlearn: 0.1053791\ttest: 0.1101450\tbest: 0.1101450 (6600)\ttotal: 3m 12s\tremaining: 8h 3m 12s\n",
      "6900:\tlearn: 0.1049777\ttest: 0.1098752\tbest: 0.1098752 (6900)\ttotal: 3m 21s\tremaining: 8h 3m 15s\n",
      "7200:\tlearn: 0.1045679\ttest: 0.1096221\tbest: 0.1096221 (7200)\ttotal: 3m 30s\tremaining: 8h 3m 6s\n",
      "7500:\tlearn: 0.1042131\ttest: 0.1093936\tbest: 0.1093936 (7500)\ttotal: 3m 39s\tremaining: 8h 3m 6s\n",
      "7800:\tlearn: 0.1038551\ttest: 0.1091999\tbest: 0.1091999 (7800)\ttotal: 3m 47s\tremaining: 8h 3m 4s\n",
      "8100:\tlearn: 0.1035252\ttest: 0.1089951\tbest: 0.1089951 (8100)\ttotal: 3m 56s\tremaining: 8h 2m 51s\n",
      "8400:\tlearn: 0.1031814\ttest: 0.1088117\tbest: 0.1088117 (8400)\ttotal: 4m 5s\tremaining: 8h 2m 42s\n",
      "8700:\tlearn: 0.1028573\ttest: 0.1086344\tbest: 0.1086344 (8700)\ttotal: 4m 14s\tremaining: 8h 2m 26s\n",
      "9000:\tlearn: 0.1025605\ttest: 0.1084522\tbest: 0.1084522 (9000)\ttotal: 4m 22s\tremaining: 8h 2m 17s\n",
      "9300:\tlearn: 0.1022591\ttest: 0.1082972\tbest: 0.1082968 (9299)\ttotal: 4m 31s\tremaining: 8h 2m 5s\n",
      "9600:\tlearn: 0.1019597\ttest: 0.1081379\tbest: 0.1081379 (9600)\ttotal: 4m 40s\tremaining: 8h 1m 56s\n",
      "9900:\tlearn: 0.1016794\ttest: 0.1079826\tbest: 0.1079826 (9900)\ttotal: 4m 48s\tremaining: 8h 1m 36s\n",
      "10200:\tlearn: 0.1014221\ttest: 0.1078498\tbest: 0.1078498 (10200)\ttotal: 4m 57s\tremaining: 8h 1m 23s\n",
      "10500:\tlearn: 0.1011566\ttest: 0.1077368\tbest: 0.1077355 (10494)\ttotal: 5m 6s\tremaining: 8h 1m 13s\n",
      "10800:\tlearn: 0.1008729\ttest: 0.1075953\tbest: 0.1075953 (10800)\ttotal: 5m 15s\tremaining: 8h 1m 16s\n",
      "11100:\tlearn: 0.1006325\ttest: 0.1074561\tbest: 0.1074561 (11100)\ttotal: 5m 24s\tremaining: 8h 1m 33s\n",
      "11400:\tlearn: 0.1004006\ttest: 0.1073513\tbest: 0.1073513 (11400)\ttotal: 5m 33s\tremaining: 8h 1m 33s\n",
      "11700:\tlearn: 0.1001656\ttest: 0.1072404\tbest: 0.1072404 (11700)\ttotal: 5m 41s\tremaining: 8h 1m 19s\n",
      "12000:\tlearn: 0.0999439\ttest: 0.1071500\tbest: 0.1071499 (11997)\ttotal: 5m 50s\tremaining: 8h 1m 7s\n",
      "12300:\tlearn: 0.0997595\ttest: 0.1070622\tbest: 0.1070621 (12296)\ttotal: 5m 59s\tremaining: 8h 52s\n",
      "12600:\tlearn: 0.0995550\ttest: 0.1069830\tbest: 0.1069823 (12599)\ttotal: 6m 8s\tremaining: 8h 38s\n",
      "12900:\tlearn: 0.0993513\ttest: 0.1069026\tbest: 0.1069026 (12900)\ttotal: 6m 16s\tremaining: 8h 21s\n",
      "13200:\tlearn: 0.0991609\ttest: 0.1068158\tbest: 0.1068158 (13200)\ttotal: 6m 25s\tremaining: 8h 11s\n",
      "13500:\tlearn: 0.0989641\ttest: 0.1067234\tbest: 0.1067234 (13500)\ttotal: 6m 34s\tremaining: 8h 1s\n",
      "13800:\tlearn: 0.0987854\ttest: 0.1066442\tbest: 0.1066437 (13799)\ttotal: 6m 42s\tremaining: 7h 59m 51s\n",
      "14100:\tlearn: 0.0986009\ttest: 0.1065726\tbest: 0.1065726 (14100)\ttotal: 6m 51s\tremaining: 7h 59m 44s\n",
      "14400:\tlearn: 0.0984242\ttest: 0.1064955\tbest: 0.1064955 (14400)\ttotal: 7m\tremaining: 7h 59m 31s\n",
      "14700:\tlearn: 0.0982547\ttest: 0.1064197\tbest: 0.1064195 (14698)\ttotal: 7m 9s\tremaining: 7h 59m 13s\n",
      "15000:\tlearn: 0.0980816\ttest: 0.1063508\tbest: 0.1063508 (15000)\ttotal: 7m 17s\tremaining: 7h 59m 2s\n",
      "15300:\tlearn: 0.0979214\ttest: 0.1062869\tbest: 0.1062869 (15300)\ttotal: 7m 26s\tremaining: 7h 58m 49s\n",
      "15600:\tlearn: 0.0977604\ttest: 0.1062125\tbest: 0.1062125 (15599)\ttotal: 7m 35s\tremaining: 7h 58m 38s\n",
      "15900:\tlearn: 0.0975963\ttest: 0.1061707\tbest: 0.1061707 (15900)\ttotal: 7m 43s\tremaining: 7h 58m 24s\n",
      "16200:\tlearn: 0.0974416\ttest: 0.1061079\tbest: 0.1061079 (16200)\ttotal: 7m 52s\tremaining: 7h 58m 11s\n",
      "16500:\tlearn: 0.0972649\ttest: 0.1060617\tbest: 0.1060609 (16498)\ttotal: 8m 1s\tremaining: 7h 58m 1s\n",
      "16800:\tlearn: 0.0971190\ttest: 0.1060116\tbest: 0.1060116 (16800)\ttotal: 8m 9s\tremaining: 7h 57m 51s\n",
      "17100:\tlearn: 0.0969715\ttest: 0.1059548\tbest: 0.1059543 (17099)\ttotal: 8m 18s\tremaining: 7h 57m 39s\n",
      "17400:\tlearn: 0.0968376\ttest: 0.1059155\tbest: 0.1059155 (17400)\ttotal: 8m 27s\tremaining: 7h 57m 28s\n",
      "17700:\tlearn: 0.0967001\ttest: 0.1058846\tbest: 0.1058846 (17700)\ttotal: 8m 36s\tremaining: 7h 57m 15s\n",
      "18000:\tlearn: 0.0965393\ttest: 0.1058265\tbest: 0.1058265 (18000)\ttotal: 8m 44s\tremaining: 7h 57m 10s\n",
      "18300:\tlearn: 0.0963799\ttest: 0.1057789\tbest: 0.1057789 (18300)\ttotal: 8m 53s\tremaining: 7h 57m\n",
      "18600:\tlearn: 0.0962272\ttest: 0.1057417\tbest: 0.1057414 (18596)\ttotal: 9m 2s\tremaining: 7h 56m 48s\n",
      "18900:\tlearn: 0.0960763\ttest: 0.1056986\tbest: 0.1056956 (18874)\ttotal: 9m 10s\tremaining: 7h 56m 36s\n",
      "19200:\tlearn: 0.0959605\ttest: 0.1056631\tbest: 0.1056628 (19192)\ttotal: 9m 19s\tremaining: 7h 56m 26s\n",
      "19500:\tlearn: 0.0958136\ttest: 0.1056137\tbest: 0.1056137 (19500)\ttotal: 9m 28s\tremaining: 7h 56m 12s\n",
      "19800:\tlearn: 0.0956808\ttest: 0.1055649\tbest: 0.1055649 (19800)\ttotal: 9m 36s\tremaining: 7h 56m\n",
      "20100:\tlearn: 0.0955434\ttest: 0.1055424\tbest: 0.1055421 (20099)\ttotal: 9m 45s\tremaining: 7h 55m 50s\n",
      "20400:\tlearn: 0.0954023\ttest: 0.1054972\tbest: 0.1054970 (20399)\ttotal: 9m 54s\tremaining: 7h 55m 39s\n",
      "20700:\tlearn: 0.0952835\ttest: 0.1054651\tbest: 0.1054637 (20691)\ttotal: 10m 2s\tremaining: 7h 55m 22s\n",
      "21000:\tlearn: 0.0951503\ttest: 0.1054310\tbest: 0.1054289 (20961)\ttotal: 10m 11s\tremaining: 7h 55m 10s\n",
      "21300:\tlearn: 0.0950153\ttest: 0.1054057\tbest: 0.1054050 (21296)\ttotal: 10m 20s\tremaining: 7h 55m 1s\n",
      "21600:\tlearn: 0.0948797\ttest: 0.1053693\tbest: 0.1053691 (21599)\ttotal: 10m 29s\tremaining: 7h 54m 50s\n",
      "21900:\tlearn: 0.0947586\ttest: 0.1053299\tbest: 0.1053296 (21894)\ttotal: 10m 37s\tremaining: 7h 54m 38s\n",
      "22200:\tlearn: 0.0946438\ttest: 0.1052996\tbest: 0.1052988 (22182)\ttotal: 10m 46s\tremaining: 7h 54m 27s\n",
      "22500:\tlearn: 0.0945097\ttest: 0.1052669\tbest: 0.1052665 (22499)\ttotal: 10m 55s\tremaining: 7h 54m 15s\n",
      "22800:\tlearn: 0.0943873\ttest: 0.1052284\tbest: 0.1052284 (22800)\ttotal: 11m 3s\tremaining: 7h 54m 4s\n",
      "23100:\tlearn: 0.0942789\ttest: 0.1051906\tbest: 0.1051906 (23099)\ttotal: 11m 12s\tremaining: 7h 53m 52s\n",
      "23400:\tlearn: 0.0941682\ttest: 0.1051714\tbest: 0.1051698 (23327)\ttotal: 11m 21s\tremaining: 7h 53m 41s\n",
      "23700:\tlearn: 0.0940716\ttest: 0.1051432\tbest: 0.1051430 (23697)\ttotal: 11m 29s\tremaining: 7h 53m 30s\n",
      "24000:\tlearn: 0.0939621\ttest: 0.1051141\tbest: 0.1051139 (23990)\ttotal: 11m 38s\tremaining: 7h 53m 19s\n",
      "24300:\tlearn: 0.0938474\ttest: 0.1050834\tbest: 0.1050827 (24278)\ttotal: 11m 47s\tremaining: 7h 53m 9s\n",
      "24600:\tlearn: 0.0937375\ttest: 0.1050486\tbest: 0.1050486 (24596)\ttotal: 11m 55s\tremaining: 7h 52m 55s\n",
      "24900:\tlearn: 0.0936307\ttest: 0.1050214\tbest: 0.1050209 (24898)\ttotal: 12m 4s\tremaining: 7h 52m 43s\n",
      "25200:\tlearn: 0.0935357\ttest: 0.1050023\tbest: 0.1049969 (25187)\ttotal: 12m 13s\tremaining: 7h 52m 34s\n",
      "25500:\tlearn: 0.0934238\ttest: 0.1049711\tbest: 0.1049710 (25499)\ttotal: 12m 21s\tremaining: 7h 52m 24s\n",
      "25800:\tlearn: 0.0933244\ttest: 0.1049406\tbest: 0.1049405 (25798)\ttotal: 12m 30s\tremaining: 7h 52m 11s\n",
      "26100:\tlearn: 0.0932349\ttest: 0.1049167\tbest: 0.1049165 (26099)\ttotal: 12m 38s\tremaining: 7h 51m 58s\n",
      "26400:\tlearn: 0.0931317\ttest: 0.1049018\tbest: 0.1049005 (26392)\ttotal: 12m 47s\tremaining: 7h 51m 42s\n",
      "26700:\tlearn: 0.0930376\ttest: 0.1048885\tbest: 0.1048872 (26655)\ttotal: 12m 56s\tremaining: 7h 51m 30s\n",
      "27000:\tlearn: 0.0929429\ttest: 0.1048685\tbest: 0.1048670 (26981)\ttotal: 13m 4s\tremaining: 7h 51m 16s\n",
      "27300:\tlearn: 0.0928427\ttest: 0.1048569\tbest: 0.1048542 (27253)\ttotal: 13m 13s\tremaining: 7h 51m 4s\n",
      "27600:\tlearn: 0.0927488\ttest: 0.1048411\tbest: 0.1048399 (27589)\ttotal: 13m 22s\tremaining: 7h 50m 55s\n",
      "27900:\tlearn: 0.0926442\ttest: 0.1048086\tbest: 0.1048086 (27900)\ttotal: 13m 30s\tremaining: 7h 50m 45s\n",
      "28200:\tlearn: 0.0925475\ttest: 0.1047838\tbest: 0.1047838 (28200)\ttotal: 13m 39s\tremaining: 7h 50m 34s\n",
      "28500:\tlearn: 0.0924488\ttest: 0.1047715\tbest: 0.1047666 (28426)\ttotal: 13m 48s\tremaining: 7h 50m 25s\n",
      "28800:\tlearn: 0.0923562\ttest: 0.1047572\tbest: 0.1047569 (28786)\ttotal: 13m 56s\tremaining: 7h 50m 15s\n",
      "29100:\tlearn: 0.0922698\ttest: 0.1047425\tbest: 0.1047425 (29100)\ttotal: 14m 5s\tremaining: 7h 50m 4s\n",
      "29400:\tlearn: 0.0921909\ttest: 0.1047231\tbest: 0.1047230 (29397)\ttotal: 14m 14s\tremaining: 7h 49m 55s\n",
      "29700:\tlearn: 0.0921153\ttest: 0.1047025\tbest: 0.1047025 (29700)\ttotal: 14m 22s\tremaining: 7h 49m 42s\n",
      "30000:\tlearn: 0.0920271\ttest: 0.1046948\tbest: 0.1046909 (29834)\ttotal: 14m 31s\tremaining: 7h 49m 30s\n",
      "30300:\tlearn: 0.0919429\ttest: 0.1046742\tbest: 0.1046740 (30294)\ttotal: 14m 39s\tremaining: 7h 49m 20s\n",
      "30600:\tlearn: 0.0918679\ttest: 0.1046583\tbest: 0.1046565 (30594)\ttotal: 14m 48s\tremaining: 7h 49m 8s\n",
      "30900:\tlearn: 0.0917913\ttest: 0.1046355\tbest: 0.1046353 (30898)\ttotal: 14m 57s\tremaining: 7h 48m 57s\n",
      "31200:\tlearn: 0.0916955\ttest: 0.1046221\tbest: 0.1046221 (31200)\ttotal: 15m 5s\tremaining: 7h 48m 46s\n",
      "31500:\tlearn: 0.0916069\ttest: 0.1046051\tbest: 0.1046050 (31499)\ttotal: 15m 14s\tremaining: 7h 48m 35s\n",
      "31800:\tlearn: 0.0915276\ttest: 0.1045879\tbest: 0.1045849 (31775)\ttotal: 15m 23s\tremaining: 7h 48m 25s\n",
      "32100:\tlearn: 0.0914426\ttest: 0.1045625\tbest: 0.1045625 (32100)\ttotal: 15m 31s\tremaining: 7h 48m 15s\n",
      "32400:\tlearn: 0.0913444\ttest: 0.1045349\tbest: 0.1045318 (32394)\ttotal: 15m 40s\tremaining: 7h 48m 6s\n",
      "32700:\tlearn: 0.0912583\ttest: 0.1045179\tbest: 0.1045179 (32700)\ttotal: 15m 49s\tremaining: 7h 47m 52s\n",
      "33000:\tlearn: 0.0911633\ttest: 0.1044983\tbest: 0.1044983 (33000)\ttotal: 15m 57s\tremaining: 7h 47m 40s\n",
      "33300:\tlearn: 0.0910883\ttest: 0.1044847\tbest: 0.1044780 (33204)\ttotal: 16m 6s\tremaining: 7h 47m 29s\n",
      "33600:\tlearn: 0.0910050\ttest: 0.1044784\tbest: 0.1044773 (33572)\ttotal: 16m 14s\tremaining: 7h 47m 18s\n",
      "33900:\tlearn: 0.0909321\ttest: 0.1044656\tbest: 0.1044655 (33898)\ttotal: 16m 23s\tremaining: 7h 47m 5s\n",
      "34200:\tlearn: 0.0908640\ttest: 0.1044545\tbest: 0.1044536 (34164)\ttotal: 16m 31s\tremaining: 7h 46m 51s\n",
      "34500:\tlearn: 0.0907766\ttest: 0.1044421\tbest: 0.1044421 (34500)\ttotal: 16m 40s\tremaining: 7h 46m 41s\n",
      "34800:\tlearn: 0.0906974\ttest: 0.1044159\tbest: 0.1044159 (34800)\ttotal: 16m 49s\tremaining: 7h 46m 34s\n",
      "35100:\tlearn: 0.0906235\ttest: 0.1044047\tbest: 0.1044030 (35090)\ttotal: 16m 57s\tremaining: 7h 46m 22s\n",
      "35400:\tlearn: 0.0905499\ttest: 0.1043880\tbest: 0.1043878 (35398)\ttotal: 17m 6s\tremaining: 7h 46m 10s\n",
      "35700:\tlearn: 0.0904804\ttest: 0.1043793\tbest: 0.1043784 (35679)\ttotal: 17m 15s\tremaining: 7h 45m 59s\n",
      "36000:\tlearn: 0.0904100\ttest: 0.1043637\tbest: 0.1043630 (35995)\ttotal: 17m 23s\tremaining: 7h 45m 50s\n",
      "36300:\tlearn: 0.0903281\ttest: 0.1043519\tbest: 0.1043510 (36283)\ttotal: 17m 32s\tremaining: 7h 45m 41s\n",
      "36600:\tlearn: 0.0902523\ttest: 0.1043335\tbest: 0.1043335 (36600)\ttotal: 17m 41s\tremaining: 7h 45m 31s\n",
      "36900:\tlearn: 0.0901754\ttest: 0.1043253\tbest: 0.1043253 (36874)\ttotal: 17m 49s\tremaining: 7h 45m 20s\n",
      "37200:\tlearn: 0.0901019\ttest: 0.1043017\tbest: 0.1043011 (37196)\ttotal: 17m 58s\tremaining: 7h 45m 11s\n",
      "37500:\tlearn: 0.0900359\ttest: 0.1042994\tbest: 0.1042987 (37419)\ttotal: 18m 7s\tremaining: 7h 45m\n",
      "37800:\tlearn: 0.0899729\ttest: 0.1042879\tbest: 0.1042858 (37771)\ttotal: 18m 15s\tremaining: 7h 44m 49s\n",
      "38100:\tlearn: 0.0898942\ttest: 0.1042780\tbest: 0.1042774 (38096)\ttotal: 18m 24s\tremaining: 7h 44m 37s\n",
      "38400:\tlearn: 0.0898247\ttest: 0.1042620\tbest: 0.1042609 (38380)\ttotal: 18m 32s\tremaining: 7h 44m 25s\n",
      "38700:\tlearn: 0.0897531\ttest: 0.1042522\tbest: 0.1042514 (38690)\ttotal: 18m 41s\tremaining: 7h 44m 13s\n",
      "39000:\tlearn: 0.0896762\ttest: 0.1042404\tbest: 0.1042404 (39000)\ttotal: 18m 49s\tremaining: 7h 44m 1s\n",
      "39300:\tlearn: 0.0896162\ttest: 0.1042371\tbest: 0.1042362 (39158)\ttotal: 18m 58s\tremaining: 7h 43m 47s\n",
      "39600:\tlearn: 0.0895416\ttest: 0.1042324\tbest: 0.1042319 (39588)\ttotal: 19m 7s\tremaining: 7h 43m 43s\n",
      "39900:\tlearn: 0.0894709\ttest: 0.1042182\tbest: 0.1042182 (39900)\ttotal: 19m 15s\tremaining: 7h 43m 34s\n",
      "40200:\tlearn: 0.0894088\ttest: 0.1042099\tbest: 0.1042099 (40200)\ttotal: 19m 24s\tremaining: 7h 43m 25s\n",
      "40500:\tlearn: 0.0893362\ttest: 0.1042031\tbest: 0.1042016 (40444)\ttotal: 19m 33s\tremaining: 7h 43m 13s\n",
      "40800:\tlearn: 0.0892583\ttest: 0.1041989\tbest: 0.1041984 (40693)\ttotal: 19m 41s\tremaining: 7h 42m 58s\n",
      "41100:\tlearn: 0.0891961\ttest: 0.1041875\tbest: 0.1041875 (41100)\ttotal: 19m 50s\tremaining: 7h 42m 48s\n",
      "41400:\tlearn: 0.0891348\ttest: 0.1041734\tbest: 0.1041733 (41399)\ttotal: 19m 59s\tremaining: 7h 42m 42s\n",
      "41700:\tlearn: 0.0890687\ttest: 0.1041713\tbest: 0.1041706 (41506)\ttotal: 20m 7s\tremaining: 7h 42m 34s\n",
      "Stopped by overfitting detector  (300 iterations wait)\n",
      "\n",
      "bestTest = 0.1041706366\n",
      "bestIteration = 41506\n",
      "\n",
      "Shrink model to first 41507 iterations.\n",
      "fold n°6\n",
      "0:\tlearn: 0.9756950\ttest: 0.9730447\tbest: 0.9730447 (0)\ttotal: 31ms\tremaining: 8h 37m 14s\n",
      "300:\tlearn: 0.1704564\ttest: 0.1696599\tbest: 0.1696599 (300)\ttotal: 8.95s\tremaining: 8h 15m 23s\n",
      "600:\tlearn: 0.1453163\ttest: 0.1440230\tbest: 0.1440230 (600)\ttotal: 17.6s\tremaining: 8h 6m 43s\n",
      "900:\tlearn: 0.1341002\ttest: 0.1330187\tbest: 0.1330187 (900)\ttotal: 26.2s\tremaining: 8h 5m 7s\n",
      "1200:\tlearn: 0.1277396\ttest: 0.1270431\tbest: 0.1270431 (1200)\ttotal: 34.9s\tremaining: 8h 4m 21s\n",
      "1500:\tlearn: 0.1238708\ttest: 0.1234863\tbest: 0.1234863 (1500)\ttotal: 43.6s\tremaining: 8h 3m 35s\n",
      "1800:\tlearn: 0.1209246\ttest: 0.1208301\tbest: 0.1208301 (1800)\ttotal: 52.4s\tremaining: 8h 3m 44s\n",
      "2100:\tlearn: 0.1185060\ttest: 0.1186865\tbest: 0.1186865 (2100)\ttotal: 1m 1s\tremaining: 8h 4m 39s\n",
      "2400:\tlearn: 0.1166840\ttest: 0.1170763\tbest: 0.1170763 (2400)\ttotal: 1m 9s\tremaining: 8h 4m 23s\n",
      "2700:\tlearn: 0.1152112\ttest: 0.1157946\tbest: 0.1157946 (2700)\ttotal: 1m 18s\tremaining: 8h 4m 9s\n",
      "3000:\tlearn: 0.1139434\ttest: 0.1147124\tbest: 0.1147124 (3000)\ttotal: 1m 27s\tremaining: 8h 4m 7s\n",
      "3300:\tlearn: 0.1127941\ttest: 0.1136848\tbest: 0.1136848 (3300)\ttotal: 1m 36s\tremaining: 8h 4m 8s\n",
      "3600:\tlearn: 0.1118059\ttest: 0.1128858\tbest: 0.1128858 (3600)\ttotal: 1m 45s\tremaining: 8h 4m 28s\n",
      "3900:\tlearn: 0.1109094\ttest: 0.1121561\tbest: 0.1121561 (3900)\ttotal: 1m 53s\tremaining: 8h 4m 42s\n",
      "4200:\tlearn: 0.1100625\ttest: 0.1114771\tbest: 0.1114771 (4200)\ttotal: 2m 2s\tremaining: 8h 5m 16s\n",
      "4500:\tlearn: 0.1093412\ttest: 0.1109348\tbest: 0.1109348 (4500)\ttotal: 2m 11s\tremaining: 8h 5m 35s\n",
      "4800:\tlearn: 0.1086665\ttest: 0.1104314\tbest: 0.1104314 (4800)\ttotal: 2m 20s\tremaining: 8h 5m 29s\n",
      "5100:\tlearn: 0.1080479\ttest: 0.1099555\tbest: 0.1099555 (5100)\ttotal: 2m 29s\tremaining: 8h 5m 32s\n",
      "5400:\tlearn: 0.1074457\ttest: 0.1095183\tbest: 0.1095183 (5400)\ttotal: 2m 38s\tremaining: 8h 5m 27s\n",
      "5700:\tlearn: 0.1069122\ttest: 0.1091365\tbest: 0.1091365 (5700)\ttotal: 2m 46s\tremaining: 8h 5m 6s\n",
      "6000:\tlearn: 0.1064058\ttest: 0.1088137\tbest: 0.1088137 (6000)\ttotal: 2m 55s\tremaining: 8h 5m 10s\n",
      "6300:\tlearn: 0.1059682\ttest: 0.1085073\tbest: 0.1085073 (6300)\ttotal: 3m 4s\tremaining: 8h 5m 17s\n",
      "6600:\tlearn: 0.1055243\ttest: 0.1082007\tbest: 0.1082007 (6600)\ttotal: 3m 13s\tremaining: 8h 5m 9s\n",
      "6900:\tlearn: 0.1050580\ttest: 0.1078962\tbest: 0.1078962 (6900)\ttotal: 3m 22s\tremaining: 8h 4m 59s\n",
      "7200:\tlearn: 0.1046509\ttest: 0.1076295\tbest: 0.1076295 (7200)\ttotal: 3m 31s\tremaining: 8h 4m 54s\n",
      "7500:\tlearn: 0.1042491\ttest: 0.1073893\tbest: 0.1073893 (7500)\ttotal: 3m 39s\tremaining: 8h 4m 33s\n",
      "7800:\tlearn: 0.1039136\ttest: 0.1071937\tbest: 0.1071937 (7800)\ttotal: 3m 48s\tremaining: 8h 4m 27s\n",
      "8100:\tlearn: 0.1035499\ttest: 0.1069860\tbest: 0.1069860 (8100)\ttotal: 3m 57s\tremaining: 8h 4m 52s\n",
      "8400:\tlearn: 0.1032200\ttest: 0.1068010\tbest: 0.1068010 (8400)\ttotal: 4m 6s\tremaining: 8h 4m 36s\n",
      "8700:\tlearn: 0.1028785\ttest: 0.1066040\tbest: 0.1066040 (8700)\ttotal: 4m 15s\tremaining: 8h 4m 18s\n",
      "9000:\tlearn: 0.1025696\ttest: 0.1064310\tbest: 0.1064307 (8999)\ttotal: 4m 23s\tremaining: 8h 4m 7s\n",
      "9300:\tlearn: 0.1022523\ttest: 0.1062788\tbest: 0.1062788 (9300)\ttotal: 4m 32s\tremaining: 8h 3m 50s\n",
      "9600:\tlearn: 0.1019737\ttest: 0.1060987\tbest: 0.1060987 (9600)\ttotal: 4m 41s\tremaining: 8h 3m 34s\n",
      "9900:\tlearn: 0.1017031\ttest: 0.1059560\tbest: 0.1059560 (9900)\ttotal: 4m 50s\tremaining: 8h 3m 39s\n",
      "10200:\tlearn: 0.1014355\ttest: 0.1058309\tbest: 0.1058309 (10199)\ttotal: 4m 59s\tremaining: 8h 3m 39s\n",
      "10500:\tlearn: 0.1011775\ttest: 0.1057166\tbest: 0.1057166 (10500)\ttotal: 5m 8s\tremaining: 8h 3m 44s\n",
      "10800:\tlearn: 0.1009419\ttest: 0.1056018\tbest: 0.1056014 (10799)\ttotal: 5m 16s\tremaining: 8h 3m 34s\n",
      "11100:\tlearn: 0.1007120\ttest: 0.1054980\tbest: 0.1054973 (11097)\ttotal: 5m 25s\tremaining: 8h 3m 21s\n",
      "11400:\tlearn: 0.1005061\ttest: 0.1054046\tbest: 0.1054041 (11398)\ttotal: 5m 34s\tremaining: 8h 3m 1s\n",
      "11700:\tlearn: 0.1002782\ttest: 0.1052999\tbest: 0.1052995 (11698)\ttotal: 5m 42s\tremaining: 8h 2m 49s\n",
      "12000:\tlearn: 0.1000540\ttest: 0.1051883\tbest: 0.1051883 (12000)\ttotal: 5m 51s\tremaining: 8h 2m 36s\n",
      "12300:\tlearn: 0.0998332\ttest: 0.1050948\tbest: 0.1050948 (12300)\ttotal: 6m\tremaining: 8h 2m 23s\n",
      "12600:\tlearn: 0.0995964\ttest: 0.1049925\tbest: 0.1049925 (12599)\ttotal: 6m 9s\tremaining: 8h 2m 13s\n",
      "12900:\tlearn: 0.0993900\ttest: 0.1049073\tbest: 0.1049073 (12900)\ttotal: 6m 17s\tremaining: 8h 2m 1s\n",
      "13200:\tlearn: 0.0991907\ttest: 0.1048525\tbest: 0.1048525 (13200)\ttotal: 6m 26s\tremaining: 8h 1m 49s\n",
      "13500:\tlearn: 0.0989967\ttest: 0.1047889\tbest: 0.1047889 (13500)\ttotal: 6m 35s\tremaining: 8h 1m 38s\n",
      "13800:\tlearn: 0.0988032\ttest: 0.1047132\tbest: 0.1047091 (13798)\ttotal: 6m 44s\tremaining: 8h 1m 23s\n",
      "14100:\tlearn: 0.0986250\ttest: 0.1046476\tbest: 0.1046473 (14098)\ttotal: 6m 52s\tremaining: 8h 1m 9s\n",
      "14400:\tlearn: 0.0984548\ttest: 0.1045868\tbest: 0.1045868 (14400)\ttotal: 7m 1s\tremaining: 8h 1m 5s\n",
      "14700:\tlearn: 0.0982713\ttest: 0.1045158\tbest: 0.1045158 (14700)\ttotal: 7m 10s\tremaining: 8h 56s\n",
      "15000:\tlearn: 0.0980898\ttest: 0.1044539\tbest: 0.1044539 (15000)\ttotal: 7m 19s\tremaining: 8h 1m 6s\n",
      "15300:\tlearn: 0.0979110\ttest: 0.1043833\tbest: 0.1043833 (15300)\ttotal: 7m 28s\tremaining: 8h 59s\n",
      "15600:\tlearn: 0.0977532\ttest: 0.1043372\tbest: 0.1043358 (15598)\ttotal: 7m 37s\tremaining: 8h 45s\n",
      "15900:\tlearn: 0.0975824\ttest: 0.1042696\tbest: 0.1042695 (15897)\ttotal: 7m 45s\tremaining: 8h 34s\n",
      "16200:\tlearn: 0.0974246\ttest: 0.1042259\tbest: 0.1042259 (16200)\ttotal: 7m 54s\tremaining: 8h 18s\n",
      "16500:\tlearn: 0.0972456\ttest: 0.1041744\tbest: 0.1041744 (16500)\ttotal: 8m 3s\tremaining: 8h 4s\n",
      "16800:\tlearn: 0.0970691\ttest: 0.1040983\tbest: 0.1040983 (16800)\ttotal: 8m 12s\tremaining: 7h 59m 53s\n",
      "17100:\tlearn: 0.0969135\ttest: 0.1040466\tbest: 0.1040460 (17089)\ttotal: 8m 20s\tremaining: 7h 59m 46s\n",
      "17400:\tlearn: 0.0967469\ttest: 0.1039944\tbest: 0.1039931 (17396)\ttotal: 8m 29s\tremaining: 7h 59m 41s\n",
      "17700:\tlearn: 0.0965945\ttest: 0.1039428\tbest: 0.1039428 (17700)\ttotal: 8m 38s\tremaining: 7h 59m 29s\n",
      "18000:\tlearn: 0.0964563\ttest: 0.1039067\tbest: 0.1039067 (18000)\ttotal: 8m 47s\tremaining: 7h 59m 16s\n",
      "18300:\tlearn: 0.0963092\ttest: 0.1038572\tbest: 0.1038572 (18299)\ttotal: 8m 55s\tremaining: 7h 58m 59s\n",
      "18600:\tlearn: 0.0961493\ttest: 0.1037963\tbest: 0.1037958 (18589)\ttotal: 9m 4s\tremaining: 7h 58m 45s\n",
      "18900:\tlearn: 0.0960191\ttest: 0.1037494\tbest: 0.1037493 (18893)\ttotal: 9m 13s\tremaining: 7h 58m 27s\n",
      "19200:\tlearn: 0.0958856\ttest: 0.1037124\tbest: 0.1037124 (19200)\ttotal: 9m 21s\tremaining: 7h 58m 13s\n",
      "19500:\tlearn: 0.0957496\ttest: 0.1036795\tbest: 0.1036791 (19491)\ttotal: 9m 30s\tremaining: 7h 57m 58s\n",
      "19800:\tlearn: 0.0956261\ttest: 0.1036335\tbest: 0.1036330 (19791)\ttotal: 9m 39s\tremaining: 7h 57m 42s\n",
      "20100:\tlearn: 0.0954788\ttest: 0.1036091\tbest: 0.1036091 (20100)\ttotal: 9m 47s\tremaining: 7h 57m 32s\n",
      "20400:\tlearn: 0.0953270\ttest: 0.1035724\tbest: 0.1035724 (20399)\ttotal: 9m 56s\tremaining: 7h 57m 21s\n",
      "20700:\tlearn: 0.0951888\ttest: 0.1035309\tbest: 0.1035298 (20693)\ttotal: 10m 5s\tremaining: 7h 57m 13s\n",
      "21000:\tlearn: 0.0950701\ttest: 0.1035108\tbest: 0.1035105 (20994)\ttotal: 10m 13s\tremaining: 7h 57m\n",
      "21300:\tlearn: 0.0949475\ttest: 0.1034843\tbest: 0.1034833 (21299)\ttotal: 10m 22s\tremaining: 7h 56m 52s\n",
      "21600:\tlearn: 0.0948298\ttest: 0.1034676\tbest: 0.1034658 (21583)\ttotal: 10m 31s\tremaining: 7h 56m 42s\n",
      "21900:\tlearn: 0.0947132\ttest: 0.1034357\tbest: 0.1034356 (21897)\ttotal: 10m 40s\tremaining: 7h 56m 33s\n",
      "22200:\tlearn: 0.0945950\ttest: 0.1034086\tbest: 0.1034084 (22197)\ttotal: 10m 49s\tremaining: 7h 56m 25s\n",
      "22500:\tlearn: 0.0944896\ttest: 0.1033915\tbest: 0.1033915 (22500)\ttotal: 10m 57s\tremaining: 7h 56m 17s\n",
      "22800:\tlearn: 0.0943792\ttest: 0.1033723\tbest: 0.1033723 (22800)\ttotal: 11m 6s\tremaining: 7h 56m 9s\n",
      "23100:\tlearn: 0.0942624\ttest: 0.1033475\tbest: 0.1033473 (23099)\ttotal: 11m 15s\tremaining: 7h 55m 59s\n",
      "23400:\tlearn: 0.0941618\ttest: 0.1033224\tbest: 0.1033223 (23398)\ttotal: 11m 23s\tremaining: 7h 55m 44s\n",
      "23700:\tlearn: 0.0940531\ttest: 0.1032892\tbest: 0.1032892 (23699)\ttotal: 11m 32s\tremaining: 7h 55m 35s\n",
      "24000:\tlearn: 0.0939433\ttest: 0.1032609\tbest: 0.1032603 (23994)\ttotal: 11m 41s\tremaining: 7h 55m 24s\n",
      "24300:\tlearn: 0.0938348\ttest: 0.1032361\tbest: 0.1032353 (24291)\ttotal: 11m 50s\tremaining: 7h 55m 12s\n",
      "24600:\tlearn: 0.0937194\ttest: 0.1032151\tbest: 0.1032150 (24593)\ttotal: 11m 58s\tremaining: 7h 55m 1s\n",
      "24900:\tlearn: 0.0936169\ttest: 0.1031993\tbest: 0.1031975 (24891)\ttotal: 12m 7s\tremaining: 7h 54m 50s\n",
      "25200:\tlearn: 0.0935023\ttest: 0.1031875\tbest: 0.1031796 (25132)\ttotal: 12m 16s\tremaining: 7h 54m 38s\n",
      "25500:\tlearn: 0.0934063\ttest: 0.1031669\tbest: 0.1031658 (25489)\ttotal: 12m 24s\tremaining: 7h 54m 23s\n",
      "25800:\tlearn: 0.0932852\ttest: 0.1031424\tbest: 0.1031424 (25800)\ttotal: 12m 33s\tremaining: 7h 54m 9s\n",
      "26100:\tlearn: 0.0931895\ttest: 0.1031266\tbest: 0.1031261 (26087)\ttotal: 12m 42s\tremaining: 7h 53m 58s\n",
      "26400:\tlearn: 0.0931023\ttest: 0.1031068\tbest: 0.1031068 (26400)\ttotal: 12m 50s\tremaining: 7h 53m 48s\n",
      "26700:\tlearn: 0.0930068\ttest: 0.1030970\tbest: 0.1030956 (26671)\ttotal: 13m\tremaining: 7h 53m 54s\n",
      "27000:\tlearn: 0.0929004\ttest: 0.1030795\tbest: 0.1030794 (26999)\ttotal: 13m 9s\tremaining: 7h 54m 8s\n",
      "27300:\tlearn: 0.0928085\ttest: 0.1030594\tbest: 0.1030585 (27282)\ttotal: 13m 18s\tremaining: 7h 54m 5s\n",
      "27600:\tlearn: 0.0927124\ttest: 0.1030366\tbest: 0.1030365 (27586)\ttotal: 13m 27s\tremaining: 7h 54m 1s\n",
      "27900:\tlearn: 0.0926258\ttest: 0.1030167\tbest: 0.1030161 (27875)\ttotal: 13m 35s\tremaining: 7h 53m 49s\n",
      "28200:\tlearn: 0.0925180\ttest: 0.1029875\tbest: 0.1029870 (28184)\ttotal: 13m 44s\tremaining: 7h 53m 37s\n",
      "28500:\tlearn: 0.0924244\ttest: 0.1029703\tbest: 0.1029685 (28487)\ttotal: 13m 53s\tremaining: 7h 53m 24s\n",
      "28800:\tlearn: 0.0923206\ttest: 0.1029560\tbest: 0.1029551 (28790)\ttotal: 14m 2s\tremaining: 7h 53m 13s\n",
      "29100:\tlearn: 0.0922333\ttest: 0.1029323\tbest: 0.1029323 (29098)\ttotal: 14m 10s\tremaining: 7h 52m 59s\n",
      "29400:\tlearn: 0.0921431\ttest: 0.1029124\tbest: 0.1029123 (29390)\ttotal: 14m 19s\tremaining: 7h 52m 44s\n",
      "29700:\tlearn: 0.0920447\ttest: 0.1028948\tbest: 0.1028948 (29700)\ttotal: 14m 28s\tremaining: 7h 52m 38s\n",
      "30000:\tlearn: 0.0919646\ttest: 0.1028730\tbest: 0.1028730 (30000)\ttotal: 14m 36s\tremaining: 7h 52m 31s\n",
      "30300:\tlearn: 0.0918687\ttest: 0.1028599\tbest: 0.1028599 (30300)\ttotal: 14m 45s\tremaining: 7h 52m 25s\n",
      "30600:\tlearn: 0.0917827\ttest: 0.1028417\tbest: 0.1028417 (30597)\ttotal: 14m 54s\tremaining: 7h 52m 14s\n",
      "30900:\tlearn: 0.0916996\ttest: 0.1028294\tbest: 0.1028294 (30900)\ttotal: 15m 3s\tremaining: 7h 52m\n",
      "31200:\tlearn: 0.0916090\ttest: 0.1028182\tbest: 0.1028160 (31086)\ttotal: 15m 11s\tremaining: 7h 51m 46s\n",
      "31500:\tlearn: 0.0915177\ttest: 0.1028030\tbest: 0.1028019 (31482)\ttotal: 15m 20s\tremaining: 7h 51m 35s\n",
      "31800:\tlearn: 0.0914352\ttest: 0.1027917\tbest: 0.1027917 (31800)\ttotal: 15m 29s\tremaining: 7h 51m 25s\n",
      "32100:\tlearn: 0.0913462\ttest: 0.1027642\tbest: 0.1027642 (32100)\ttotal: 15m 37s\tremaining: 7h 51m 16s\n",
      "32400:\tlearn: 0.0912488\ttest: 0.1027384\tbest: 0.1027367 (32373)\ttotal: 15m 46s\tremaining: 7h 51m 6s\n",
      "32700:\tlearn: 0.0911670\ttest: 0.1027222\tbest: 0.1027220 (32698)\ttotal: 15m 55s\tremaining: 7h 50m 56s\n",
      "33000:\tlearn: 0.0910920\ttest: 0.1027020\tbest: 0.1027014 (32993)\ttotal: 16m 3s\tremaining: 7h 50m 45s\n",
      "33300:\tlearn: 0.0910152\ttest: 0.1026895\tbest: 0.1026894 (33299)\ttotal: 16m 12s\tremaining: 7h 50m 34s\n",
      "33600:\tlearn: 0.0909328\ttest: 0.1026834\tbest: 0.1026831 (33598)\ttotal: 16m 21s\tremaining: 7h 50m 22s\n",
      "33900:\tlearn: 0.0908576\ttest: 0.1026629\tbest: 0.1026629 (33900)\ttotal: 16m 29s\tremaining: 7h 50m 10s\n",
      "34200:\tlearn: 0.0907768\ttest: 0.1026451\tbest: 0.1026451 (34200)\ttotal: 16m 38s\tremaining: 7h 50m\n",
      "34500:\tlearn: 0.0907034\ttest: 0.1026355\tbest: 0.1026352 (34484)\ttotal: 16m 47s\tremaining: 7h 49m 47s\n",
      "34800:\tlearn: 0.0906193\ttest: 0.1026257\tbest: 0.1026225 (34695)\ttotal: 16m 55s\tremaining: 7h 49m 35s\n",
      "35100:\tlearn: 0.0905514\ttest: 0.1026125\tbest: 0.1026119 (35096)\ttotal: 17m 4s\tremaining: 7h 49m 23s\n",
      "35400:\tlearn: 0.0904633\ttest: 0.1026029\tbest: 0.1025992 (35335)\ttotal: 17m 13s\tremaining: 7h 49m 17s\n",
      "35700:\tlearn: 0.0903857\ttest: 0.1025963\tbest: 0.1025947 (35656)\ttotal: 17m 22s\tremaining: 7h 49m 19s\n",
      "36000:\tlearn: 0.0903112\ttest: 0.1025893\tbest: 0.1025835 (35945)\ttotal: 17m 31s\tremaining: 7h 49m 12s\n",
      "36300:\tlearn: 0.0902410\ttest: 0.1025806\tbest: 0.1025806 (36300)\ttotal: 17m 40s\tremaining: 7h 49m 2s\n",
      "36600:\tlearn: 0.0901700\ttest: 0.1025679\tbest: 0.1025679 (36600)\ttotal: 17m 48s\tremaining: 7h 48m 53s\n",
      "36900:\tlearn: 0.0901042\ttest: 0.1025590\tbest: 0.1025587 (36898)\ttotal: 17m 57s\tremaining: 7h 48m 42s\n",
      "37200:\tlearn: 0.0900296\ttest: 0.1025539\tbest: 0.1025527 (37167)\ttotal: 18m 6s\tremaining: 7h 48m 30s\n",
      "37500:\tlearn: 0.0899610\ttest: 0.1025404\tbest: 0.1025390 (37490)\ttotal: 18m 14s\tremaining: 7h 48m 21s\n",
      "37800:\tlearn: 0.0898856\ttest: 0.1025326\tbest: 0.1025310 (37793)\ttotal: 18m 23s\tremaining: 7h 48m 8s\n",
      "38100:\tlearn: 0.0898228\ttest: 0.1025186\tbest: 0.1025183 (38098)\ttotal: 18m 32s\tremaining: 7h 48m\n",
      "38400:\tlearn: 0.0897514\ttest: 0.1025087\tbest: 0.1025043 (38271)\ttotal: 18m 41s\tremaining: 7h 47m 50s\n",
      "Stopped by overfitting detector  (300 iterations wait)\n",
      "\n",
      "bestTest = 0.1025042678\n",
      "bestIteration = 38271\n",
      "\n",
      "Shrink model to first 38272 iterations.\n",
      "fold n°7\n",
      "0:\tlearn: 0.9759754\ttest: 0.9701364\tbest: 0.9701364 (0)\ttotal: 31.6ms\tremaining: 8h 46m 22s\n",
      "300:\tlearn: 0.1702948\ttest: 0.1689693\tbest: 0.1689693 (300)\ttotal: 9.03s\tremaining: 8h 19m 57s\n",
      "600:\tlearn: 0.1445917\ttest: 0.1435341\tbest: 0.1435341 (600)\ttotal: 17.7s\tremaining: 8h 11m 42s\n",
      "900:\tlearn: 0.1340390\ttest: 0.1332285\tbest: 0.1332285 (900)\ttotal: 26.5s\tremaining: 8h 10m 15s\n",
      "1200:\tlearn: 0.1276554\ttest: 0.1270711\tbest: 0.1270711 (1200)\ttotal: 35.3s\tremaining: 8h 9m 36s\n",
      "1500:\tlearn: 0.1236382\ttest: 0.1232987\tbest: 0.1232987 (1500)\ttotal: 44.2s\tremaining: 8h 9m 30s\n",
      "1800:\tlearn: 0.1206281\ttest: 0.1205725\tbest: 0.1205725 (1800)\ttotal: 53s\tremaining: 8h 9m 49s\n",
      "2100:\tlearn: 0.1183132\ttest: 0.1186449\tbest: 0.1186449 (2100)\ttotal: 1m 1s\tremaining: 8h 9m 39s\n",
      "2400:\tlearn: 0.1165030\ttest: 0.1171449\tbest: 0.1171449 (2400)\ttotal: 1m 10s\tremaining: 8h 9m 37s\n",
      "2700:\tlearn: 0.1149754\ttest: 0.1159402\tbest: 0.1159402 (2700)\ttotal: 1m 19s\tremaining: 8h 10m 39s\n",
      "3000:\tlearn: 0.1136546\ttest: 0.1148876\tbest: 0.1148876 (3000)\ttotal: 1m 29s\tremaining: 8h 13m 44s\n",
      "3300:\tlearn: 0.1125164\ttest: 0.1139881\tbest: 0.1139881 (3300)\ttotal: 1m 38s\tremaining: 8h 16m 46s\n",
      "3600:\tlearn: 0.1115416\ttest: 0.1132427\tbest: 0.1132427 (3600)\ttotal: 1m 48s\tremaining: 8h 19m 55s\n",
      "3900:\tlearn: 0.1106057\ttest: 0.1125483\tbest: 0.1125483 (3900)\ttotal: 1m 57s\tremaining: 8h 21m 15s\n",
      "4200:\tlearn: 0.1097760\ttest: 0.1119593\tbest: 0.1119593 (4200)\ttotal: 2m 6s\tremaining: 8h 20m 34s\n",
      "4500:\tlearn: 0.1090942\ttest: 0.1114496\tbest: 0.1114496 (4500)\ttotal: 2m 15s\tremaining: 8h 19m 53s\n",
      "4800:\tlearn: 0.1084614\ttest: 0.1110107\tbest: 0.1110107 (4800)\ttotal: 2m 24s\tremaining: 8h 19m 2s\n",
      "5100:\tlearn: 0.1078291\ttest: 0.1105498\tbest: 0.1105498 (5100)\ttotal: 2m 33s\tremaining: 8h 18m 12s\n",
      "5400:\tlearn: 0.1072772\ttest: 0.1101657\tbest: 0.1101657 (5400)\ttotal: 2m 41s\tremaining: 8h 17m 11s\n",
      "5700:\tlearn: 0.1067399\ttest: 0.1098029\tbest: 0.1098029 (5700)\ttotal: 2m 50s\tremaining: 8h 16m 36s\n",
      "6000:\tlearn: 0.1062387\ttest: 0.1094973\tbest: 0.1094973 (6000)\ttotal: 2m 59s\tremaining: 8h 15m 37s\n",
      "6300:\tlearn: 0.1057753\ttest: 0.1092455\tbest: 0.1092455 (6300)\ttotal: 3m 8s\tremaining: 8h 15m 3s\n",
      "6600:\tlearn: 0.1053248\ttest: 0.1089592\tbest: 0.1089591 (6599)\ttotal: 3m 17s\tremaining: 8h 14m 23s\n",
      "6900:\tlearn: 0.1048806\ttest: 0.1087161\tbest: 0.1087161 (6900)\ttotal: 3m 25s\tremaining: 8h 13m 42s\n",
      "7200:\tlearn: 0.1044986\ttest: 0.1084996\tbest: 0.1084996 (7200)\ttotal: 3m 34s\tremaining: 8h 13m 7s\n",
      "7500:\tlearn: 0.1041315\ttest: 0.1082842\tbest: 0.1082842 (7500)\ttotal: 3m 43s\tremaining: 8h 12m 33s\n",
      "7800:\tlearn: 0.1037870\ttest: 0.1080876\tbest: 0.1080876 (7800)\ttotal: 3m 52s\tremaining: 8h 12m 2s\n",
      "8100:\tlearn: 0.1034497\ttest: 0.1078966\tbest: 0.1078966 (8100)\ttotal: 4m\tremaining: 8h 11m 39s\n",
      "8400:\tlearn: 0.1031261\ttest: 0.1077184\tbest: 0.1077184 (8400)\ttotal: 4m 9s\tremaining: 8h 11m 7s\n",
      "8700:\tlearn: 0.1028126\ttest: 0.1075612\tbest: 0.1075612 (8700)\ttotal: 4m 18s\tremaining: 8h 10m 41s\n",
      "9000:\tlearn: 0.1025279\ttest: 0.1073826\tbest: 0.1073826 (9000)\ttotal: 4m 27s\tremaining: 8h 10m 13s\n",
      "9300:\tlearn: 0.1022254\ttest: 0.1072296\tbest: 0.1072295 (9299)\ttotal: 4m 35s\tremaining: 8h 9m 46s\n",
      "9600:\tlearn: 0.1019423\ttest: 0.1070866\tbest: 0.1070866 (9600)\ttotal: 4m 44s\tremaining: 8h 9m 15s\n",
      "9900:\tlearn: 0.1016637\ttest: 0.1069495\tbest: 0.1069495 (9900)\ttotal: 4m 53s\tremaining: 8h 8m 53s\n",
      "10200:\tlearn: 0.1014229\ttest: 0.1068301\tbest: 0.1068301 (10200)\ttotal: 5m 2s\tremaining: 8h 8m 27s\n",
      "10500:\tlearn: 0.1011704\ttest: 0.1067040\tbest: 0.1067040 (10500)\ttotal: 5m 10s\tremaining: 8h 8m 5s\n",
      "10800:\tlearn: 0.1009500\ttest: 0.1066004\tbest: 0.1066004 (10800)\ttotal: 5m 19s\tremaining: 8h 7m 43s\n",
      "11100:\tlearn: 0.1007099\ttest: 0.1064861\tbest: 0.1064861 (11100)\ttotal: 5m 28s\tremaining: 8h 7m 17s\n",
      "11400:\tlearn: 0.1004664\ttest: 0.1063706\tbest: 0.1063706 (11400)\ttotal: 5m 36s\tremaining: 8h 6m 54s\n",
      "11700:\tlearn: 0.1002324\ttest: 0.1062794\tbest: 0.1062789 (11699)\ttotal: 5m 45s\tremaining: 8h 6m 46s\n",
      "12000:\tlearn: 0.1000088\ttest: 0.1061905\tbest: 0.1061902 (11999)\ttotal: 5m 54s\tremaining: 8h 6m 21s\n",
      "12300:\tlearn: 0.0998050\ttest: 0.1060861\tbest: 0.1060861 (12300)\ttotal: 6m 3s\tremaining: 8h 5m 56s\n",
      "12600:\tlearn: 0.0995828\ttest: 0.1059903\tbest: 0.1059903 (12600)\ttotal: 6m 11s\tremaining: 8h 5m 40s\n",
      "12900:\tlearn: 0.0993836\ttest: 0.1059103\tbest: 0.1059103 (12900)\ttotal: 6m 20s\tremaining: 8h 5m 13s\n",
      "13200:\tlearn: 0.0991733\ttest: 0.1058266\tbest: 0.1058266 (13200)\ttotal: 6m 29s\tremaining: 8h 4m 51s\n",
      "13500:\tlearn: 0.0989940\ttest: 0.1057505\tbest: 0.1057505 (13500)\ttotal: 6m 37s\tremaining: 8h 4m 29s\n",
      "13800:\tlearn: 0.0987901\ttest: 0.1056630\tbest: 0.1056630 (13800)\ttotal: 6m 46s\tremaining: 8h 4m 10s\n",
      "14100:\tlearn: 0.0986082\ttest: 0.1055962\tbest: 0.1055959 (14097)\ttotal: 6m 55s\tremaining: 8h 3m 53s\n",
      "14400:\tlearn: 0.0984174\ttest: 0.1055363\tbest: 0.1055363 (14400)\ttotal: 7m 4s\tremaining: 8h 3m 41s\n",
      "14700:\tlearn: 0.0982216\ttest: 0.1054694\tbest: 0.1054694 (14700)\ttotal: 7m 12s\tremaining: 8h 3m 18s\n",
      "15000:\tlearn: 0.0980507\ttest: 0.1053923\tbest: 0.1053923 (15000)\ttotal: 7m 21s\tremaining: 8h 2m 57s\n",
      "15300:\tlearn: 0.0978713\ttest: 0.1053260\tbest: 0.1053255 (15278)\ttotal: 7m 29s\tremaining: 8h 2m 38s\n",
      "15600:\tlearn: 0.0976999\ttest: 0.1052644\tbest: 0.1052638 (15595)\ttotal: 7m 38s\tremaining: 8h 2m 15s\n",
      "15900:\tlearn: 0.0975439\ttest: 0.1051948\tbest: 0.1051948 (15900)\ttotal: 7m 47s\tremaining: 8h 2m 3s\n",
      "16200:\tlearn: 0.0973650\ttest: 0.1051238\tbest: 0.1051238 (16200)\ttotal: 7m 56s\tremaining: 8h 1m 49s\n",
      "16500:\tlearn: 0.0971905\ttest: 0.1050626\tbest: 0.1050621 (16493)\ttotal: 8m 4s\tremaining: 8h 1m 33s\n",
      "16800:\tlearn: 0.0970269\ttest: 0.1050183\tbest: 0.1050183 (16799)\ttotal: 8m 13s\tremaining: 8h 1m 12s\n",
      "17100:\tlearn: 0.0968490\ttest: 0.1049535\tbest: 0.1049535 (17100)\ttotal: 8m 22s\tremaining: 8h 58s\n",
      "17400:\tlearn: 0.0966841\ttest: 0.1049053\tbest: 0.1049044 (17395)\ttotal: 8m 30s\tremaining: 8h 37s\n",
      "17700:\tlearn: 0.0965421\ttest: 0.1048519\tbest: 0.1048517 (17695)\ttotal: 8m 39s\tremaining: 8h 14s\n",
      "18000:\tlearn: 0.0963884\ttest: 0.1048033\tbest: 0.1048015 (17996)\ttotal: 8m 47s\tremaining: 7h 59m 59s\n",
      "18300:\tlearn: 0.0962539\ttest: 0.1047566\tbest: 0.1047566 (18300)\ttotal: 8m 56s\tremaining: 7h 59m 44s\n",
      "18600:\tlearn: 0.0961092\ttest: 0.1047103\tbest: 0.1047102 (18598)\ttotal: 9m 5s\tremaining: 7h 59m 29s\n",
      "18900:\tlearn: 0.0959677\ttest: 0.1046765\tbest: 0.1046761 (18868)\ttotal: 9m 13s\tremaining: 7h 59m 10s\n",
      "19200:\tlearn: 0.0958200\ttest: 0.1046296\tbest: 0.1046295 (19198)\ttotal: 9m 22s\tremaining: 7h 58m 54s\n",
      "19500:\tlearn: 0.0956846\ttest: 0.1045888\tbest: 0.1045883 (19499)\ttotal: 9m 31s\tremaining: 7h 58m 38s\n",
      "19800:\tlearn: 0.0955465\ttest: 0.1045510\tbest: 0.1045510 (19800)\ttotal: 9m 39s\tremaining: 7h 58m 22s\n",
      "20100:\tlearn: 0.0954062\ttest: 0.1045200\tbest: 0.1045195 (20096)\ttotal: 9m 48s\tremaining: 7h 58m 7s\n",
      "20400:\tlearn: 0.0952686\ttest: 0.1044831\tbest: 0.1044812 (20395)\ttotal: 9m 57s\tremaining: 7h 57m 51s\n",
      "20700:\tlearn: 0.0951354\ttest: 0.1044525\tbest: 0.1044525 (20700)\ttotal: 10m 5s\tremaining: 7h 57m 36s\n",
      "21000:\tlearn: 0.0950091\ttest: 0.1044220\tbest: 0.1044217 (20994)\ttotal: 10m 14s\tremaining: 7h 57m 17s\n",
      "21300:\tlearn: 0.0948767\ttest: 0.1043893\tbest: 0.1043883 (21299)\ttotal: 10m 23s\tremaining: 7h 57m 12s\n",
      "21600:\tlearn: 0.0947522\ttest: 0.1043525\tbest: 0.1043525 (21600)\ttotal: 10m 32s\tremaining: 7h 57m 7s\n",
      "21900:\tlearn: 0.0946256\ttest: 0.1043194\tbest: 0.1043191 (21896)\ttotal: 10m 40s\tremaining: 7h 57m 3s\n",
      "22200:\tlearn: 0.0945078\ttest: 0.1042868\tbest: 0.1042868 (22200)\ttotal: 10m 49s\tremaining: 7h 56m 52s\n",
      "22500:\tlearn: 0.0943933\ttest: 0.1042664\tbest: 0.1042640 (22475)\ttotal: 10m 58s\tremaining: 7h 56m 42s\n",
      "22800:\tlearn: 0.0942790\ttest: 0.1042348\tbest: 0.1042341 (22799)\ttotal: 11m 7s\tremaining: 7h 56m 28s\n",
      "23100:\tlearn: 0.0941475\ttest: 0.1042116\tbest: 0.1042116 (23100)\ttotal: 11m 15s\tremaining: 7h 56m 12s\n",
      "23400:\tlearn: 0.0940359\ttest: 0.1041861\tbest: 0.1041861 (23400)\ttotal: 11m 24s\tremaining: 7h 55m 55s\n",
      "23700:\tlearn: 0.0939257\ttest: 0.1041699\tbest: 0.1041692 (23678)\ttotal: 11m 32s\tremaining: 7h 55m 39s\n",
      "24000:\tlearn: 0.0938164\ttest: 0.1041451\tbest: 0.1041451 (24000)\ttotal: 11m 41s\tremaining: 7h 55m 27s\n",
      "24300:\tlearn: 0.0937145\ttest: 0.1041348\tbest: 0.1041327 (24265)\ttotal: 11m 50s\tremaining: 7h 55m 16s\n",
      "24600:\tlearn: 0.0936166\ttest: 0.1041124\tbest: 0.1041124 (24599)\ttotal: 11m 58s\tremaining: 7h 54m 59s\n",
      "24900:\tlearn: 0.0935260\ttest: 0.1040920\tbest: 0.1040918 (24898)\ttotal: 12m 7s\tremaining: 7h 54m 42s\n",
      "25200:\tlearn: 0.0934134\ttest: 0.1040641\tbest: 0.1040626 (25194)\ttotal: 12m 16s\tremaining: 7h 54m 30s\n",
      "25500:\tlearn: 0.0933041\ttest: 0.1040426\tbest: 0.1040425 (25497)\ttotal: 12m 24s\tremaining: 7h 54m 18s\n",
      "25800:\tlearn: 0.0931836\ttest: 0.1040182\tbest: 0.1040177 (25795)\ttotal: 12m 33s\tremaining: 7h 54m 6s\n",
      "26100:\tlearn: 0.0930833\ttest: 0.1039902\tbest: 0.1039902 (26100)\ttotal: 12m 42s\tremaining: 7h 53m 54s\n",
      "26400:\tlearn: 0.0929766\ttest: 0.1039659\tbest: 0.1039659 (26400)\ttotal: 12m 50s\tremaining: 7h 53m 48s\n",
      "26700:\tlearn: 0.0928853\ttest: 0.1039437\tbest: 0.1039437 (26700)\ttotal: 12m 59s\tremaining: 7h 53m 35s\n",
      "27000:\tlearn: 0.0927937\ttest: 0.1039140\tbest: 0.1039139 (26999)\ttotal: 13m 8s\tremaining: 7h 53m 21s\n",
      "27300:\tlearn: 0.0926998\ttest: 0.1039029\tbest: 0.1039024 (27295)\ttotal: 13m 16s\tremaining: 7h 53m 7s\n",
      "27600:\tlearn: 0.0925940\ttest: 0.1038849\tbest: 0.1038849 (27600)\ttotal: 13m 25s\tremaining: 7h 52m 54s\n",
      "27900:\tlearn: 0.0924927\ttest: 0.1038704\tbest: 0.1038691 (27860)\ttotal: 13m 34s\tremaining: 7h 52m 44s\n",
      "28200:\tlearn: 0.0923880\ttest: 0.1038558\tbest: 0.1038558 (28200)\ttotal: 13m 42s\tremaining: 7h 52m 32s\n",
      "28500:\tlearn: 0.0923054\ttest: 0.1038355\tbest: 0.1038355 (28500)\ttotal: 13m 51s\tremaining: 7h 52m 18s\n",
      "28800:\tlearn: 0.0922125\ttest: 0.1038101\tbest: 0.1038100 (28790)\ttotal: 14m\tremaining: 7h 52m 5s\n",
      "29100:\tlearn: 0.0921125\ttest: 0.1037988\tbest: 0.1037972 (29093)\ttotal: 14m 8s\tremaining: 7h 51m 51s\n",
      "29400:\tlearn: 0.0920253\ttest: 0.1037910\tbest: 0.1037863 (29267)\ttotal: 14m 17s\tremaining: 7h 51m 37s\n",
      "29700:\tlearn: 0.0919351\ttest: 0.1037741\tbest: 0.1037732 (29685)\ttotal: 14m 25s\tremaining: 7h 51m 27s\n",
      "30000:\tlearn: 0.0918483\ttest: 0.1037399\tbest: 0.1037398 (29998)\ttotal: 14m 34s\tremaining: 7h 51m 19s\n",
      "30300:\tlearn: 0.0917517\ttest: 0.1037174\tbest: 0.1037174 (30299)\ttotal: 14m 43s\tremaining: 7h 51m 8s\n",
      "30600:\tlearn: 0.0916685\ttest: 0.1036992\tbest: 0.1036992 (30600)\ttotal: 14m 52s\tremaining: 7h 50m 57s\n",
      "30900:\tlearn: 0.0915832\ttest: 0.1036715\tbest: 0.1036699 (30897)\ttotal: 15m\tremaining: 7h 50m 51s\n",
      "31200:\tlearn: 0.0914869\ttest: 0.1036530\tbest: 0.1036525 (31198)\ttotal: 15m 9s\tremaining: 7h 50m 41s\n",
      "31500:\tlearn: 0.0913873\ttest: 0.1036473\tbest: 0.1036466 (31497)\ttotal: 15m 18s\tremaining: 7h 50m 27s\n",
      "31800:\tlearn: 0.0913068\ttest: 0.1036343\tbest: 0.1036343 (31800)\ttotal: 15m 26s\tremaining: 7h 50m 14s\n",
      "32100:\tlearn: 0.0912217\ttest: 0.1036185\tbest: 0.1036180 (32090)\ttotal: 15m 35s\tremaining: 7h 50m 5s\n",
      "32400:\tlearn: 0.0911324\ttest: 0.1036011\tbest: 0.1035997 (32365)\ttotal: 15m 44s\tremaining: 7h 49m 58s\n",
      "32700:\tlearn: 0.0910427\ttest: 0.1035905\tbest: 0.1035897 (32642)\ttotal: 15m 52s\tremaining: 7h 49m 48s\n",
      "33000:\tlearn: 0.0909652\ttest: 0.1035796\tbest: 0.1035788 (32963)\ttotal: 16m 1s\tremaining: 7h 49m 33s\n",
      "33300:\tlearn: 0.0908742\ttest: 0.1035749\tbest: 0.1035712 (33187)\ttotal: 16m 10s\tremaining: 7h 49m 21s\n",
      "33600:\tlearn: 0.0907925\ttest: 0.1035650\tbest: 0.1035619 (33482)\ttotal: 16m 18s\tremaining: 7h 49m 7s\n",
      "33900:\tlearn: 0.0907046\ttest: 0.1035570\tbest: 0.1035539 (33818)\ttotal: 16m 27s\tremaining: 7h 48m 53s\n",
      "34200:\tlearn: 0.0906117\ttest: 0.1035530\tbest: 0.1035499 (34088)\ttotal: 16m 35s\tremaining: 7h 48m 40s\n",
      "34500:\tlearn: 0.0905386\ttest: 0.1035374\tbest: 0.1035353 (34461)\ttotal: 16m 44s\tremaining: 7h 48m 27s\n",
      "34800:\tlearn: 0.0904562\ttest: 0.1035260\tbest: 0.1035247 (34784)\ttotal: 16m 52s\tremaining: 7h 48m 14s\n",
      "35100:\tlearn: 0.0903779\ttest: 0.1035141\tbest: 0.1035124 (35025)\ttotal: 17m 1s\tremaining: 7h 47m 58s\n",
      "35400:\tlearn: 0.0902933\ttest: 0.1034999\tbest: 0.1034989 (35399)\ttotal: 17m 9s\tremaining: 7h 47m 43s\n",
      "35700:\tlearn: 0.0902122\ttest: 0.1034880\tbest: 0.1034858 (35583)\ttotal: 17m 18s\tremaining: 7h 47m 31s\n",
      "36000:\tlearn: 0.0901327\ttest: 0.1034751\tbest: 0.1034751 (36000)\ttotal: 17m 27s\tremaining: 7h 47m 17s\n",
      "36300:\tlearn: 0.0900579\ttest: 0.1034661\tbest: 0.1034661 (36300)\ttotal: 17m 35s\tremaining: 7h 47m 6s\n",
      "36600:\tlearn: 0.0899890\ttest: 0.1034573\tbest: 0.1034573 (36599)\ttotal: 17m 44s\tremaining: 7h 46m 55s\n",
      "36900:\tlearn: 0.0899215\ttest: 0.1034431\tbest: 0.1034400 (36875)\ttotal: 17m 53s\tremaining: 7h 46m 45s\n",
      "37200:\tlearn: 0.0898585\ttest: 0.1034297\tbest: 0.1034295 (37194)\ttotal: 18m 1s\tremaining: 7h 46m 32s\n",
      "37500:\tlearn: 0.0897878\ttest: 0.1034278\tbest: 0.1034275 (37281)\ttotal: 18m 10s\tremaining: 7h 46m 21s\n",
      "37800:\tlearn: 0.0897102\ttest: 0.1034201\tbest: 0.1034201 (37800)\ttotal: 18m 18s\tremaining: 7h 46m 10s\n",
      "38100:\tlearn: 0.0896331\ttest: 0.1033887\tbest: 0.1033887 (38100)\ttotal: 18m 27s\tremaining: 7h 45m 57s\n",
      "38400:\tlearn: 0.0895569\ttest: 0.1033854\tbest: 0.1033854 (38400)\ttotal: 18m 35s\tremaining: 7h 45m 44s\n",
      "38700:\tlearn: 0.0894751\ttest: 0.1033873\tbest: 0.1033809 (38574)\ttotal: 18m 44s\tremaining: 7h 45m 32s\n",
      "39000:\tlearn: 0.0894007\ttest: 0.1033749\tbest: 0.1033722 (38963)\ttotal: 18m 53s\tremaining: 7h 45m 22s\n",
      "39300:\tlearn: 0.0893269\ttest: 0.1033602\tbest: 0.1033602 (39300)\ttotal: 19m 1s\tremaining: 7h 45m 15s\n",
      "39600:\tlearn: 0.0892490\ttest: 0.1033531\tbest: 0.1033523 (39587)\ttotal: 19m 10s\tremaining: 7h 45m 5s\n",
      "39900:\tlearn: 0.0891781\ttest: 0.1033409\tbest: 0.1033379 (39897)\ttotal: 19m 19s\tremaining: 7h 44m 54s\n",
      "40200:\tlearn: 0.0891097\ttest: 0.1033337\tbest: 0.1033336 (40199)\ttotal: 19m 27s\tremaining: 7h 44m 43s\n",
      "40500:\tlearn: 0.0890477\ttest: 0.1033283\tbest: 0.1033283 (40500)\ttotal: 19m 36s\tremaining: 7h 44m 32s\n",
      "40800:\tlearn: 0.0889743\ttest: 0.1033172\tbest: 0.1033148 (40791)\ttotal: 19m 45s\tremaining: 7h 44m 20s\n",
      "41100:\tlearn: 0.0889061\ttest: 0.1033188\tbest: 0.1033134 (40924)\ttotal: 19m 53s\tremaining: 7h 44m 8s\n",
      "41400:\tlearn: 0.0888379\ttest: 0.1033069\tbest: 0.1033067 (41397)\ttotal: 20m 2s\tremaining: 7h 43m 57s\n",
      "41700:\tlearn: 0.0887664\ttest: 0.1032967\tbest: 0.1032967 (41700)\ttotal: 20m 10s\tremaining: 7h 43m 45s\n",
      "42000:\tlearn: 0.0887021\ttest: 0.1032840\tbest: 0.1032840 (41998)\ttotal: 20m 19s\tremaining: 7h 43m 32s\n",
      "42300:\tlearn: 0.0886402\ttest: 0.1032748\tbest: 0.1032748 (42297)\ttotal: 20m 27s\tremaining: 7h 43m 20s\n",
      "42600:\tlearn: 0.0885748\ttest: 0.1032775\tbest: 0.1032727 (42370)\ttotal: 20m 36s\tremaining: 7h 43m 9s\n",
      "Stopped by overfitting detector  (300 iterations wait)\n",
      "\n",
      "bestTest = 0.1032726553\n",
      "bestIteration = 42370\n",
      "\n",
      "Shrink model to first 42371 iterations.\n",
      "fold n°8\n",
      "0:\tlearn: 0.9753275\ttest: 0.9742226\tbest: 0.9742226 (0)\ttotal: 31.9ms\tremaining: 8h 51m 58s\n",
      "300:\tlearn: 0.1699710\ttest: 0.1712473\tbest: 0.1712473 (300)\ttotal: 8.88s\tremaining: 8h 11m 22s\n",
      "600:\tlearn: 0.1443236\ttest: 0.1468633\tbest: 0.1468633 (600)\ttotal: 17.5s\tremaining: 8h 4m 33s\n",
      "900:\tlearn: 0.1331981\ttest: 0.1365988\tbest: 0.1365988 (900)\ttotal: 26.2s\tremaining: 8h 3m 52s\n",
      "1200:\tlearn: 0.1269090\ttest: 0.1308316\tbest: 0.1308316 (1200)\ttotal: 34.9s\tremaining: 8h 3m 42s\n",
      "1500:\tlearn: 0.1229977\ttest: 0.1273455\tbest: 0.1273455 (1500)\ttotal: 43.6s\tremaining: 8h 3m 39s\n",
      "1800:\tlearn: 0.1201338\ttest: 0.1247768\tbest: 0.1247768 (1800)\ttotal: 52.4s\tremaining: 8h 4m 11s\n",
      "2100:\tlearn: 0.1178300\ttest: 0.1227358\tbest: 0.1227358 (2100)\ttotal: 1m 1s\tremaining: 8h 4m 36s\n",
      "2400:\tlearn: 0.1160174\ttest: 0.1211417\tbest: 0.1211417 (2400)\ttotal: 1m 9s\tremaining: 8h 4m\n",
      "2700:\tlearn: 0.1145360\ttest: 0.1198574\tbest: 0.1198574 (2700)\ttotal: 1m 18s\tremaining: 8h 4m 11s\n",
      "3000:\tlearn: 0.1132608\ttest: 0.1187859\tbest: 0.1187859 (3000)\ttotal: 1m 27s\tremaining: 8h 4m 13s\n",
      "3300:\tlearn: 0.1121642\ttest: 0.1178628\tbest: 0.1178628 (3300)\ttotal: 1m 36s\tremaining: 8h 4m 17s\n",
      "3600:\tlearn: 0.1111637\ttest: 0.1170436\tbest: 0.1170436 (3600)\ttotal: 1m 45s\tremaining: 8h 4m 31s\n",
      "3900:\tlearn: 0.1102532\ttest: 0.1163121\tbest: 0.1163121 (3900)\ttotal: 1m 53s\tremaining: 8h 4m 20s\n",
      "4200:\tlearn: 0.1094426\ttest: 0.1157356\tbest: 0.1157356 (4200)\ttotal: 2m 2s\tremaining: 8h 4m 14s\n",
      "4500:\tlearn: 0.1087560\ttest: 0.1152125\tbest: 0.1152125 (4500)\ttotal: 2m 11s\tremaining: 8h 4m 40s\n",
      "4800:\tlearn: 0.1081014\ttest: 0.1147253\tbest: 0.1147253 (4800)\ttotal: 2m 20s\tremaining: 8h 4m 27s\n",
      "5100:\tlearn: 0.1074998\ttest: 0.1142892\tbest: 0.1142892 (5100)\ttotal: 2m 28s\tremaining: 8h 4m 12s\n",
      "5400:\tlearn: 0.1069812\ttest: 0.1139075\tbest: 0.1139075 (5400)\ttotal: 2m 37s\tremaining: 8h 4m\n",
      "5700:\tlearn: 0.1064401\ttest: 0.1135238\tbest: 0.1135238 (5700)\ttotal: 2m 46s\tremaining: 8h 4m\n",
      "6000:\tlearn: 0.1059373\ttest: 0.1131825\tbest: 0.1131825 (6000)\ttotal: 2m 55s\tremaining: 8h 4m 4s\n",
      "6300:\tlearn: 0.1054640\ttest: 0.1128587\tbest: 0.1128587 (6300)\ttotal: 3m 4s\tremaining: 8h 3m 39s\n",
      "6600:\tlearn: 0.1050113\ttest: 0.1125715\tbest: 0.1125715 (6600)\ttotal: 3m 12s\tremaining: 8h 3m 34s\n",
      "6900:\tlearn: 0.1046189\ttest: 0.1123033\tbest: 0.1123033 (6900)\ttotal: 3m 21s\tremaining: 8h 3m 35s\n",
      "7200:\tlearn: 0.1041915\ttest: 0.1120135\tbest: 0.1120135 (7200)\ttotal: 3m 30s\tremaining: 8h 3m 22s\n",
      "7500:\tlearn: 0.1038207\ttest: 0.1117845\tbest: 0.1117845 (7500)\ttotal: 3m 39s\tremaining: 8h 3m 4s\n",
      "7800:\tlearn: 0.1034646\ttest: 0.1115549\tbest: 0.1115549 (7800)\ttotal: 3m 47s\tremaining: 8h 3m\n",
      "8100:\tlearn: 0.1031271\ttest: 0.1113608\tbest: 0.1113608 (8100)\ttotal: 3m 56s\tremaining: 8h 2m 43s\n",
      "8400:\tlearn: 0.1027744\ttest: 0.1111396\tbest: 0.1111396 (8400)\ttotal: 4m 5s\tremaining: 8h 2m 26s\n",
      "8700:\tlearn: 0.1024545\ttest: 0.1109374\tbest: 0.1109374 (8700)\ttotal: 4m 14s\tremaining: 8h 2m 18s\n",
      "9000:\tlearn: 0.1021628\ttest: 0.1107648\tbest: 0.1107648 (9000)\ttotal: 4m 22s\tremaining: 8h 2m 34s\n",
      "9300:\tlearn: 0.1018711\ttest: 0.1105854\tbest: 0.1105854 (9300)\ttotal: 4m 31s\tremaining: 8h 2m 51s\n",
      "9600:\tlearn: 0.1015966\ttest: 0.1104344\tbest: 0.1104344 (9600)\ttotal: 4m 40s\tremaining: 8h 2m 54s\n",
      "9900:\tlearn: 0.1013244\ttest: 0.1102799\tbest: 0.1102799 (9900)\ttotal: 4m 49s\tremaining: 8h 2m 39s\n",
      "10200:\tlearn: 0.1010363\ttest: 0.1101344\tbest: 0.1101344 (10200)\ttotal: 4m 58s\tremaining: 8h 2m 27s\n",
      "10500:\tlearn: 0.1007570\ttest: 0.1099910\tbest: 0.1099910 (10500)\ttotal: 5m 7s\tremaining: 8h 2m 14s\n",
      "10800:\tlearn: 0.1005257\ttest: 0.1098689\tbest: 0.1098689 (10800)\ttotal: 5m 15s\tremaining: 8h 1m 57s\n",
      "11100:\tlearn: 0.1002890\ttest: 0.1097435\tbest: 0.1097435 (11100)\ttotal: 5m 24s\tremaining: 8h 1m 48s\n",
      "11400:\tlearn: 0.1000721\ttest: 0.1096401\tbest: 0.1096401 (11400)\ttotal: 5m 33s\tremaining: 8h 1m 31s\n",
      "11700:\tlearn: 0.0998452\ttest: 0.1095549\tbest: 0.1095548 (11692)\ttotal: 5m 41s\tremaining: 8h 1m 15s\n",
      "12000:\tlearn: 0.0996187\ttest: 0.1094433\tbest: 0.1094433 (12000)\ttotal: 5m 50s\tremaining: 8h 1m 7s\n",
      "12300:\tlearn: 0.0994088\ttest: 0.1093358\tbest: 0.1093357 (12298)\ttotal: 5m 59s\tremaining: 8h 56s\n",
      "12600:\tlearn: 0.0991942\ttest: 0.1092204\tbest: 0.1092204 (12600)\ttotal: 6m 8s\tremaining: 8h 43s\n",
      "12900:\tlearn: 0.0989740\ttest: 0.1091224\tbest: 0.1091224 (12900)\ttotal: 6m 16s\tremaining: 8h 28s\n",
      "13200:\tlearn: 0.0987596\ttest: 0.1090286\tbest: 0.1090278 (13199)\ttotal: 6m 25s\tremaining: 8h 16s\n",
      "13500:\tlearn: 0.0985742\ttest: 0.1089588\tbest: 0.1089588 (13500)\ttotal: 6m 34s\tremaining: 8h 1s\n",
      "13800:\tlearn: 0.0983943\ttest: 0.1088800\tbest: 0.1088800 (13800)\ttotal: 6m 42s\tremaining: 7h 59m 53s\n",
      "14100:\tlearn: 0.0982013\ttest: 0.1087947\tbest: 0.1087947 (14100)\ttotal: 6m 51s\tremaining: 7h 59m 44s\n",
      "14400:\tlearn: 0.0980144\ttest: 0.1087224\tbest: 0.1087220 (14398)\ttotal: 7m\tremaining: 7h 59m 26s\n",
      "14700:\tlearn: 0.0978208\ttest: 0.1086435\tbest: 0.1086429 (14696)\ttotal: 7m 9s\tremaining: 7h 59m 17s\n",
      "15000:\tlearn: 0.0976441\ttest: 0.1085716\tbest: 0.1085716 (15000)\ttotal: 7m 17s\tremaining: 7h 59m 2s\n",
      "15300:\tlearn: 0.0974737\ttest: 0.1085166\tbest: 0.1085166 (15300)\ttotal: 7m 26s\tremaining: 7h 58m 50s\n",
      "15600:\tlearn: 0.0973009\ttest: 0.1084563\tbest: 0.1084552 (15596)\ttotal: 7m 35s\tremaining: 7h 58m 35s\n",
      "15900:\tlearn: 0.0971244\ttest: 0.1083905\tbest: 0.1083905 (15900)\ttotal: 7m 43s\tremaining: 7h 58m 25s\n",
      "16200:\tlearn: 0.0969712\ttest: 0.1083344\tbest: 0.1083335 (16191)\ttotal: 7m 52s\tremaining: 7h 58m 13s\n",
      "16500:\tlearn: 0.0968268\ttest: 0.1082679\tbest: 0.1082679 (16500)\ttotal: 8m 1s\tremaining: 7h 58m 1s\n",
      "16800:\tlearn: 0.0966875\ttest: 0.1082253\tbest: 0.1082240 (16783)\ttotal: 8m 9s\tremaining: 7h 57m 50s\n",
      "17100:\tlearn: 0.0965338\ttest: 0.1081730\tbest: 0.1081725 (17096)\ttotal: 8m 18s\tremaining: 7h 57m 40s\n",
      "17400:\tlearn: 0.0963846\ttest: 0.1081106\tbest: 0.1081106 (17400)\ttotal: 8m 27s\tremaining: 7h 57m 25s\n",
      "17700:\tlearn: 0.0962423\ttest: 0.1080630\tbest: 0.1080609 (17661)\ttotal: 8m 35s\tremaining: 7h 57m 14s\n",
      "18000:\tlearn: 0.0960792\ttest: 0.1080060\tbest: 0.1080059 (17999)\ttotal: 8m 44s\tremaining: 7h 57m 4s\n",
      "18300:\tlearn: 0.0959384\ttest: 0.1079555\tbest: 0.1079555 (18300)\ttotal: 8m 53s\tremaining: 7h 56m 51s\n",
      "18600:\tlearn: 0.0958002\ttest: 0.1079050\tbest: 0.1079033 (18598)\ttotal: 9m 2s\tremaining: 7h 56m 36s\n",
      "18900:\tlearn: 0.0956584\ttest: 0.1078659\tbest: 0.1078659 (18900)\ttotal: 9m 10s\tremaining: 7h 56m 21s\n",
      "19200:\tlearn: 0.0955147\ttest: 0.1078225\tbest: 0.1078224 (19198)\ttotal: 9m 19s\tremaining: 7h 56m 6s\n",
      "19500:\tlearn: 0.0953744\ttest: 0.1078012\tbest: 0.1077978 (19490)\ttotal: 9m 27s\tremaining: 7h 55m 55s\n",
      "19800:\tlearn: 0.0952252\ttest: 0.1077518\tbest: 0.1077515 (19799)\ttotal: 9m 36s\tremaining: 7h 55m 46s\n",
      "20100:\tlearn: 0.0950931\ttest: 0.1077009\tbest: 0.1077002 (20093)\ttotal: 9m 45s\tremaining: 7h 55m 36s\n",
      "20400:\tlearn: 0.0949648\ttest: 0.1076546\tbest: 0.1076546 (20400)\ttotal: 9m 54s\tremaining: 7h 55m 28s\n",
      "20700:\tlearn: 0.0948256\ttest: 0.1076106\tbest: 0.1076106 (20700)\ttotal: 10m 2s\tremaining: 7h 55m 15s\n",
      "21000:\tlearn: 0.0946912\ttest: 0.1075708\tbest: 0.1075708 (21000)\ttotal: 10m 11s\tremaining: 7h 55m 2s\n",
      "21300:\tlearn: 0.0945589\ttest: 0.1075351\tbest: 0.1075342 (21298)\ttotal: 10m 20s\tremaining: 7h 54m 51s\n",
      "21600:\tlearn: 0.0944457\ttest: 0.1075108\tbest: 0.1075089 (21575)\ttotal: 10m 28s\tremaining: 7h 54m 39s\n",
      "21900:\tlearn: 0.0943357\ttest: 0.1074834\tbest: 0.1074816 (21893)\ttotal: 10m 37s\tremaining: 7h 54m 30s\n",
      "22200:\tlearn: 0.0941979\ttest: 0.1074422\tbest: 0.1074422 (22200)\ttotal: 10m 46s\tremaining: 7h 54m 21s\n",
      "22500:\tlearn: 0.0940670\ttest: 0.1074073\tbest: 0.1074073 (22500)\ttotal: 10m 54s\tremaining: 7h 54m 7s\n",
      "22800:\tlearn: 0.0939452\ttest: 0.1073770\tbest: 0.1073760 (22798)\ttotal: 11m 3s\tremaining: 7h 53m 55s\n",
      "23100:\tlearn: 0.0938299\ttest: 0.1073413\tbest: 0.1073413 (23100)\ttotal: 11m 12s\tremaining: 7h 53m 44s\n",
      "23400:\tlearn: 0.0937251\ttest: 0.1073056\tbest: 0.1073055 (23396)\ttotal: 11m 20s\tremaining: 7h 53m 32s\n",
      "23700:\tlearn: 0.0936200\ttest: 0.1072702\tbest: 0.1072670 (23693)\ttotal: 11m 29s\tremaining: 7h 53m 20s\n",
      "24000:\tlearn: 0.0935043\ttest: 0.1072377\tbest: 0.1072375 (23990)\ttotal: 11m 38s\tremaining: 7h 53m 10s\n",
      "24300:\tlearn: 0.0933923\ttest: 0.1072131\tbest: 0.1072129 (24297)\ttotal: 11m 46s\tremaining: 7h 52m 56s\n",
      "24600:\tlearn: 0.0932892\ttest: 0.1071875\tbest: 0.1071869 (24598)\ttotal: 11m 55s\tremaining: 7h 52m 43s\n",
      "24900:\tlearn: 0.0931729\ttest: 0.1071548\tbest: 0.1071545 (24880)\ttotal: 12m 4s\tremaining: 7h 52m 32s\n",
      "25200:\tlearn: 0.0930522\ttest: 0.1071181\tbest: 0.1071180 (25198)\ttotal: 12m 12s\tremaining: 7h 52m 23s\n",
      "25500:\tlearn: 0.0929467\ttest: 0.1070916\tbest: 0.1070916 (25500)\ttotal: 12m 21s\tremaining: 7h 52m 12s\n",
      "25800:\tlearn: 0.0928597\ttest: 0.1070736\tbest: 0.1070736 (25797)\ttotal: 12m 29s\tremaining: 7h 51m 57s\n",
      "26100:\tlearn: 0.0927746\ttest: 0.1070533\tbest: 0.1070492 (26056)\ttotal: 12m 38s\tremaining: 7h 51m 44s\n",
      "26400:\tlearn: 0.0926797\ttest: 0.1070240\tbest: 0.1070240 (26399)\ttotal: 12m 47s\tremaining: 7h 51m 32s\n",
      "26700:\tlearn: 0.0925853\ttest: 0.1070028\tbest: 0.1070022 (26672)\ttotal: 12m 55s\tremaining: 7h 51m 15s\n",
      "27000:\tlearn: 0.0924998\ttest: 0.1069792\tbest: 0.1069787 (26995)\ttotal: 13m 4s\tremaining: 7h 51m 7s\n",
      "27300:\tlearn: 0.0924021\ttest: 0.1069554\tbest: 0.1069554 (27300)\ttotal: 13m 13s\tremaining: 7h 50m 58s\n",
      "27600:\tlearn: 0.0923046\ttest: 0.1069289\tbest: 0.1069271 (27555)\ttotal: 13m 21s\tremaining: 7h 50m 46s\n",
      "27900:\tlearn: 0.0922204\ttest: 0.1069129\tbest: 0.1069129 (27899)\ttotal: 13m 30s\tremaining: 7h 50m 38s\n",
      "28200:\tlearn: 0.0921322\ttest: 0.1068909\tbest: 0.1068885 (28182)\ttotal: 13m 39s\tremaining: 7h 50m 30s\n",
      "28500:\tlearn: 0.0920526\ttest: 0.1068802\tbest: 0.1068802 (28500)\ttotal: 13m 47s\tremaining: 7h 50m 18s\n",
      "28800:\tlearn: 0.0919692\ttest: 0.1068555\tbest: 0.1068549 (28765)\ttotal: 13m 56s\tremaining: 7h 50m 6s\n",
      "29100:\tlearn: 0.0918756\ttest: 0.1068289\tbest: 0.1068289 (29100)\ttotal: 14m 5s\tremaining: 7h 49m 55s\n",
      "29400:\tlearn: 0.0917903\ttest: 0.1068098\tbest: 0.1068085 (29390)\ttotal: 14m 13s\tremaining: 7h 49m 45s\n",
      "29700:\tlearn: 0.0917096\ttest: 0.1067917\tbest: 0.1067917 (29700)\ttotal: 14m 22s\tremaining: 7h 49m 33s\n",
      "30000:\tlearn: 0.0916175\ttest: 0.1067689\tbest: 0.1067689 (30000)\ttotal: 14m 31s\tremaining: 7h 49m 22s\n",
      "30300:\tlearn: 0.0915467\ttest: 0.1067565\tbest: 0.1067563 (30296)\ttotal: 14m 39s\tremaining: 7h 49m 10s\n",
      "30600:\tlearn: 0.0914581\ttest: 0.1067218\tbest: 0.1067217 (30593)\ttotal: 14m 48s\tremaining: 7h 48m 57s\n",
      "30900:\tlearn: 0.0913768\ttest: 0.1067027\tbest: 0.1067019 (30886)\ttotal: 14m 56s\tremaining: 7h 48m 47s\n",
      "31200:\tlearn: 0.0913011\ttest: 0.1066852\tbest: 0.1066841 (31177)\ttotal: 15m 5s\tremaining: 7h 48m 34s\n",
      "31500:\tlearn: 0.0912225\ttest: 0.1066804\tbest: 0.1066804 (31500)\ttotal: 15m 14s\tremaining: 7h 48m 23s\n",
      "31800:\tlearn: 0.0911326\ttest: 0.1066564\tbest: 0.1066564 (31800)\ttotal: 15m 22s\tremaining: 7h 48m 12s\n",
      "32100:\tlearn: 0.0910513\ttest: 0.1066393\tbest: 0.1066393 (32099)\ttotal: 15m 31s\tremaining: 7h 48m 1s\n",
      "32400:\tlearn: 0.0909728\ttest: 0.1066203\tbest: 0.1066197 (32393)\ttotal: 15m 39s\tremaining: 7h 47m 47s\n",
      "32700:\tlearn: 0.0908834\ttest: 0.1065980\tbest: 0.1065980 (32698)\ttotal: 15m 48s\tremaining: 7h 47m 37s\n",
      "33000:\tlearn: 0.0908154\ttest: 0.1065775\tbest: 0.1065775 (33000)\ttotal: 15m 57s\tremaining: 7h 47m 27s\n",
      "33300:\tlearn: 0.0907387\ttest: 0.1065480\tbest: 0.1065480 (33300)\ttotal: 16m 5s\tremaining: 7h 47m 15s\n",
      "33600:\tlearn: 0.0906600\ttest: 0.1065279\tbest: 0.1065277 (33594)\ttotal: 16m 14s\tremaining: 7h 47m 6s\n",
      "33900:\tlearn: 0.0905765\ttest: 0.1065162\tbest: 0.1065157 (33886)\ttotal: 16m 23s\tremaining: 7h 46m 57s\n",
      "34200:\tlearn: 0.0904894\ttest: 0.1064960\tbest: 0.1064960 (34200)\ttotal: 16m 31s\tremaining: 7h 46m 44s\n",
      "34500:\tlearn: 0.0904103\ttest: 0.1064870\tbest: 0.1064868 (34495)\ttotal: 16m 40s\tremaining: 7h 46m 35s\n",
      "34800:\tlearn: 0.0903344\ttest: 0.1064825\tbest: 0.1064789 (34665)\ttotal: 16m 48s\tremaining: 7h 46m 22s\n",
      "35100:\tlearn: 0.0902605\ttest: 0.1064726\tbest: 0.1064726 (35100)\ttotal: 16m 57s\tremaining: 7h 46m 9s\n",
      "35400:\tlearn: 0.0901823\ttest: 0.1064562\tbest: 0.1064539 (35356)\ttotal: 17m 6s\tremaining: 7h 46m 1s\n",
      "35700:\tlearn: 0.0901117\ttest: 0.1064433\tbest: 0.1064433 (35700)\ttotal: 17m 14s\tremaining: 7h 45m 50s\n",
      "36000:\tlearn: 0.0900395\ttest: 0.1064320\tbest: 0.1064320 (36000)\ttotal: 17m 23s\tremaining: 7h 45m 37s\n",
      "36300:\tlearn: 0.0899654\ttest: 0.1064112\tbest: 0.1064107 (36295)\ttotal: 17m 32s\tremaining: 7h 45m 28s\n",
      "36600:\tlearn: 0.0898942\ttest: 0.1063991\tbest: 0.1063985 (36589)\ttotal: 17m 40s\tremaining: 7h 45m 20s\n",
      "36900:\tlearn: 0.0898152\ttest: 0.1063942\tbest: 0.1063934 (36896)\ttotal: 17m 49s\tremaining: 7h 45m 11s\n",
      "37200:\tlearn: 0.0897546\ttest: 0.1063852\tbest: 0.1063837 (37159)\ttotal: 17m 58s\tremaining: 7h 45m 4s\n",
      "37500:\tlearn: 0.0896924\ttest: 0.1063809\tbest: 0.1063809 (37500)\ttotal: 18m 6s\tremaining: 7h 44m 52s\n",
      "37800:\tlearn: 0.0896177\ttest: 0.1063648\tbest: 0.1063639 (37792)\ttotal: 18m 15s\tremaining: 7h 44m 41s\n",
      "38100:\tlearn: 0.0895473\ttest: 0.1063448\tbest: 0.1063437 (38092)\ttotal: 18m 23s\tremaining: 7h 44m 31s\n",
      "38400:\tlearn: 0.0894792\ttest: 0.1063327\tbest: 0.1063327 (38400)\ttotal: 18m 32s\tremaining: 7h 44m 21s\n",
      "38700:\tlearn: 0.0894143\ttest: 0.1063285\tbest: 0.1063279 (38699)\ttotal: 18m 41s\tremaining: 7h 44m 9s\n",
      "39000:\tlearn: 0.0893449\ttest: 0.1063176\tbest: 0.1063176 (39000)\ttotal: 18m 49s\tremaining: 7h 43m 58s\n",
      "39300:\tlearn: 0.0892836\ttest: 0.1063050\tbest: 0.1063048 (39293)\ttotal: 18m 58s\tremaining: 7h 43m 51s\n",
      "39600:\tlearn: 0.0892174\ttest: 0.1062949\tbest: 0.1062935 (39579)\ttotal: 19m 7s\tremaining: 7h 43m 55s\n",
      "39900:\tlearn: 0.0891470\ttest: 0.1062943\tbest: 0.1062897 (39643)\ttotal: 19m 16s\tremaining: 7h 43m 48s\n",
      "Stopped by overfitting detector  (300 iterations wait)\n",
      "\n",
      "bestTest = 0.106289738\n",
      "bestIteration = 39643\n",
      "\n",
      "Shrink model to first 39644 iterations.\n",
      "fold n°9\n",
      "0:\tlearn: 0.9753526\ttest: 0.9747301\tbest: 0.9747301 (0)\ttotal: 31.7ms\tremaining: 8h 48m 12s\n",
      "300:\tlearn: 0.1709836\ttest: 0.1685803\tbest: 0.1685803 (300)\ttotal: 8.9s\tremaining: 8h 12m 33s\n",
      "600:\tlearn: 0.1454374\ttest: 0.1429883\tbest: 0.1429883 (600)\ttotal: 17.6s\tremaining: 8h 8m 3s\n",
      "900:\tlearn: 0.1342212\ttest: 0.1318654\tbest: 0.1318654 (900)\ttotal: 26.2s\tremaining: 8h 4m 38s\n",
      "1200:\tlearn: 0.1277865\ttest: 0.1256931\tbest: 0.1256931 (1200)\ttotal: 34.9s\tremaining: 8h 3m 27s\n",
      "1500:\tlearn: 0.1237945\ttest: 0.1219367\tbest: 0.1219367 (1500)\ttotal: 43.6s\tremaining: 8h 3m 34s\n",
      "1800:\tlearn: 0.1209057\ttest: 0.1192733\tbest: 0.1192733 (1800)\ttotal: 52.4s\tremaining: 8h 3m 51s\n",
      "2100:\tlearn: 0.1186274\ttest: 0.1171799\tbest: 0.1171799 (2100)\ttotal: 1m 1s\tremaining: 8h 5m 8s\n",
      "2400:\tlearn: 0.1168598\ttest: 0.1155305\tbest: 0.1155305 (2400)\ttotal: 1m 10s\tremaining: 8h 5m 37s\n",
      "2700:\tlearn: 0.1154023\ttest: 0.1142226\tbest: 0.1142226 (2700)\ttotal: 1m 18s\tremaining: 8h 5m 22s\n",
      "3000:\tlearn: 0.1140743\ttest: 0.1130610\tbest: 0.1130610 (3000)\ttotal: 1m 27s\tremaining: 8h 4m 48s\n",
      "3300:\tlearn: 0.1130001\ttest: 0.1121199\tbest: 0.1121199 (3300)\ttotal: 1m 36s\tremaining: 8h 4m 46s\n",
      "3600:\tlearn: 0.1120470\ttest: 0.1113319\tbest: 0.1113319 (3600)\ttotal: 1m 45s\tremaining: 8h 4m 21s\n",
      "3900:\tlearn: 0.1111159\ttest: 0.1105773\tbest: 0.1105773 (3900)\ttotal: 1m 53s\tremaining: 8h 4m 30s\n",
      "4200:\tlearn: 0.1103535\ttest: 0.1099722\tbest: 0.1099722 (4200)\ttotal: 2m 2s\tremaining: 8h 4m 37s\n",
      "4500:\tlearn: 0.1096143\ttest: 0.1094111\tbest: 0.1094111 (4500)\ttotal: 2m 11s\tremaining: 8h 4m 42s\n",
      "4800:\tlearn: 0.1089392\ttest: 0.1088779\tbest: 0.1088779 (4800)\ttotal: 2m 20s\tremaining: 8h 4m 52s\n",
      "5100:\tlearn: 0.1083407\ttest: 0.1084296\tbest: 0.1084296 (5100)\ttotal: 2m 29s\tremaining: 8h 4m 50s\n",
      "5400:\tlearn: 0.1077421\ttest: 0.1080036\tbest: 0.1080036 (5400)\ttotal: 2m 37s\tremaining: 8h 4m 30s\n",
      "5700:\tlearn: 0.1072185\ttest: 0.1076253\tbest: 0.1076253 (5700)\ttotal: 2m 46s\tremaining: 8h 4m 20s\n",
      "6000:\tlearn: 0.1067053\ttest: 0.1072626\tbest: 0.1072626 (6000)\ttotal: 2m 55s\tremaining: 8h 4m 17s\n",
      "6300:\tlearn: 0.1062724\ttest: 0.1069548\tbest: 0.1069548 (6300)\ttotal: 3m 4s\tremaining: 8h 4m 27s\n",
      "6600:\tlearn: 0.1057959\ttest: 0.1066110\tbest: 0.1066110 (6600)\ttotal: 3m 13s\tremaining: 8h 4m 32s\n",
      "6900:\tlearn: 0.1054035\ttest: 0.1063496\tbest: 0.1063495 (6898)\ttotal: 3m 22s\tremaining: 8h 4m 29s\n",
      "7200:\tlearn: 0.1049948\ttest: 0.1060718\tbest: 0.1060716 (7199)\ttotal: 3m 30s\tremaining: 8h 4m 21s\n",
      "7500:\tlearn: 0.1046375\ttest: 0.1058358\tbest: 0.1058358 (7500)\ttotal: 3m 39s\tremaining: 8h 4m 31s\n",
      "7800:\tlearn: 0.1042818\ttest: 0.1056088\tbest: 0.1056088 (7800)\ttotal: 3m 48s\tremaining: 8h 5m 7s\n",
      "8100:\tlearn: 0.1039019\ttest: 0.1053645\tbest: 0.1053644 (8098)\ttotal: 3m 57s\tremaining: 8h 5m 25s\n",
      "8400:\tlearn: 0.1036001\ttest: 0.1051823\tbest: 0.1051823 (8400)\ttotal: 4m 6s\tremaining: 8h 5m 13s\n",
      "8700:\tlearn: 0.1033067\ttest: 0.1050003\tbest: 0.1050003 (8700)\ttotal: 4m 15s\tremaining: 8h 4m 57s\n",
      "9000:\tlearn: 0.1029840\ttest: 0.1048291\tbest: 0.1048291 (9000)\ttotal: 4m 24s\tremaining: 8h 4m 38s\n",
      "9300:\tlearn: 0.1026970\ttest: 0.1046549\tbest: 0.1046549 (9300)\ttotal: 4m 32s\tremaining: 8h 4m 21s\n",
      "9600:\tlearn: 0.1024085\ttest: 0.1044873\tbest: 0.1044873 (9600)\ttotal: 4m 41s\tremaining: 8h 3m 58s\n",
      "9900:\tlearn: 0.1021230\ttest: 0.1043295\tbest: 0.1043295 (9900)\ttotal: 4m 50s\tremaining: 8h 3m 43s\n",
      "10200:\tlearn: 0.1018628\ttest: 0.1041992\tbest: 0.1041992 (10200)\ttotal: 4m 59s\tremaining: 8h 3m 36s\n",
      "10500:\tlearn: 0.1016182\ttest: 0.1040676\tbest: 0.1040670 (10498)\ttotal: 5m 7s\tremaining: 8h 3m 32s\n",
      "10800:\tlearn: 0.1013555\ttest: 0.1039321\tbest: 0.1039321 (10800)\ttotal: 5m 16s\tremaining: 8h 3m 17s\n",
      "11100:\tlearn: 0.1011060\ttest: 0.1038098\tbest: 0.1038098 (11100)\ttotal: 5m 25s\tremaining: 8h 3m 3s\n",
      "11400:\tlearn: 0.1008715\ttest: 0.1036992\tbest: 0.1036992 (11400)\ttotal: 5m 34s\tremaining: 8h 2m 46s\n",
      "11700:\tlearn: 0.1006424\ttest: 0.1035875\tbest: 0.1035875 (11700)\ttotal: 5m 42s\tremaining: 8h 2m 35s\n",
      "12000:\tlearn: 0.1004345\ttest: 0.1034880\tbest: 0.1034880 (12000)\ttotal: 5m 51s\tremaining: 8h 2m 14s\n",
      "12300:\tlearn: 0.1002030\ttest: 0.1033974\tbest: 0.1033951 (12281)\ttotal: 6m\tremaining: 8h 1m 55s\n",
      "12600:\tlearn: 0.0999910\ttest: 0.1032970\tbest: 0.1032970 (12600)\ttotal: 6m 8s\tremaining: 8h 1m 42s\n",
      "12900:\tlearn: 0.0997721\ttest: 0.1031860\tbest: 0.1031860 (12900)\ttotal: 6m 17s\tremaining: 8h 1m 37s\n",
      "13200:\tlearn: 0.0995662\ttest: 0.1030833\tbest: 0.1030832 (13199)\ttotal: 6m 26s\tremaining: 8h 1m 23s\n",
      "13500:\tlearn: 0.0993860\ttest: 0.1030045\tbest: 0.1030045 (13500)\ttotal: 6m 35s\tremaining: 8h 1m 8s\n",
      "13800:\tlearn: 0.0992056\ttest: 0.1029229\tbest: 0.1029229 (13800)\ttotal: 6m 43s\tremaining: 8h 54s\n",
      "14100:\tlearn: 0.0990265\ttest: 0.1028352\tbest: 0.1028352 (14100)\ttotal: 6m 52s\tremaining: 8h 39s\n",
      "14400:\tlearn: 0.0988308\ttest: 0.1027495\tbest: 0.1027495 (14400)\ttotal: 7m 1s\tremaining: 8h 30s\n",
      "14700:\tlearn: 0.0986297\ttest: 0.1026715\tbest: 0.1026709 (14699)\ttotal: 7m 9s\tremaining: 8h 19s\n",
      "15000:\tlearn: 0.0984514\ttest: 0.1025791\tbest: 0.1025791 (15000)\ttotal: 7m 18s\tremaining: 8h 11s\n",
      "15300:\tlearn: 0.0982761\ttest: 0.1025071\tbest: 0.1025071 (15300)\ttotal: 7m 27s\tremaining: 7h 59m 56s\n",
      "15600:\tlearn: 0.0981097\ttest: 0.1024282\tbest: 0.1024278 (15596)\ttotal: 7m 36s\tremaining: 7h 59m 44s\n",
      "15900:\tlearn: 0.0979628\ttest: 0.1023737\tbest: 0.1023737 (15900)\ttotal: 7m 44s\tremaining: 7h 59m 27s\n",
      "16200:\tlearn: 0.0978206\ttest: 0.1023149\tbest: 0.1023149 (16200)\ttotal: 7m 53s\tremaining: 7h 59m 20s\n",
      "16500:\tlearn: 0.0976787\ttest: 0.1022597\tbest: 0.1022596 (16497)\ttotal: 8m 2s\tremaining: 7h 59m 3s\n",
      "16800:\tlearn: 0.0975207\ttest: 0.1021929\tbest: 0.1021929 (16789)\ttotal: 8m 10s\tremaining: 7h 58m 51s\n",
      "17100:\tlearn: 0.0973540\ttest: 0.1021316\tbest: 0.1021316 (17100)\ttotal: 8m 19s\tremaining: 7h 58m 42s\n",
      "17400:\tlearn: 0.0971941\ttest: 0.1020677\tbest: 0.1020677 (17389)\ttotal: 8m 28s\tremaining: 7h 58m 38s\n",
      "17700:\tlearn: 0.0970334\ttest: 0.1020047\tbest: 0.1020002 (17697)\ttotal: 8m 37s\tremaining: 7h 58m 26s\n",
      "18000:\tlearn: 0.0968846\ttest: 0.1019411\tbest: 0.1019411 (18000)\ttotal: 8m 45s\tremaining: 7h 58m 12s\n",
      "18300:\tlearn: 0.0967295\ttest: 0.1018753\tbest: 0.1018739 (18276)\ttotal: 8m 54s\tremaining: 7h 58m 1s\n",
      "18600:\tlearn: 0.0965764\ttest: 0.1018298\tbest: 0.1018277 (18573)\ttotal: 9m 3s\tremaining: 7h 57m 46s\n",
      "18900:\tlearn: 0.0964319\ttest: 0.1017822\tbest: 0.1017776 (18874)\ttotal: 9m 11s\tremaining: 7h 57m 30s\n",
      "19200:\tlearn: 0.0962969\ttest: 0.1017297\tbest: 0.1017297 (19200)\ttotal: 9m 20s\tremaining: 7h 57m 17s\n",
      "19500:\tlearn: 0.0961533\ttest: 0.1016725\tbest: 0.1016725 (19500)\ttotal: 9m 29s\tremaining: 7h 57m 2s\n",
      "19800:\tlearn: 0.0960089\ttest: 0.1016229\tbest: 0.1016229 (19797)\ttotal: 9m 37s\tremaining: 7h 56m 48s\n",
      "20100:\tlearn: 0.0958771\ttest: 0.1015848\tbest: 0.1015848 (20100)\ttotal: 9m 46s\tremaining: 7h 56m 35s\n",
      "20400:\tlearn: 0.0957502\ttest: 0.1015478\tbest: 0.1015478 (20400)\ttotal: 9m 55s\tremaining: 7h 56m 20s\n",
      "20700:\tlearn: 0.0956125\ttest: 0.1014978\tbest: 0.1014975 (20694)\ttotal: 10m 4s\tremaining: 7h 56m 13s\n",
      "21000:\tlearn: 0.0954975\ttest: 0.1014656\tbest: 0.1014655 (20997)\ttotal: 10m 12s\tremaining: 7h 56m\n",
      "21300:\tlearn: 0.0953746\ttest: 0.1014459\tbest: 0.1014410 (21225)\ttotal: 10m 21s\tremaining: 7h 55m 51s\n",
      "21600:\tlearn: 0.0952501\ttest: 0.1014171\tbest: 0.1014170 (21591)\ttotal: 10m 30s\tremaining: 7h 55m 39s\n",
      "21900:\tlearn: 0.0951316\ttest: 0.1013786\tbest: 0.1013781 (21898)\ttotal: 10m 38s\tremaining: 7h 55m 32s\n",
      "22200:\tlearn: 0.0950044\ttest: 0.1013328\tbest: 0.1013328 (22200)\ttotal: 10m 47s\tremaining: 7h 55m 22s\n",
      "22500:\tlearn: 0.0948795\ttest: 0.1012925\tbest: 0.1012925 (22500)\ttotal: 10m 56s\tremaining: 7h 55m 12s\n",
      "22800:\tlearn: 0.0947541\ttest: 0.1012464\tbest: 0.1012462 (22798)\ttotal: 11m 5s\tremaining: 7h 55m 1s\n",
      "23100:\tlearn: 0.0946282\ttest: 0.1012083\tbest: 0.1012083 (23100)\ttotal: 11m 13s\tremaining: 7h 54m 48s\n",
      "23400:\tlearn: 0.0945174\ttest: 0.1011809\tbest: 0.1011804 (23392)\ttotal: 11m 22s\tremaining: 7h 54m 36s\n",
      "23700:\tlearn: 0.0944024\ttest: 0.1011540\tbest: 0.1011538 (23690)\ttotal: 11m 30s\tremaining: 7h 54m 22s\n",
      "24000:\tlearn: 0.0942890\ttest: 0.1011278\tbest: 0.1011268 (23958)\ttotal: 11m 39s\tremaining: 7h 54m 8s\n",
      "24300:\tlearn: 0.0941875\ttest: 0.1011106\tbest: 0.1011099 (24297)\ttotal: 11m 48s\tremaining: 7h 53m 57s\n",
      "24600:\tlearn: 0.0940711\ttest: 0.1010722\tbest: 0.1010722 (24600)\ttotal: 11m 56s\tremaining: 7h 53m 43s\n",
      "24900:\tlearn: 0.0939675\ttest: 0.1010392\tbest: 0.1010392 (24900)\ttotal: 12m 5s\tremaining: 7h 53m 29s\n",
      "25200:\tlearn: 0.0938606\ttest: 0.1010180\tbest: 0.1010168 (25173)\ttotal: 12m 14s\tremaining: 7h 53m 17s\n",
      "25500:\tlearn: 0.0937517\ttest: 0.1009865\tbest: 0.1009865 (25500)\ttotal: 12m 22s\tremaining: 7h 53m 4s\n",
      "25800:\tlearn: 0.0936465\ttest: 0.1009505\tbest: 0.1009504 (25798)\ttotal: 12m 31s\tremaining: 7h 52m 48s\n",
      "26100:\tlearn: 0.0935498\ttest: 0.1009169\tbest: 0.1009164 (26099)\ttotal: 12m 40s\tremaining: 7h 52m 37s\n",
      "26400:\tlearn: 0.0934435\ttest: 0.1008992\tbest: 0.1008992 (26400)\ttotal: 12m 48s\tremaining: 7h 52m 25s\n",
      "26700:\tlearn: 0.0933167\ttest: 0.1008691\tbest: 0.1008684 (26697)\ttotal: 12m 57s\tremaining: 7h 52m 14s\n",
      "27000:\tlearn: 0.0932059\ttest: 0.1008505\tbest: 0.1008496 (26985)\ttotal: 13m 6s\tremaining: 7h 52m 4s\n",
      "27300:\tlearn: 0.0930970\ttest: 0.1008297\tbest: 0.1008273 (27272)\ttotal: 13m 14s\tremaining: 7h 51m 54s\n",
      "27600:\tlearn: 0.0930053\ttest: 0.1008102\tbest: 0.1008075 (27574)\ttotal: 13m 23s\tremaining: 7h 51m 44s\n",
      "27900:\tlearn: 0.0929163\ttest: 0.1007863\tbest: 0.1007859 (27878)\ttotal: 13m 32s\tremaining: 7h 51m 34s\n",
      "28200:\tlearn: 0.0928194\ttest: 0.1007621\tbest: 0.1007614 (28173)\ttotal: 13m 40s\tremaining: 7h 51m 21s\n",
      "28500:\tlearn: 0.0927183\ttest: 0.1007432\tbest: 0.1007432 (28500)\ttotal: 13m 49s\tremaining: 7h 51m 9s\n",
      "28800:\tlearn: 0.0926249\ttest: 0.1007254\tbest: 0.1007249 (28773)\ttotal: 13m 58s\tremaining: 7h 50m 58s\n",
      "29100:\tlearn: 0.0925228\ttest: 0.1006989\tbest: 0.1006989 (29100)\ttotal: 14m 6s\tremaining: 7h 50m 48s\n",
      "29400:\tlearn: 0.0924194\ttest: 0.1006775\tbest: 0.1006771 (29365)\ttotal: 14m 15s\tremaining: 7h 50m 35s\n",
      "29700:\tlearn: 0.0923263\ttest: 0.1006565\tbest: 0.1006549 (29696)\ttotal: 14m 24s\tremaining: 7h 50m 29s\n",
      "30000:\tlearn: 0.0922359\ttest: 0.1006375\tbest: 0.1006370 (29978)\ttotal: 14m 33s\tremaining: 7h 50m 35s\n",
      "30300:\tlearn: 0.0921482\ttest: 0.1006099\tbest: 0.1006099 (30299)\ttotal: 14m 42s\tremaining: 7h 50m 48s\n",
      "30600:\tlearn: 0.0920609\ttest: 0.1005874\tbest: 0.1005864 (30571)\ttotal: 14m 51s\tremaining: 7h 50m 37s\n",
      "30900:\tlearn: 0.0919625\ttest: 0.1005609\tbest: 0.1005609 (30900)\ttotal: 14m 59s\tremaining: 7h 50m 23s\n",
      "31200:\tlearn: 0.0918742\ttest: 0.1005473\tbest: 0.1005473 (31200)\ttotal: 15m 8s\tremaining: 7h 50m 11s\n",
      "31500:\tlearn: 0.0917865\ttest: 0.1005273\tbest: 0.1005273 (31500)\ttotal: 15m 17s\tremaining: 7h 49m 57s\n",
      "31800:\tlearn: 0.0916993\ttest: 0.1005080\tbest: 0.1005069 (31786)\ttotal: 15m 25s\tremaining: 7h 49m 45s\n",
      "32100:\tlearn: 0.0916156\ttest: 0.1004861\tbest: 0.1004850 (32085)\ttotal: 15m 34s\tremaining: 7h 49m 34s\n",
      "32400:\tlearn: 0.0915305\ttest: 0.1004585\tbest: 0.1004585 (32400)\ttotal: 15m 43s\tremaining: 7h 49m 22s\n",
      "32700:\tlearn: 0.0914434\ttest: 0.1004304\tbest: 0.1004299 (32676)\ttotal: 15m 51s\tremaining: 7h 49m 7s\n",
      "33000:\tlearn: 0.0913523\ttest: 0.1004226\tbest: 0.1004203 (32981)\ttotal: 16m\tremaining: 7h 48m 55s\n",
      "33300:\tlearn: 0.0912747\ttest: 0.1004088\tbest: 0.1004088 (33300)\ttotal: 16m 8s\tremaining: 7h 48m 40s\n",
      "33600:\tlearn: 0.0911885\ttest: 0.1003803\tbest: 0.1003803 (33600)\ttotal: 16m 17s\tremaining: 7h 48m 27s\n",
      "33900:\tlearn: 0.0911128\ttest: 0.1003599\tbest: 0.1003595 (33897)\ttotal: 16m 25s\tremaining: 7h 48m 14s\n",
      "34200:\tlearn: 0.0910377\ttest: 0.1003550\tbest: 0.1003537 (34185)\ttotal: 16m 34s\tremaining: 7h 48m 2s\n",
      "34500:\tlearn: 0.0909558\ttest: 0.1003463\tbest: 0.1003463 (34500)\ttotal: 16m 43s\tremaining: 7h 47m 50s\n",
      "34800:\tlearn: 0.0908696\ttest: 0.1003251\tbest: 0.1003251 (34800)\ttotal: 16m 51s\tremaining: 7h 47m 38s\n",
      "35100:\tlearn: 0.0907976\ttest: 0.1003132\tbest: 0.1003132 (35100)\ttotal: 17m\tremaining: 7h 47m 24s\n",
      "35400:\tlearn: 0.0907152\ttest: 0.1002941\tbest: 0.1002941 (35400)\ttotal: 17m 8s\tremaining: 7h 47m 10s\n",
      "35700:\tlearn: 0.0906394\ttest: 0.1002732\tbest: 0.1002729 (35689)\ttotal: 17m 17s\tremaining: 7h 46m 57s\n",
      "36000:\tlearn: 0.0905647\ttest: 0.1002583\tbest: 0.1002583 (36000)\ttotal: 17m 25s\tremaining: 7h 46m 44s\n",
      "36300:\tlearn: 0.0904893\ttest: 0.1002507\tbest: 0.1002507 (36300)\ttotal: 17m 34s\tremaining: 7h 46m 34s\n",
      "36600:\tlearn: 0.0904340\ttest: 0.1002383\tbest: 0.1002379 (36590)\ttotal: 17m 43s\tremaining: 7h 46m 23s\n",
      "36900:\tlearn: 0.0903517\ttest: 0.1002172\tbest: 0.1002171 (36899)\ttotal: 17m 51s\tremaining: 7h 46m 12s\n",
      "37200:\tlearn: 0.0902798\ttest: 0.1002004\tbest: 0.1002004 (37200)\ttotal: 18m\tremaining: 7h 46m 2s\n",
      "37500:\tlearn: 0.0902071\ttest: 0.1001858\tbest: 0.1001853 (37455)\ttotal: 18m 9s\tremaining: 7h 45m 54s\n",
      "37800:\tlearn: 0.0901437\ttest: 0.1001760\tbest: 0.1001760 (37800)\ttotal: 18m 17s\tremaining: 7h 45m 42s\n",
      "38100:\tlearn: 0.0900808\ttest: 0.1001596\tbest: 0.1001582 (38098)\ttotal: 18m 26s\tremaining: 7h 45m 30s\n",
      "38400:\tlearn: 0.0900146\ttest: 0.1001528\tbest: 0.1001528 (38400)\ttotal: 18m 34s\tremaining: 7h 45m 17s\n",
      "38700:\tlearn: 0.0899443\ttest: 0.1001396\tbest: 0.1001375 (38537)\ttotal: 18m 43s\tremaining: 7h 45m 6s\n",
      "39000:\tlearn: 0.0898792\ttest: 0.1001253\tbest: 0.1001250 (38996)\ttotal: 18m 52s\tremaining: 7h 44m 55s\n",
      "39300:\tlearn: 0.0897994\ttest: 0.1001133\tbest: 0.1001095 (39261)\ttotal: 19m\tremaining: 7h 44m 44s\n",
      "39600:\tlearn: 0.0897349\ttest: 0.1000885\tbest: 0.1000885 (39597)\ttotal: 19m 9s\tremaining: 7h 44m 31s\n",
      "39900:\tlearn: 0.0896754\ttest: 0.1000765\tbest: 0.1000739 (39796)\ttotal: 19m 17s\tremaining: 7h 44m 18s\n",
      "40200:\tlearn: 0.0896098\ttest: 0.1000573\tbest: 0.1000569 (40196)\ttotal: 19m 26s\tremaining: 7h 44m 7s\n",
      "40500:\tlearn: 0.0895417\ttest: 0.1000472\tbest: 0.1000463 (40498)\ttotal: 19m 34s\tremaining: 7h 43m 56s\n",
      "40800:\tlearn: 0.0894793\ttest: 0.1000335\tbest: 0.1000331 (40771)\ttotal: 19m 43s\tremaining: 7h 43m 44s\n",
      "41100:\tlearn: 0.0894173\ttest: 0.1000280\tbest: 0.1000280 (41100)\ttotal: 19m 52s\tremaining: 7h 43m 34s\n",
      "41400:\tlearn: 0.0893588\ttest: 0.1000166\tbest: 0.1000164 (41394)\ttotal: 20m\tremaining: 7h 43m 22s\n",
      "41700:\tlearn: 0.0892899\ttest: 0.0999998\tbest: 0.0999977 (41671)\ttotal: 20m 9s\tremaining: 7h 43m 9s\n",
      "42000:\tlearn: 0.0892323\ttest: 0.0999928\tbest: 0.0999925 (41987)\ttotal: 20m 17s\tremaining: 7h 42m 58s\n",
      "42300:\tlearn: 0.0891715\ttest: 0.0999896\tbest: 0.0999874 (42288)\ttotal: 20m 26s\tremaining: 7h 42m 47s\n",
      "42600:\tlearn: 0.0891040\ttest: 0.0999758\tbest: 0.0999750 (42598)\ttotal: 20m 35s\tremaining: 7h 42m 38s\n",
      "42900:\tlearn: 0.0890415\ttest: 0.0999670\tbest: 0.0999661 (42888)\ttotal: 20m 43s\tremaining: 7h 42m 30s\n",
      "43200:\tlearn: 0.0889690\ttest: 0.0999585\tbest: 0.0999584 (43199)\ttotal: 20m 52s\tremaining: 7h 42m 20s\n",
      "43500:\tlearn: 0.0889078\ttest: 0.0999453\tbest: 0.0999443 (43498)\ttotal: 21m 1s\tremaining: 7h 42m 12s\n",
      "43800:\tlearn: 0.0888383\ttest: 0.0999346\tbest: 0.0999334 (43751)\ttotal: 21m 10s\tremaining: 7h 42m 5s\n",
      "44100:\tlearn: 0.0887717\ttest: 0.0999174\tbest: 0.0999174 (44100)\ttotal: 21m 19s\tremaining: 7h 42m 2s\n",
      "44400:\tlearn: 0.0886978\ttest: 0.0999094\tbest: 0.0999087 (44376)\ttotal: 21m 27s\tremaining: 7h 41m 58s\n",
      "44700:\tlearn: 0.0886372\ttest: 0.0999031\tbest: 0.0999023 (44685)\ttotal: 21m 36s\tremaining: 7h 41m 50s\n",
      "45000:\tlearn: 0.0885730\ttest: 0.0998883\tbest: 0.0998879 (44985)\ttotal: 21m 45s\tremaining: 7h 41m 45s\n",
      "45300:\tlearn: 0.0885090\ttest: 0.0998794\tbest: 0.0998787 (45279)\ttotal: 21m 54s\tremaining: 7h 41m 42s\n",
      "45600:\tlearn: 0.0884382\ttest: 0.0998694\tbest: 0.0998651 (45507)\ttotal: 22m 3s\tremaining: 7h 41m 43s\n",
      "45900:\tlearn: 0.0883781\ttest: 0.0998642\tbest: 0.0998627 (45834)\ttotal: 22m 12s\tremaining: 7h 41m 41s\n",
      "46200:\tlearn: 0.0883219\ttest: 0.0998562\tbest: 0.0998562 (46199)\ttotal: 22m 21s\tremaining: 7h 41m 36s\n",
      "46500:\tlearn: 0.0882540\ttest: 0.0998475\tbest: 0.0998463 (46476)\ttotal: 22m 30s\tremaining: 7h 41m 39s\n",
      "46800:\tlearn: 0.0881959\ttest: 0.0998394\tbest: 0.0998390 (46798)\ttotal: 22m 39s\tremaining: 7h 41m 26s\n",
      "47100:\tlearn: 0.0881392\ttest: 0.0998344\tbest: 0.0998343 (47098)\ttotal: 22m 48s\tremaining: 7h 41m 15s\n",
      "47400:\tlearn: 0.0880890\ttest: 0.0998334\tbest: 0.0998306 (47257)\ttotal: 22m 56s\tremaining: 7h 41m 5s\n",
      "47700:\tlearn: 0.0880315\ttest: 0.0998222\tbest: 0.0998216 (47695)\ttotal: 23m 5s\tremaining: 7h 40m 52s\n",
      "48000:\tlearn: 0.0879780\ttest: 0.0998151\tbest: 0.0998151 (48000)\ttotal: 23m 13s\tremaining: 7h 40m 38s\n",
      "48300:\tlearn: 0.0879257\ttest: 0.0998103\tbest: 0.0998095 (48244)\ttotal: 23m 22s\tremaining: 7h 40m 27s\n",
      "48600:\tlearn: 0.0878716\ttest: 0.0998057\tbest: 0.0998057 (48600)\ttotal: 23m 30s\tremaining: 7h 40m 18s\n",
      "48900:\tlearn: 0.0878192\ttest: 0.0997967\tbest: 0.0997966 (48886)\ttotal: 23m 39s\tremaining: 7h 40m 6s\n",
      "49200:\tlearn: 0.0877651\ttest: 0.0997896\tbest: 0.0997896 (49200)\ttotal: 23m 48s\tremaining: 7h 39m 56s\n",
      "49500:\tlearn: 0.0877016\ttest: 0.0997885\tbest: 0.0997814 (49316)\ttotal: 23m 56s\tremaining: 7h 39m 44s\n",
      "Stopped by overfitting detector  (300 iterations wait)\n",
      "\n",
      "bestTest = 0.09978139216\n",
      "bestIteration = 49316\n",
      "\n",
      "Shrink model to first 49317 iterations.\n",
      "fold n°10\n",
      "0:\tlearn: 0.9748651\ttest: 0.9841651\tbest: 0.9841651 (0)\ttotal: 32.8ms\tremaining: 9h 6m 34s\n",
      "300:\tlearn: 0.1696163\ttest: 0.1692896\tbest: 0.1692896 (300)\ttotal: 9.04s\tremaining: 8h 20m 18s\n",
      "600:\tlearn: 0.1443461\ttest: 0.1449237\tbest: 0.1449237 (600)\ttotal: 17.6s\tremaining: 8h 8m 20s\n",
      "900:\tlearn: 0.1331484\ttest: 0.1344046\tbest: 0.1344046 (900)\ttotal: 26.4s\tremaining: 8h 8m 31s\n",
      "1200:\tlearn: 0.1271330\ttest: 0.1288292\tbest: 0.1288292 (1200)\ttotal: 35.2s\tremaining: 8h 8m 4s\n",
      "1500:\tlearn: 0.1232253\ttest: 0.1252291\tbest: 0.1252291 (1500)\ttotal: 43.9s\tremaining: 8h 7m 8s\n",
      "1800:\tlearn: 0.1203595\ttest: 0.1226880\tbest: 0.1226880 (1800)\ttotal: 52.7s\tremaining: 8h 6m 24s\n",
      "2100:\tlearn: 0.1181347\ttest: 0.1207594\tbest: 0.1207594 (2100)\ttotal: 1m 1s\tremaining: 8h 6m 57s\n",
      "2400:\tlearn: 0.1163666\ttest: 0.1192262\tbest: 0.1192262 (2400)\ttotal: 1m 10s\tremaining: 8h 6m 37s\n",
      "2700:\tlearn: 0.1148582\ttest: 0.1179032\tbest: 0.1179032 (2700)\ttotal: 1m 18s\tremaining: 8h 5m 45s\n",
      "3000:\tlearn: 0.1136217\ttest: 0.1168966\tbest: 0.1168966 (3000)\ttotal: 1m 27s\tremaining: 8h 5m 48s\n",
      "3300:\tlearn: 0.1124811\ttest: 0.1159674\tbest: 0.1159674 (3300)\ttotal: 1m 36s\tremaining: 8h 6m 4s\n",
      "3600:\tlearn: 0.1115111\ttest: 0.1152111\tbest: 0.1152111 (3600)\ttotal: 1m 45s\tremaining: 8h 6m 9s\n",
      "3900:\tlearn: 0.1106173\ttest: 0.1145398\tbest: 0.1145398 (3900)\ttotal: 1m 54s\tremaining: 8h 6m 17s\n",
      "4200:\tlearn: 0.1098648\ttest: 0.1139361\tbest: 0.1139361 (4200)\ttotal: 2m 3s\tremaining: 8h 6m 15s\n",
      "4500:\tlearn: 0.1090570\ttest: 0.1133680\tbest: 0.1133680 (4500)\ttotal: 2m 11s\tremaining: 8h 6m 9s\n",
      "4800:\tlearn: 0.1083531\ttest: 0.1128596\tbest: 0.1128596 (4800)\ttotal: 2m 20s\tremaining: 8h 6m 2s\n",
      "5100:\tlearn: 0.1077410\ttest: 0.1124241\tbest: 0.1124241 (5100)\ttotal: 2m 29s\tremaining: 8h 5m 41s\n",
      "5400:\tlearn: 0.1071517\ttest: 0.1120194\tbest: 0.1120193 (5399)\ttotal: 2m 38s\tremaining: 8h 5m 39s\n",
      "5700:\tlearn: 0.1065892\ttest: 0.1116222\tbest: 0.1116222 (5700)\ttotal: 2m 47s\tremaining: 8h 5m 28s\n",
      "6000:\tlearn: 0.1060904\ttest: 0.1112944\tbest: 0.1112944 (6000)\ttotal: 2m 55s\tremaining: 8h 5m 6s\n",
      "6300:\tlearn: 0.1056284\ttest: 0.1109825\tbest: 0.1109825 (6300)\ttotal: 3m 4s\tremaining: 8h 4m 56s\n",
      "6600:\tlearn: 0.1052169\ttest: 0.1107287\tbest: 0.1107287 (6600)\ttotal: 3m 13s\tremaining: 8h 4m 48s\n",
      "6900:\tlearn: 0.1047804\ttest: 0.1104767\tbest: 0.1104767 (6900)\ttotal: 3m 22s\tremaining: 8h 4m 37s\n",
      "7200:\tlearn: 0.1043829\ttest: 0.1102410\tbest: 0.1102410 (7200)\ttotal: 3m 30s\tremaining: 8h 4m 37s\n",
      "7500:\tlearn: 0.1039882\ttest: 0.1100242\tbest: 0.1100242 (7500)\ttotal: 3m 39s\tremaining: 8h 4m 29s\n",
      "7800:\tlearn: 0.1036275\ttest: 0.1098085\tbest: 0.1098085 (7800)\ttotal: 3m 48s\tremaining: 8h 4m 11s\n",
      "8100:\tlearn: 0.1033034\ttest: 0.1096251\tbest: 0.1096246 (8099)\ttotal: 3m 57s\tremaining: 8h 4m 11s\n",
      "8400:\tlearn: 0.1029720\ttest: 0.1094294\tbest: 0.1094294 (8400)\ttotal: 4m 6s\tremaining: 8h 4m 9s\n",
      "8700:\tlearn: 0.1026568\ttest: 0.1092455\tbest: 0.1092455 (8700)\ttotal: 4m 14s\tremaining: 8h 4m 8s\n",
      "9000:\tlearn: 0.1022968\ttest: 0.1090527\tbest: 0.1090527 (9000)\ttotal: 4m 23s\tremaining: 8h 4m 8s\n",
      "9300:\tlearn: 0.1019976\ttest: 0.1089109\tbest: 0.1089106 (9299)\ttotal: 4m 32s\tremaining: 8h 4m\n",
      "9600:\tlearn: 0.1017167\ttest: 0.1087605\tbest: 0.1087605 (9600)\ttotal: 4m 41s\tremaining: 8h 3m 45s\n",
      "9900:\tlearn: 0.1014297\ttest: 0.1086162\tbest: 0.1086162 (9899)\ttotal: 4m 50s\tremaining: 8h 3m 35s\n",
      "10200:\tlearn: 0.1011568\ttest: 0.1084760\tbest: 0.1084760 (10200)\ttotal: 4m 58s\tremaining: 8h 3m 17s\n",
      "10500:\tlearn: 0.1008839\ttest: 0.1083611\tbest: 0.1083608 (10497)\ttotal: 5m 7s\tremaining: 8h 3m 26s\n",
      "10800:\tlearn: 0.1006161\ttest: 0.1082450\tbest: 0.1082450 (10800)\ttotal: 5m 16s\tremaining: 8h 3m 37s\n",
      "11100:\tlearn: 0.1003812\ttest: 0.1081326\tbest: 0.1081326 (11100)\ttotal: 5m 25s\tremaining: 8h 3m 41s\n",
      "11400:\tlearn: 0.1001543\ttest: 0.1080356\tbest: 0.1080356 (11400)\ttotal: 5m 34s\tremaining: 8h 3m 31s\n",
      "11700:\tlearn: 0.0999416\ttest: 0.1079318\tbest: 0.1079318 (11700)\ttotal: 5m 43s\tremaining: 8h 3m 26s\n",
      "12000:\tlearn: 0.0997321\ttest: 0.1078460\tbest: 0.1078456 (11999)\ttotal: 5m 52s\tremaining: 8h 3m 7s\n",
      "12300:\tlearn: 0.0995277\ttest: 0.1077544\tbest: 0.1077543 (12299)\ttotal: 6m\tremaining: 8h 2m 45s\n",
      "12600:\tlearn: 0.0992963\ttest: 0.1076570\tbest: 0.1076560 (12599)\ttotal: 6m 9s\tremaining: 8h 2m 33s\n",
      "12900:\tlearn: 0.0990944\ttest: 0.1075662\tbest: 0.1075655 (12899)\ttotal: 6m 18s\tremaining: 8h 2m 16s\n",
      "13200:\tlearn: 0.0989066\ttest: 0.1074978\tbest: 0.1074978 (13200)\ttotal: 6m 27s\tremaining: 8h 2m 9s\n",
      "13500:\tlearn: 0.0987092\ttest: 0.1074129\tbest: 0.1074125 (13498)\ttotal: 6m 35s\tremaining: 8h 2m 2s\n",
      "13800:\tlearn: 0.0985227\ttest: 0.1073313\tbest: 0.1073313 (13800)\ttotal: 6m 44s\tremaining: 8h 1m 46s\n",
      "14100:\tlearn: 0.0983332\ttest: 0.1072677\tbest: 0.1072677 (14095)\ttotal: 6m 53s\tremaining: 8h 1m 26s\n",
      "14400:\tlearn: 0.0981486\ttest: 0.1071871\tbest: 0.1071871 (14400)\ttotal: 7m 1s\tremaining: 8h 1m 19s\n",
      "14700:\tlearn: 0.0979792\ttest: 0.1071195\tbest: 0.1071180 (14695)\ttotal: 7m 10s\tremaining: 8h 1m 9s\n",
      "15000:\tlearn: 0.0977900\ttest: 0.1070507\tbest: 0.1070507 (15000)\ttotal: 7m 19s\tremaining: 8h 1m 2s\n",
      "15300:\tlearn: 0.0976131\ttest: 0.1069868\tbest: 0.1069868 (15300)\ttotal: 7m 28s\tremaining: 8h 51s\n",
      "15600:\tlearn: 0.0974458\ttest: 0.1069157\tbest: 0.1069155 (15599)\ttotal: 7m 37s\tremaining: 8h 39s\n",
      "15900:\tlearn: 0.0972812\ttest: 0.1068643\tbest: 0.1068643 (15900)\ttotal: 7m 45s\tremaining: 8h 24s\n",
      "16200:\tlearn: 0.0971295\ttest: 0.1068048\tbest: 0.1068047 (16199)\ttotal: 7m 54s\tremaining: 8h 16s\n",
      "16500:\tlearn: 0.0969577\ttest: 0.1067568\tbest: 0.1067568 (16500)\ttotal: 8m 3s\tremaining: 8h 2s\n",
      "16800:\tlearn: 0.0967989\ttest: 0.1066996\tbest: 0.1066996 (16800)\ttotal: 8m 11s\tremaining: 7h 59m 47s\n",
      "17100:\tlearn: 0.0966368\ttest: 0.1066524\tbest: 0.1066524 (17100)\ttotal: 8m 20s\tremaining: 7h 59m 34s\n",
      "17400:\tlearn: 0.0964745\ttest: 0.1065895\tbest: 0.1065895 (17400)\ttotal: 8m 29s\tremaining: 7h 59m 17s\n",
      "17700:\tlearn: 0.0963286\ttest: 0.1065335\tbest: 0.1065335 (17700)\ttotal: 8m 37s\tremaining: 7h 59m 5s\n",
      "18000:\tlearn: 0.0961963\ttest: 0.1064886\tbest: 0.1064882 (17993)\ttotal: 8m 46s\tremaining: 7h 58m 54s\n",
      "18300:\tlearn: 0.0960502\ttest: 0.1064338\tbest: 0.1064338 (18300)\ttotal: 8m 55s\tremaining: 7h 58m 38s\n",
      "18600:\tlearn: 0.0959226\ttest: 0.1063803\tbest: 0.1063803 (18600)\ttotal: 9m 4s\tremaining: 7h 58m 29s\n",
      "18900:\tlearn: 0.0957897\ttest: 0.1063359\tbest: 0.1063353 (18899)\ttotal: 9m 12s\tremaining: 7h 58m 18s\n",
      "19200:\tlearn: 0.0956585\ttest: 0.1062928\tbest: 0.1062926 (19198)\ttotal: 9m 21s\tremaining: 7h 58m 16s\n",
      "19500:\tlearn: 0.0955042\ttest: 0.1062484\tbest: 0.1062451 (19482)\ttotal: 9m 30s\tremaining: 7h 58m 8s\n",
      "19800:\tlearn: 0.0953743\ttest: 0.1062078\tbest: 0.1062078 (19800)\ttotal: 9m 39s\tremaining: 7h 57m 59s\n",
      "20100:\tlearn: 0.0952472\ttest: 0.1061644\tbest: 0.1061644 (20100)\ttotal: 9m 48s\tremaining: 7h 57m 55s\n",
      "20400:\tlearn: 0.0951148\ttest: 0.1061247\tbest: 0.1061247 (20400)\ttotal: 9m 56s\tremaining: 7h 57m 45s\n",
      "20700:\tlearn: 0.0949987\ttest: 0.1060969\tbest: 0.1060951 (20673)\ttotal: 10m 5s\tremaining: 7h 57m 34s\n",
      "21000:\tlearn: 0.0948833\ttest: 0.1060745\tbest: 0.1060720 (20982)\ttotal: 10m 14s\tremaining: 7h 57m 20s\n",
      "21300:\tlearn: 0.0947604\ttest: 0.1060443\tbest: 0.1060443 (21300)\ttotal: 10m 23s\tremaining: 7h 57m 13s\n",
      "21600:\tlearn: 0.0946392\ttest: 0.1060145\tbest: 0.1060145 (21600)\ttotal: 10m 32s\tremaining: 7h 57m 11s\n",
      "21900:\tlearn: 0.0945248\ttest: 0.1059854\tbest: 0.1059851 (21899)\ttotal: 10m 40s\tremaining: 7h 56m 56s\n",
      "22200:\tlearn: 0.0943961\ttest: 0.1059337\tbest: 0.1059337 (22199)\ttotal: 10m 49s\tremaining: 7h 56m 41s\n",
      "22500:\tlearn: 0.0942926\ttest: 0.1059092\tbest: 0.1059092 (22491)\ttotal: 10m 58s\tremaining: 7h 56m 30s\n",
      "22800:\tlearn: 0.0941746\ttest: 0.1058826\tbest: 0.1058826 (22800)\ttotal: 11m 6s\tremaining: 7h 56m 16s\n",
      "23100:\tlearn: 0.0940758\ttest: 0.1058629\tbest: 0.1058585 (23065)\ttotal: 11m 15s\tremaining: 7h 56m 2s\n",
      "23400:\tlearn: 0.0939593\ttest: 0.1058295\tbest: 0.1058282 (23387)\ttotal: 11m 23s\tremaining: 7h 55m 44s\n",
      "23700:\tlearn: 0.0938454\ttest: 0.1057932\tbest: 0.1057920 (23667)\ttotal: 11m 32s\tremaining: 7h 55m 30s\n",
      "24000:\tlearn: 0.0937379\ttest: 0.1057661\tbest: 0.1057656 (23991)\ttotal: 11m 41s\tremaining: 7h 55m 18s\n",
      "24300:\tlearn: 0.0936346\ttest: 0.1057411\tbest: 0.1057411 (24300)\ttotal: 11m 49s\tremaining: 7h 55m 5s\n",
      "24600:\tlearn: 0.0935223\ttest: 0.1057046\tbest: 0.1057046 (24600)\ttotal: 11m 58s\tremaining: 7h 54m 53s\n",
      "24900:\tlearn: 0.0934158\ttest: 0.1056937\tbest: 0.1056924 (24865)\ttotal: 12m 7s\tremaining: 7h 54m 42s\n",
      "25200:\tlearn: 0.0933163\ttest: 0.1056686\tbest: 0.1056686 (25199)\ttotal: 12m 16s\tremaining: 7h 54m 30s\n",
      "25500:\tlearn: 0.0932153\ttest: 0.1056454\tbest: 0.1056448 (25499)\ttotal: 12m 24s\tremaining: 7h 54m 16s\n",
      "25800:\tlearn: 0.0931187\ttest: 0.1056240\tbest: 0.1056240 (25800)\ttotal: 12m 33s\tremaining: 7h 54m 7s\n",
      "26100:\tlearn: 0.0930275\ttest: 0.1056005\tbest: 0.1055993 (26075)\ttotal: 12m 42s\tremaining: 7h 53m 53s\n",
      "26400:\tlearn: 0.0929312\ttest: 0.1055734\tbest: 0.1055732 (26399)\ttotal: 12m 50s\tremaining: 7h 53m 41s\n",
      "26700:\tlearn: 0.0928295\ttest: 0.1055426\tbest: 0.1055402 (26676)\ttotal: 12m 59s\tremaining: 7h 53m 31s\n",
      "27000:\tlearn: 0.0927301\ttest: 0.1055170\tbest: 0.1055163 (26995)\ttotal: 13m 8s\tremaining: 7h 53m 18s\n",
      "27300:\tlearn: 0.0926317\ttest: 0.1054878\tbest: 0.1054868 (27291)\ttotal: 13m 16s\tremaining: 7h 53m 1s\n",
      "27600:\tlearn: 0.0925348\ttest: 0.1054756\tbest: 0.1054756 (27600)\ttotal: 13m 25s\tremaining: 7h 52m 50s\n",
      "27900:\tlearn: 0.0924393\ttest: 0.1054494\tbest: 0.1054486 (27897)\ttotal: 13m 33s\tremaining: 7h 52m 37s\n",
      "28200:\tlearn: 0.0923526\ttest: 0.1054257\tbest: 0.1054247 (28182)\ttotal: 13m 42s\tremaining: 7h 52m 25s\n",
      "28500:\tlearn: 0.0922544\ttest: 0.1054059\tbest: 0.1054055 (28495)\ttotal: 13m 51s\tremaining: 7h 52m 17s\n",
      "28800:\tlearn: 0.0921469\ttest: 0.1053939\tbest: 0.1053939 (28800)\ttotal: 14m\tremaining: 7h 52m 14s\n",
      "29100:\tlearn: 0.0920523\ttest: 0.1053657\tbest: 0.1053656 (29097)\ttotal: 14m 8s\tremaining: 7h 52m 3s\n",
      "29400:\tlearn: 0.0919504\ttest: 0.1053447\tbest: 0.1053442 (29380)\ttotal: 14m 17s\tremaining: 7h 51m 47s\n",
      "29700:\tlearn: 0.0918514\ttest: 0.1053171\tbest: 0.1053159 (29665)\ttotal: 14m 26s\tremaining: 7h 51m 35s\n",
      "30000:\tlearn: 0.0917564\ttest: 0.1052944\tbest: 0.1052927 (29982)\ttotal: 14m 34s\tremaining: 7h 51m 21s\n",
      "30300:\tlearn: 0.0916635\ttest: 0.1052918\tbest: 0.1052874 (30114)\ttotal: 14m 43s\tremaining: 7h 51m 7s\n",
      "30600:\tlearn: 0.0915812\ttest: 0.1052732\tbest: 0.1052730 (30590)\ttotal: 14m 51s\tremaining: 7h 50m 55s\n",
      "30900:\tlearn: 0.0914934\ttest: 0.1052551\tbest: 0.1052551 (30899)\ttotal: 15m\tremaining: 7h 50m 43s\n",
      "31200:\tlearn: 0.0914118\ttest: 0.1052451\tbest: 0.1052438 (31134)\ttotal: 15m 9s\tremaining: 7h 50m 27s\n",
      "31500:\tlearn: 0.0913271\ttest: 0.1052195\tbest: 0.1052189 (31498)\ttotal: 15m 17s\tremaining: 7h 50m 15s\n",
      "31800:\tlearn: 0.0912411\ttest: 0.1052084\tbest: 0.1052082 (31799)\ttotal: 15m 26s\tremaining: 7h 50m 4s\n",
      "32100:\tlearn: 0.0911576\ttest: 0.1051939\tbest: 0.1051936 (32036)\ttotal: 15m 35s\tremaining: 7h 49m 53s\n",
      "32400:\tlearn: 0.0910706\ttest: 0.1051758\tbest: 0.1051756 (32399)\ttotal: 15m 43s\tremaining: 7h 49m 39s\n",
      "32700:\tlearn: 0.0909854\ttest: 0.1051682\tbest: 0.1051674 (32692)\ttotal: 15m 52s\tremaining: 7h 49m 27s\n",
      "33000:\tlearn: 0.0908940\ttest: 0.1051585\tbest: 0.1051585 (33000)\ttotal: 16m\tremaining: 7h 49m 17s\n",
      "33300:\tlearn: 0.0908005\ttest: 0.1051413\tbest: 0.1051412 (33299)\ttotal: 16m 9s\tremaining: 7h 49m 7s\n",
      "33600:\tlearn: 0.0907237\ttest: 0.1051345\tbest: 0.1051311 (33535)\ttotal: 16m 18s\tremaining: 7h 48m 55s\n",
      "33900:\tlearn: 0.0906403\ttest: 0.1051239\tbest: 0.1051213 (33828)\ttotal: 16m 26s\tremaining: 7h 48m 41s\n",
      "34200:\tlearn: 0.0905573\ttest: 0.1051082\tbest: 0.1051070 (34122)\ttotal: 16m 35s\tremaining: 7h 48m 27s\n",
      "34500:\tlearn: 0.0904832\ttest: 0.1050871\tbest: 0.1050865 (34471)\ttotal: 16m 44s\tremaining: 7h 48m 16s\n",
      "34800:\tlearn: 0.0904073\ttest: 0.1050840\tbest: 0.1050840 (34800)\ttotal: 16m 52s\tremaining: 7h 48m 4s\n",
      "35100:\tlearn: 0.0903214\ttest: 0.1050795\tbest: 0.1050783 (35064)\ttotal: 17m 1s\tremaining: 7h 47m 52s\n",
      "35400:\tlearn: 0.0902352\ttest: 0.1050601\tbest: 0.1050600 (35397)\ttotal: 17m 9s\tremaining: 7h 47m 42s\n",
      "35700:\tlearn: 0.0901609\ttest: 0.1050459\tbest: 0.1050459 (35698)\ttotal: 17m 18s\tremaining: 7h 47m 29s\n",
      "36000:\tlearn: 0.0900863\ttest: 0.1050331\tbest: 0.1050310 (35971)\ttotal: 17m 27s\tremaining: 7h 47m 17s\n",
      "36300:\tlearn: 0.0900155\ttest: 0.1050185\tbest: 0.1050185 (36300)\ttotal: 17m 35s\tremaining: 7h 47m 6s\n",
      "36600:\tlearn: 0.0899507\ttest: 0.1050031\tbest: 0.1050018 (36572)\ttotal: 17m 44s\tremaining: 7h 46m 53s\n",
      "36900:\tlearn: 0.0898749\ttest: 0.1049851\tbest: 0.1049846 (36896)\ttotal: 17m 52s\tremaining: 7h 46m 41s\n",
      "37200:\tlearn: 0.0898013\ttest: 0.1049797\tbest: 0.1049792 (37183)\ttotal: 18m 1s\tremaining: 7h 46m 30s\n",
      "37500:\tlearn: 0.0897279\ttest: 0.1049714\tbest: 0.1049696 (37466)\ttotal: 18m 10s\tremaining: 7h 46m 20s\n",
      "37800:\tlearn: 0.0896588\ttest: 0.1049620\tbest: 0.1049599 (37791)\ttotal: 18m 18s\tremaining: 7h 46m 10s\n",
      "38100:\tlearn: 0.0895794\ttest: 0.1049555\tbest: 0.1049532 (38061)\ttotal: 18m 27s\tremaining: 7h 46m\n",
      "38400:\tlearn: 0.0895074\ttest: 0.1049485\tbest: 0.1049479 (38386)\ttotal: 18m 36s\tremaining: 7h 45m 48s\n",
      "38700:\tlearn: 0.0894287\ttest: 0.1049375\tbest: 0.1049362 (38632)\ttotal: 18m 44s\tremaining: 7h 45m 33s\n",
      "39000:\tlearn: 0.0893490\ttest: 0.1049243\tbest: 0.1049226 (38985)\ttotal: 18m 53s\tremaining: 7h 45m 22s\n",
      "39300:\tlearn: 0.0892792\ttest: 0.1049141\tbest: 0.1049128 (39194)\ttotal: 19m 1s\tremaining: 7h 45m 12s\n",
      "39600:\tlearn: 0.0892090\ttest: 0.1048907\tbest: 0.1048907 (39600)\ttotal: 19m 10s\tremaining: 7h 45m 1s\n",
      "39900:\tlearn: 0.0891389\ttest: 0.1048757\tbest: 0.1048743 (39850)\ttotal: 19m 19s\tremaining: 7h 44m 49s\n",
      "40200:\tlearn: 0.0890805\ttest: 0.1048695\tbest: 0.1048695 (40200)\ttotal: 19m 27s\tremaining: 7h 44m 37s\n",
      "40500:\tlearn: 0.0890150\ttest: 0.1048644\tbest: 0.1048629 (40486)\ttotal: 19m 36s\tremaining: 7h 44m 23s\n",
      "40800:\tlearn: 0.0889495\ttest: 0.1048623\tbest: 0.1048619 (40788)\ttotal: 19m 44s\tremaining: 7h 44m 11s\n",
      "41100:\tlearn: 0.0888778\ttest: 0.1048705\tbest: 0.1048610 (40976)\ttotal: 19m 53s\tremaining: 7h 44m 4s\n",
      "Stopped by overfitting detector  (300 iterations wait)\n",
      "\n",
      "bestTest = 0.1048610226\n",
      "bestIteration = 40976\n",
      "\n",
      "Shrink model to first 40977 iterations.\n",
      "catboost score: 435.52878790\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "catboost\n",
    "\"\"\"\n",
    "\n",
    "kfolder = KFold(n_splits=2, shuffle=True)\n",
    "oof_cb = np.zeros(len(X_data))\n",
    "predictions_cb = np.zeros(len(X_test))\n",
    "predictions_train_cb = np.zeros(len(X_data))\n",
    "kfold = kfolder.split(X_data, Y_data)\n",
    "fold_ = 0\n",
    "for train_index, vali_index in kfold:\n",
    "    fold_ = fold_ + 1\n",
    "    print(\"fold n°{}\".format(fold_))\n",
    "    k_x_train = X_data[train_index]\n",
    "    k_y_train = Y_data[train_index]\n",
    "    k_x_vali = X_data[vali_index]\n",
    "    k_y_vali = Y_data[vali_index]\n",
    "    cb_params = {\n",
    "        \"n_estimators\": 1000000,          # 最多训练的迭代次数（树的数量），非常大，通常配合 early_stopping 使用\n",
    "        \"loss_function\": \"MAE\",           # 损失函数使用 MAE（Mean Absolute Error）—— 绝对值误差，更鲁棒于离群值\n",
    "        \"eval_metric\": \"MAE\",             # 验证时的评估指标也是 MAE（与 loss_function 一致）\n",
    "        \"learning_rate\": 0.02,            # 学习率，小学习率配合大 n_estimators，训练更稳定\n",
    "        \"depth\": 6,                       # 每棵树的最大深度，控制模型复杂度（一般 6~10）\n",
    "        \"use_best_model\": True,           # 使用验证集找到最佳模型（用于 early stopping）\n",
    "        \"subsample\": 0.6,                 # 每次训练使用 60% 的样本，防止过拟合\n",
    "        \"bootstrap_type\": \"Bernoulli\",   # 使用 Bernoulli 采样方法来做子样本（和 subsample 一起使用）\n",
    "        \"reg_lambda\": 3,                  # L2 正则化系数，防止过拟合\n",
    "        \"one_hot_max_size\": 2,           # 如果类别变量的唯一值数量 ≤ 2，则使用 One-Hot 编码\n",
    "    }\n",
    "    model_cb = CatBoostRegressor(**cb_params)\n",
    "    # train the model\n",
    "    model_cb.fit(\n",
    "        k_x_train,\n",
    "        k_y_train,\n",
    "        eval_set=[(k_x_vali, k_y_vali)],\n",
    "        verbose=300,\n",
    "        early_stopping_rounds=300,\n",
    "    )\n",
    "    oof_cb[vali_index] = model_cb.predict(k_x_vali, ntree_end=model_cb.best_iteration_)\n",
    "    predictions_cb += (\n",
    "        model_cb.predict(X_test, ntree_end=model_cb.best_iteration_) / kfolder.n_splits\n",
    "    )\n",
    "    predictions_train_cb += (\n",
    "        model_cb.predict(X_data, ntree_end=model_cb.best_iteration_) / kfolder.n_splits\n",
    "    )\n",
    "\n",
    "print(\n",
    "    \"catboost score: {:<8.8f}\".format(\n",
    "        mean_absolute_error(np.expm1(oof_cb), np.expm1(Y_data))\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = os.path.join(path, \"user_data\")\n",
    "# 测试集输出\n",
    "predictions = predictions_cb\n",
    "predictions[predictions < 0] = 0\n",
    "sub = pd.DataFrame()\n",
    "sub[\"SaleID\"] = TestA_data.SaleID\n",
    "sub[\"price\"] = predictions\n",
    "sub.to_csv(os.path.join(output_path, \"test_cab.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 验证集输出\n",
    "oof_cb[oof_cb < 0] = 0\n",
    "sub = pd.DataFrame()\n",
    "sub[\"SaleID\"] = Train_data.SaleID  # Train_data.SaleID的长度是149999，与oof_cb一致\n",
    "sub[\"price\"] = oof_cb\n",
    "sub.to_csv(os.path.join(output_path, \"train_cab.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "神经网络\n",
    "\"\"\"\n",
    "\n",
    "## 读取神经网络模型数据\n",
    "path = os.path.abspath(os.path.dirname(os.getcwd()) + os.path.sep + \".\")\n",
    "tree_data_path = os.path.join(path, \"code\", \"user_data\")\n",
    "Train_NN_data = pd.read_csv(os.path.join(tree_data_path, \"train_nn.csv\"), sep=\" \")\n",
    "Test_NN_data = pd.read_csv(os.path.join(tree_data_path, \"test_nn.csv\"), sep=\" \")\n",
    "\n",
    "numerical_cols = Train_NN_data.columns\n",
    "print(numerical_cols)\n",
    "feature_cols = [col for col in numerical_cols if col not in [\"price\", \"SaleID\"]]\n",
    "## 提前特征列，标签列构造训练样本和测试样本\n",
    "X_data = Train_NN_data[feature_cols]\n",
    "X_test = Test_NN_data[feature_cols]\n",
    "print(X_data.shape)\n",
    "print(X_test.shape)\n",
    "\n",
    "x = np.array(X_data)\n",
    "y = np.array(Train_NN_data[\"price\"])\n",
    "x_test = np.array(X_test)\n",
    "\n",
    "print(x)\n",
    "print(x_test)\n",
    "print(y)\n",
    "\n",
    "\n",
    "# 调整训练过程的学习率\n",
    "# 调整训练过程的学习率\n",
    "def scheduler(epoch):\n",
    "    # 到规定的epoch，学习率减小为原来的1/10\n",
    "    if epoch == 1400:\n",
    "        # 使用适用于TensorFlow 2.x的方式更新学习率\n",
    "        lr = float(model.optimizer.learning_rate.numpy())\n",
    "        model.optimizer.learning_rate.assign(lr * 0.1)\n",
    "        print(\"lr changed to {}\".format(lr * 0.1))\n",
    "    if epoch == 1700:\n",
    "        lr = float(model.optimizer.learning_rate.numpy())\n",
    "        model.optimizer.learning_rate.assign(lr * 0.1)\n",
    "        print(\"lr changed to {}\".format(lr * 0.1))\n",
    "    if epoch == 1900:\n",
    "        lr = float(model.optimizer.learning_rate.numpy())\n",
    "        model.optimizer.learning_rate.assign(lr * 0.1)\n",
    "        print(\"lr changed to {}\".format(lr * 0.1))\n",
    "    return float(model.optimizer.learning_rate.numpy())\n",
    "\n",
    "\n",
    "reduce_lr = LearningRateScheduler(scheduler)\n",
    "\n",
    "\n",
    "kfolder = KFold(n_splits=2, shuffle=True)\n",
    "oof_nn = np.zeros(len(x))\n",
    "predictions_nn = np.zeros(len(x_test))\n",
    "predictions_train_nn = np.zeros(len(x))\n",
    "kfold = kfolder.split(x, y)\n",
    "fold_ = 0\n",
    "for train_index, vali_index in kfold:\n",
    "    k_x_train = x[train_index]\n",
    "    k_y_train = y[train_index]\n",
    "    k_x_vali = x[vali_index]\n",
    "    k_y_vali = y[vali_index]\n",
    "\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(\n",
    "        tf.keras.layers.Dense(\n",
    "            512, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(0.02)\n",
    "        )\n",
    "    )\n",
    "    model.add(\n",
    "        tf.keras.layers.Dense(\n",
    "            256, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(0.02)\n",
    "        )\n",
    "    )\n",
    "    model.add(\n",
    "        tf.keras.layers.Dense(\n",
    "            128, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(0.02)\n",
    "        )\n",
    "    )\n",
    "    model.add(\n",
    "        tf.keras.layers.Dense(\n",
    "            64, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(0.02)\n",
    "        )\n",
    "    )\n",
    "    model.add(\n",
    "        tf.keras.layers.Dense(1, kernel_regularizer=tf.keras.regularizers.l2(0.02))\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        loss=\"mean_absolute_error\",\n",
    "        optimizer=tf.keras.optimizers.Adam(),\n",
    "        metrics=[\"mae\"],\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        k_x_train,\n",
    "        k_y_train,\n",
    "        batch_size=512,\n",
    "        epochs=2000,\n",
    "        validation_data=(k_x_vali, k_y_vali),\n",
    "        callbacks=[reduce_lr],\n",
    "    )  # callbacks=callbacks,\n",
    "    oof_nn[vali_index] = model.predict(k_x_vali).reshape(\n",
    "        (model.predict(k_x_vali).shape[0],)\n",
    "    )\n",
    "    predictions_nn += (\n",
    "        model.predict(x_test).reshape((model.predict(x_test).shape[0],))\n",
    "        / kfolder.n_splits\n",
    "    )\n",
    "    predictions_train_nn += (\n",
    "        model.predict(x).reshape((model.predict(x).shape[0],)) / kfolder.n_splits\n",
    "    )\n",
    "\n",
    "print(\"NN score: {:<8.8f}\".format(mean_absolute_error(oof_nn, y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = os.path.join(path, \"code\", \"user_data\")\n",
    "# 测试集输出\n",
    "predictions = predictions_nn\n",
    "predictions[predictions < 0] = 0\n",
    "sub = pd.DataFrame()\n",
    "sub[\"SaleID\"] = Test_NN_data.SaleID\n",
    "sub[\"price\"] = predictions\n",
    "sub.to_csv(os.path.join(output_path, \"test_nn.csv\"), index=False)\n",
    "\n",
    "# 验证集输出\n",
    "oof_nn[oof_nn < 0] = 0\n",
    "sub = pd.DataFrame()\n",
    "sub[\"SaleID\"] = Train_NN_data.SaleID\n",
    "sub[\"price\"] = oof_nn\n",
    "sub.to_csv(os.path.join(output_path, \"train_nn.csv\"), index=False)\n",
    "\n",
    "\n",
    "tree_data_path = os.path.join(path, \"code\", \"user_data\")\n",
    "\n",
    "# 导入树模型lgb预测数据\n",
    "predictions_lgb = np.array(\n",
    "    pd.read_csv(os.path.join(tree_data_path, \"test_lgb.csv\"))[\"price\"]\n",
    ")\n",
    "oof_lgb = np.array(pd.read_csv(os.path.join(tree_data_path, \"train_lgb.csv\"))[\"price\"])\n",
    "\n",
    "# 导入树模型cab预测数据\n",
    "predictions_cb = np.array(\n",
    "    pd.read_csv(os.path.join(tree_data_path, \"test_cab.csv\"))[\"price\"]\n",
    ")\n",
    "oof_cb = np.array(pd.read_csv(os.path.join(tree_data_path, \"train_cab.csv\"))[\"price\"])\n",
    "\n",
    "# 读取price，对验证集进行评估\n",
    "Train_data = pd.read_csv(os.path.join(tree_data_path, \"train_tree.csv\"), sep=\" \")\n",
    "TestA_data = pd.read_csv(os.path.join(tree_data_path, \"test_tree.csv\"), sep=\" \")\n",
    "Y_data = Train_data[\"price\"]\n",
    "\n",
    "train_stack = np.vstack([oof_lgb, oof_cb]).transpose()\n",
    "test_stack = np.vstack([predictions_lgb, predictions_cb]).transpose()\n",
    "folds_stack = RepeatedKFold(n_splits=2, n_repeats=2)\n",
    "tree_stack = np.zeros(train_stack.shape[0])\n",
    "predictions = np.zeros(test_stack.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0\n",
      "fold 1\n",
      "fold 2\n",
      "fold 3\n",
      "树模型：二层贝叶斯: 433.24430404\n"
     ]
    }
   ],
   "source": [
    "# 二层贝叶斯回归stack\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds_stack.split(train_stack, Y_data)):\n",
    "    print(\"fold {}\".format(fold_))\n",
    "    trn_data, trn_y = train_stack[trn_idx], Y_data[trn_idx]\n",
    "    val_data, val_y = train_stack[val_idx], Y_data[val_idx]\n",
    "\n",
    "    Bayes = linear_model.BayesianRidge()\n",
    "    Bayes.fit(trn_data, trn_y)\n",
    "    tree_stack[val_idx] = Bayes.predict(val_data)\n",
    "    predictions += Bayes.predict(test_stack) / 4\n",
    "\n",
    "tree_predictions = np.expm1(predictions)\n",
    "tree_stack = np.expm1(tree_stack)\n",
    "tree_point = mean_absolute_error(tree_stack, np.expm1(Y_data))\n",
    "print(\"树模型：二层贝叶斯: {:<8.8f}\".format(tree_point))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "总输出：三层融合: 433.24430404\n"
     ]
    }
   ],
   "source": [
    "# 导入神经网络模型预测训练集数据，进行三层融合\n",
    "\n",
    "# nn_point = mean_absolute_error(oof_nn, np.expm1(Y_data))\n",
    "# print(\"神经网络: {:<8.8f}\".format(nn_point))\n",
    "\n",
    "mix_nn = False\n",
    "if mix_nn:\n",
    "    oof_nn = np.array(pd.read_csv(tree_data_path + \"nn_train.csv\")[\"price\"])\n",
    "    oof = (oof_nn + tree_stack) / 2\n",
    "    predictions_nn = np.array(pd.read_csv(tree_data_path + \"nn_test.csv\")[\"price\"])\n",
    "    predictions = (tree_predictions + predictions_nn) / 2\n",
    "else:\n",
    "    oof = tree_stack\n",
    "    predictions = tree_predictions\n",
    "all_point = mean_absolute_error(oof, np.expm1(Y_data))\n",
    "print(\"总输出：三层融合: {:<8.8f}\".format(all_point))\n",
    "\n",
    "\n",
    "output_path = os.path.join(path, \"prediction_result\")\n",
    "# 测试集输出\n",
    "sub = pd.DataFrame()\n",
    "sub[\"SaleID\"] = TestA_data.SaleID\n",
    "predictions[predictions < 0] = 0\n",
    "sub[\"price\"] = predictions\n",
    "import random\n",
    "\n",
    "x = random.randint(1, 10000)\n",
    "\n",
    "sub.to_csv(os.path.join(output_path, f\"predictions_{x}.csv\"), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_opencv_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
